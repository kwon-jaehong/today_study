{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 구성 중\n",
      "데이터셋 구성 완료\n",
      "epoch 1/11 64/15287 train loss 4.8696 acc : 0.00% - (0/64)\n",
      "epoch 1/11 128/15287 train loss 4.7361 acc : 7.81% - (5/64)\n",
      "epoch 1/11 192/15287 train loss 4.6332 acc : 17.19% - (11/64)\n",
      "epoch 1/11 256/15287 train loss 4.3926 acc : 28.12% - (18/64)\n",
      "epoch 1/11 320/15287 train loss 4.4196 acc : 25.00% - (16/64)\n",
      "epoch 1/11 384/15287 train loss 4.0722 acc : 31.25% - (20/64)\n",
      "epoch 1/11 448/15287 train loss 3.9765 acc : 34.38% - (22/64)\n",
      "epoch 1/11 512/15287 train loss 3.6998 acc : 35.94% - (23/64)\n",
      "epoch 1/11 576/15287 train loss 3.3909 acc : 34.38% - (22/64)\n",
      "epoch 1/11 640/15287 train loss 3.7405 acc : 25.00% - (16/64)\n",
      "epoch 1/11 704/15287 train loss 3.7975 acc : 18.75% - (12/64)\n",
      "epoch 1/11 768/15287 train loss 3.4993 acc : 26.56% - (17/64)\n",
      "epoch 1/11 832/15287 train loss 2.9713 acc : 34.38% - (22/64)\n",
      "epoch 1/11 896/15287 train loss 3.5653 acc : 26.56% - (17/64)\n",
      "epoch 1/11 960/15287 train loss 3.2588 acc : 35.94% - (23/64)\n",
      "epoch 1/11 1024/15287 train loss 3.5828 acc : 31.25% - (20/64)\n",
      "epoch 1/11 1088/15287 train loss 3.8773 acc : 18.75% - (12/64)\n",
      "epoch 1/11 1152/15287 train loss 3.3935 acc : 26.56% - (17/64)\n",
      "epoch 1/11 1216/15287 train loss 2.6479 acc : 34.38% - (22/64)\n",
      "epoch 1/11 1280/15287 train loss 2.7373 acc : 43.75% - (28/64)\n",
      "epoch 1/11 1344/15287 train loss 3.0175 acc : 28.12% - (18/64)\n",
      "epoch 1/11 1408/15287 train loss 3.0718 acc : 31.25% - (20/64)\n",
      "epoch 1/11 1472/15287 train loss 3.0299 acc : 39.06% - (25/64)\n",
      "epoch 1/11 1536/15287 train loss 2.7538 acc : 43.75% - (28/64)\n",
      "epoch 1/11 1600/15287 train loss 2.8817 acc : 43.75% - (28/64)\n",
      "epoch 1/11 1664/15287 train loss 2.6547 acc : 45.31% - (29/64)\n",
      "epoch 1/11 1728/15287 train loss 2.7693 acc : 40.62% - (26/64)\n",
      "epoch 1/11 1792/15287 train loss 2.6329 acc : 48.44% - (31/64)\n",
      "epoch 1/11 1856/15287 train loss 3.2299 acc : 34.38% - (22/64)\n",
      "epoch 1/11 1920/15287 train loss 2.8248 acc : 43.75% - (28/64)\n",
      "epoch 1/11 1984/15287 train loss 2.9806 acc : 35.94% - (23/64)\n",
      "epoch 1/11 2048/15287 train loss 2.6676 acc : 46.88% - (30/64)\n",
      "epoch 1/11 2112/15287 train loss 2.3990 acc : 48.44% - (31/64)\n",
      "epoch 1/11 2176/15287 train loss 3.0951 acc : 31.25% - (20/64)\n",
      "epoch 1/11 2240/15287 train loss 2.8991 acc : 39.06% - (25/64)\n",
      "epoch 1/11 2304/15287 train loss 2.7105 acc : 42.19% - (27/64)\n",
      "epoch 1/11 2368/15287 train loss 2.4935 acc : 46.88% - (30/64)\n",
      "epoch 1/11 2432/15287 train loss 2.4331 acc : 50.00% - (32/64)\n",
      "epoch 1/11 2496/15287 train loss 3.0127 acc : 29.69% - (19/64)\n",
      "epoch 1/11 2560/15287 train loss 2.7268 acc : 42.19% - (27/64)\n",
      "epoch 1/11 2624/15287 train loss 2.6783 acc : 45.31% - (29/64)\n",
      "epoch 1/11 2688/15287 train loss 2.5569 acc : 46.88% - (30/64)\n",
      "epoch 1/11 2752/15287 train loss 2.9905 acc : 40.62% - (26/64)\n",
      "epoch 1/11 2816/15287 train loss 2.3106 acc : 51.56% - (33/64)\n",
      "epoch 1/11 2880/15287 train loss 2.4770 acc : 50.00% - (32/64)\n",
      "epoch 1/11 2944/15287 train loss 2.5178 acc : 42.19% - (27/64)\n",
      "epoch 1/11 3008/15287 train loss 2.3939 acc : 42.19% - (27/64)\n",
      "epoch 1/11 3072/15287 train loss 2.3372 acc : 51.56% - (33/64)\n",
      "epoch 1/11 3136/15287 train loss 2.0393 acc : 54.69% - (35/64)\n",
      "epoch 1/11 3200/15287 train loss 2.4688 acc : 46.88% - (30/64)\n",
      "epoch 1/11 3264/15287 train loss 2.1410 acc : 51.56% - (33/64)\n",
      "epoch 1/11 3328/15287 train loss 2.4524 acc : 50.00% - (32/64)\n",
      "epoch 1/11 3392/15287 train loss 2.2510 acc : 54.69% - (35/64)\n",
      "epoch 1/11 3456/15287 train loss 2.1188 acc : 59.38% - (38/64)\n",
      "epoch 1/11 3520/15287 train loss 2.2165 acc : 54.69% - (35/64)\n",
      "epoch 1/11 3584/15287 train loss 2.3956 acc : 43.75% - (28/64)\n",
      "epoch 1/11 3648/15287 train loss 2.3134 acc : 50.00% - (32/64)\n",
      "epoch 1/11 3712/15287 train loss 2.2406 acc : 42.19% - (27/64)\n",
      "epoch 1/11 3776/15287 train loss 2.8069 acc : 42.19% - (27/64)\n",
      "epoch 1/11 3840/15287 train loss 2.1256 acc : 53.12% - (34/64)\n",
      "epoch 1/11 3904/15287 train loss 2.3370 acc : 48.44% - (31/64)\n",
      "epoch 1/11 3968/15287 train loss 2.2904 acc : 48.44% - (31/64)\n",
      "epoch 1/11 4032/15287 train loss 2.2329 acc : 48.44% - (31/64)\n",
      "epoch 1/11 4096/15287 train loss 2.5430 acc : 46.88% - (30/64)\n",
      "epoch 1/11 4160/15287 train loss 2.2375 acc : 53.12% - (34/64)\n",
      "epoch 1/11 4224/15287 train loss 1.9196 acc : 54.69% - (35/64)\n",
      "epoch 1/11 4288/15287 train loss 2.0870 acc : 50.00% - (32/64)\n",
      "epoch 1/11 4352/15287 train loss 2.0355 acc : 54.69% - (35/64)\n",
      "epoch 1/11 4416/15287 train loss 3.0298 acc : 35.94% - (23/64)\n",
      "epoch 1/11 4480/15287 train loss 1.9002 acc : 54.69% - (35/64)\n",
      "epoch 1/11 4544/15287 train loss 2.6169 acc : 48.44% - (31/64)\n",
      "epoch 1/11 4608/15287 train loss 2.1096 acc : 54.69% - (35/64)\n",
      "epoch 1/11 4672/15287 train loss 2.3453 acc : 53.12% - (34/64)\n",
      "epoch 1/11 4736/15287 train loss 1.8008 acc : 59.38% - (38/64)\n",
      "epoch 1/11 4800/15287 train loss 2.4738 acc : 48.44% - (31/64)\n",
      "epoch 1/11 4864/15287 train loss 2.1775 acc : 53.12% - (34/64)\n",
      "epoch 1/11 4928/15287 train loss 1.7064 acc : 59.38% - (38/64)\n",
      "epoch 1/11 4992/15287 train loss 2.4132 acc : 42.19% - (27/64)\n",
      "epoch 1/11 5056/15287 train loss 1.7156 acc : 62.50% - (40/64)\n",
      "epoch 1/11 5120/15287 train loss 2.1081 acc : 51.56% - (33/64)\n",
      "epoch 1/11 5184/15287 train loss 2.0037 acc : 54.69% - (35/64)\n",
      "epoch 1/11 5248/15287 train loss 1.8053 acc : 62.50% - (40/64)\n",
      "epoch 1/11 5312/15287 train loss 1.9323 acc : 57.81% - (37/64)\n",
      "epoch 1/11 5376/15287 train loss 2.0647 acc : 48.44% - (31/64)\n",
      "epoch 1/11 5440/15287 train loss 1.6444 acc : 68.75% - (44/64)\n",
      "epoch 1/11 5504/15287 train loss 2.2998 acc : 43.75% - (28/64)\n",
      "epoch 1/11 5568/15287 train loss 2.3621 acc : 53.12% - (34/64)\n",
      "epoch 1/11 5632/15287 train loss 1.8945 acc : 54.69% - (35/64)\n",
      "epoch 1/11 5696/15287 train loss 2.1768 acc : 51.56% - (33/64)\n",
      "epoch 1/11 5760/15287 train loss 1.8789 acc : 60.94% - (39/64)\n",
      "epoch 1/11 5824/15287 train loss 1.7528 acc : 60.94% - (39/64)\n",
      "epoch 1/11 5888/15287 train loss 2.2715 acc : 57.81% - (37/64)\n",
      "epoch 1/11 5952/15287 train loss 2.0060 acc : 67.19% - (43/64)\n",
      "epoch 1/11 6016/15287 train loss 2.1456 acc : 53.12% - (34/64)\n",
      "epoch 1/11 6080/15287 train loss 1.6369 acc : 68.75% - (44/64)\n",
      "epoch 1/11 6144/15287 train loss 2.0671 acc : 54.69% - (35/64)\n",
      "epoch 1/11 6208/15287 train loss 2.3593 acc : 43.75% - (28/64)\n",
      "epoch 1/11 6272/15287 train loss 1.6855 acc : 64.06% - (41/64)\n",
      "epoch 1/11 6336/15287 train loss 1.9320 acc : 53.12% - (34/64)\n",
      "epoch 1/11 6400/15287 train loss 1.8623 acc : 50.00% - (32/64)\n",
      "epoch 1/11 6464/15287 train loss 2.1262 acc : 57.81% - (37/64)\n",
      "epoch 1/11 6528/15287 train loss 1.7240 acc : 60.94% - (39/64)\n",
      "epoch 1/11 6592/15287 train loss 1.8867 acc : 60.94% - (39/64)\n",
      "epoch 1/11 6656/15287 train loss 2.0036 acc : 51.56% - (33/64)\n",
      "epoch 1/11 6720/15287 train loss 1.7078 acc : 60.94% - (39/64)\n",
      "epoch 1/11 6784/15287 train loss 2.0284 acc : 53.12% - (34/64)\n",
      "epoch 1/11 6848/15287 train loss 1.7587 acc : 64.06% - (41/64)\n",
      "epoch 1/11 6912/15287 train loss 1.9680 acc : 53.12% - (34/64)\n",
      "epoch 1/11 6976/15287 train loss 2.0492 acc : 56.25% - (36/64)\n",
      "epoch 1/11 7040/15287 train loss 1.7004 acc : 59.38% - (38/64)\n",
      "epoch 1/11 7104/15287 train loss 1.6186 acc : 67.19% - (43/64)\n",
      "epoch 1/11 7168/15287 train loss 1.6584 acc : 62.50% - (40/64)\n",
      "epoch 1/11 7232/15287 train loss 1.5997 acc : 68.75% - (44/64)\n",
      "epoch 1/11 7296/15287 train loss 1.5061 acc : 64.06% - (41/64)\n",
      "epoch 1/11 7360/15287 train loss 1.9502 acc : 57.81% - (37/64)\n",
      "epoch 1/11 7424/15287 train loss 1.9205 acc : 59.38% - (38/64)\n",
      "epoch 1/11 7488/15287 train loss 1.6334 acc : 65.62% - (42/64)\n",
      "epoch 1/11 7552/15287 train loss 1.8191 acc : 56.25% - (36/64)\n",
      "epoch 1/11 7616/15287 train loss 1.5653 acc : 62.50% - (40/64)\n",
      "epoch 1/11 7680/15287 train loss 1.6736 acc : 60.94% - (39/64)\n",
      "epoch 1/11 7744/15287 train loss 1.5853 acc : 62.50% - (40/64)\n",
      "epoch 1/11 7808/15287 train loss 2.2632 acc : 45.31% - (29/64)\n",
      "epoch 1/11 7872/15287 train loss 1.4396 acc : 62.50% - (40/64)\n",
      "epoch 1/11 7936/15287 train loss 1.8581 acc : 57.81% - (37/64)\n",
      "epoch 1/11 8000/15287 train loss 1.7651 acc : 68.75% - (44/64)\n",
      "epoch 1/11 8064/15287 train loss 1.7789 acc : 64.06% - (41/64)\n",
      "epoch 1/11 8128/15287 train loss 1.9226 acc : 53.12% - (34/64)\n",
      "epoch 1/11 8192/15287 train loss 1.6471 acc : 59.38% - (38/64)\n",
      "epoch 1/11 8256/15287 train loss 1.6267 acc : 65.62% - (42/64)\n",
      "epoch 1/11 8320/15287 train loss 1.8438 acc : 59.38% - (38/64)\n",
      "epoch 1/11 8384/15287 train loss 1.5196 acc : 62.50% - (40/64)\n",
      "epoch 1/11 8448/15287 train loss 1.5731 acc : 68.75% - (44/64)\n",
      "epoch 1/11 8512/15287 train loss 1.4842 acc : 70.31% - (45/64)\n",
      "epoch 1/11 8576/15287 train loss 1.4212 acc : 60.94% - (39/64)\n",
      "epoch 1/11 8640/15287 train loss 2.1362 acc : 46.88% - (30/64)\n",
      "epoch 1/11 8704/15287 train loss 1.7168 acc : 60.94% - (39/64)\n",
      "epoch 1/11 8768/15287 train loss 1.4073 acc : 71.88% - (46/64)\n",
      "epoch 1/11 8832/15287 train loss 1.3090 acc : 70.31% - (45/64)\n",
      "epoch 1/11 8896/15287 train loss 1.4541 acc : 64.06% - (41/64)\n",
      "epoch 1/11 8960/15287 train loss 1.7592 acc : 64.06% - (41/64)\n",
      "epoch 1/11 9024/15287 train loss 1.8319 acc : 60.94% - (39/64)\n",
      "epoch 1/11 9088/15287 train loss 1.8154 acc : 65.62% - (42/64)\n",
      "epoch 1/11 9152/15287 train loss 1.6418 acc : 59.38% - (38/64)\n",
      "epoch 1/11 9216/15287 train loss 1.5012 acc : 70.31% - (45/64)\n",
      "epoch 1/11 9280/15287 train loss 1.4884 acc : 68.75% - (44/64)\n",
      "epoch 1/11 9344/15287 train loss 1.5300 acc : 67.19% - (43/64)\n",
      "epoch 1/11 9408/15287 train loss 1.7386 acc : 59.38% - (38/64)\n",
      "epoch 1/11 9472/15287 train loss 1.4158 acc : 67.19% - (43/64)\n",
      "epoch 1/11 9536/15287 train loss 1.7604 acc : 59.38% - (38/64)\n",
      "epoch 1/11 9600/15287 train loss 1.9025 acc : 50.00% - (32/64)\n",
      "epoch 1/11 9664/15287 train loss 1.6852 acc : 60.94% - (39/64)\n",
      "epoch 1/11 9728/15287 train loss 1.6256 acc : 62.50% - (40/64)\n",
      "epoch 1/11 9792/15287 train loss 2.0254 acc : 57.81% - (37/64)\n",
      "epoch 1/11 9856/15287 train loss 1.5913 acc : 60.94% - (39/64)\n",
      "epoch 1/11 9920/15287 train loss 1.4054 acc : 65.62% - (42/64)\n",
      "epoch 1/11 9984/15287 train loss 1.3637 acc : 65.62% - (42/64)\n",
      "epoch 1/11 10048/15287 train loss 1.6858 acc : 65.62% - (42/64)\n",
      "epoch 1/11 10112/15287 train loss 1.4462 acc : 76.56% - (49/64)\n",
      "epoch 1/11 10176/15287 train loss 1.7775 acc : 60.94% - (39/64)\n",
      "epoch 1/11 10240/15287 train loss 1.3970 acc : 71.88% - (46/64)\n",
      "epoch 1/11 10304/15287 train loss 1.6735 acc : 64.06% - (41/64)\n",
      "epoch 1/11 10368/15287 train loss 1.7222 acc : 59.38% - (38/64)\n",
      "epoch 1/11 10432/15287 train loss 2.0319 acc : 50.00% - (32/64)\n",
      "epoch 1/11 10496/15287 train loss 1.1424 acc : 81.25% - (52/64)\n",
      "epoch 1/11 10560/15287 train loss 1.8984 acc : 56.25% - (36/64)\n",
      "epoch 1/11 10624/15287 train loss 1.7216 acc : 64.06% - (41/64)\n",
      "epoch 1/11 10688/15287 train loss 1.0840 acc : 78.12% - (50/64)\n",
      "epoch 1/11 10752/15287 train loss 1.3213 acc : 75.00% - (48/64)\n",
      "epoch 1/11 10816/15287 train loss 1.9483 acc : 64.06% - (41/64)\n",
      "epoch 1/11 10880/15287 train loss 1.4995 acc : 67.19% - (43/64)\n",
      "epoch 1/11 10944/15287 train loss 1.5159 acc : 60.94% - (39/64)\n",
      "epoch 1/11 11008/15287 train loss 1.6727 acc : 62.50% - (40/64)\n",
      "epoch 1/11 11072/15287 train loss 1.4449 acc : 64.06% - (41/64)\n",
      "epoch 1/11 11136/15287 train loss 1.5617 acc : 70.31% - (45/64)\n",
      "epoch 1/11 11200/15287 train loss 1.3688 acc : 71.88% - (46/64)\n",
      "epoch 1/11 11264/15287 train loss 1.3975 acc : 70.31% - (45/64)\n",
      "epoch 1/11 11328/15287 train loss 1.2789 acc : 65.62% - (42/64)\n",
      "epoch 1/11 11392/15287 train loss 1.7382 acc : 70.31% - (45/64)\n",
      "epoch 1/11 11456/15287 train loss 1.3568 acc : 70.31% - (45/64)\n",
      "epoch 1/11 11520/15287 train loss 1.3599 acc : 65.62% - (42/64)\n",
      "epoch 1/11 11584/15287 train loss 1.4156 acc : 67.19% - (43/64)\n",
      "epoch 1/11 11648/15287 train loss 1.5421 acc : 65.62% - (42/64)\n",
      "epoch 1/11 11712/15287 train loss 1.4026 acc : 64.06% - (41/64)\n",
      "epoch 1/11 11776/15287 train loss 1.6621 acc : 64.06% - (41/64)\n",
      "epoch 1/11 11840/15287 train loss 1.9142 acc : 56.25% - (36/64)\n",
      "epoch 1/11 11904/15287 train loss 1.7832 acc : 56.25% - (36/64)\n",
      "epoch 1/11 11968/15287 train loss 1.3838 acc : 65.62% - (42/64)\n",
      "epoch 1/11 12032/15287 train loss 1.4191 acc : 60.94% - (39/64)\n",
      "epoch 1/11 12096/15287 train loss 1.3380 acc : 71.88% - (46/64)\n",
      "epoch 1/11 12160/15287 train loss 1.5305 acc : 70.31% - (45/64)\n",
      "epoch 1/11 12224/15287 train loss 1.5763 acc : 60.94% - (39/64)\n",
      "epoch 1/11 12288/15287 train loss 1.9384 acc : 62.50% - (40/64)\n",
      "epoch 1/11 12352/15287 train loss 1.3730 acc : 70.31% - (45/64)\n",
      "epoch 1/11 12416/15287 train loss 1.5590 acc : 64.06% - (41/64)\n",
      "epoch 1/11 12480/15287 train loss 1.6850 acc : 60.94% - (39/64)\n",
      "epoch 1/11 12544/15287 train loss 1.5168 acc : 59.38% - (38/64)\n",
      "epoch 1/11 12608/15287 train loss 1.5414 acc : 64.06% - (41/64)\n",
      "epoch 1/11 12672/15287 train loss 1.2931 acc : 73.44% - (47/64)\n",
      "epoch 1/11 12736/15287 train loss 1.4372 acc : 67.19% - (43/64)\n",
      "epoch 1/11 12800/15287 train loss 1.7514 acc : 59.38% - (38/64)\n",
      "epoch 1/11 12864/15287 train loss 1.3176 acc : 68.75% - (44/64)\n",
      "epoch 1/11 12928/15287 train loss 1.4640 acc : 64.06% - (41/64)\n",
      "epoch 1/11 12992/15287 train loss 1.4669 acc : 64.06% - (41/64)\n",
      "epoch 1/11 13056/15287 train loss 1.2600 acc : 71.88% - (46/64)\n",
      "epoch 1/11 13120/15287 train loss 1.8799 acc : 59.38% - (38/64)\n",
      "epoch 1/11 13184/15287 train loss 1.5412 acc : 57.81% - (37/64)\n",
      "epoch 1/11 13248/15287 train loss 1.4172 acc : 73.44% - (47/64)\n",
      "epoch 1/11 13312/15287 train loss 1.2915 acc : 71.88% - (46/64)\n",
      "epoch 1/11 13376/15287 train loss 1.4993 acc : 65.62% - (42/64)\n",
      "epoch 1/11 13440/15287 train loss 1.5218 acc : 59.38% - (38/64)\n",
      "epoch 1/11 13504/15287 train loss 1.5369 acc : 67.19% - (43/64)\n",
      "epoch 1/11 13568/15287 train loss 1.3154 acc : 64.06% - (41/64)\n",
      "epoch 1/11 13632/15287 train loss 1.6443 acc : 64.06% - (41/64)\n",
      "epoch 1/11 13696/15287 train loss 1.7073 acc : 64.06% - (41/64)\n",
      "epoch 1/11 13760/15287 train loss 1.2582 acc : 70.31% - (45/64)\n",
      "epoch 1/11 13824/15287 train loss 1.2915 acc : 68.75% - (44/64)\n",
      "epoch 1/11 13888/15287 train loss 1.3797 acc : 70.31% - (45/64)\n",
      "epoch 1/11 13952/15287 train loss 1.2789 acc : 73.44% - (47/64)\n",
      "epoch 1/11 14016/15287 train loss 1.2288 acc : 62.50% - (40/64)\n",
      "epoch 1/11 14080/15287 train loss 1.2976 acc : 68.75% - (44/64)\n",
      "epoch 1/11 14144/15287 train loss 1.2815 acc : 76.56% - (49/64)\n",
      "epoch 1/11 14208/15287 train loss 1.3119 acc : 73.44% - (47/64)\n",
      "epoch 1/11 14272/15287 train loss 1.3640 acc : 65.62% - (42/64)\n",
      "epoch 1/11 14336/15287 train loss 1.0935 acc : 76.56% - (49/64)\n",
      "epoch 1/11 14400/15287 train loss 1.2149 acc : 71.88% - (46/64)\n",
      "epoch 1/11 14464/15287 train loss 1.4080 acc : 65.62% - (42/64)\n",
      "epoch 1/11 14528/15287 train loss 1.4916 acc : 59.38% - (38/64)\n",
      "epoch 1/11 14592/15287 train loss 1.6058 acc : 62.50% - (40/64)\n",
      "epoch 1/11 14656/15287 train loss 1.3372 acc : 68.75% - (44/64)\n",
      "epoch 1/11 14720/15287 train loss 1.3485 acc : 71.88% - (46/64)\n",
      "epoch 1/11 14784/15287 train loss 1.3646 acc : 76.56% - (49/64)\n",
      "epoch 1/11 14848/15287 train loss 1.5537 acc : 62.50% - (40/64)\n",
      "epoch 1/11 14912/15287 train loss 1.5711 acc : 67.19% - (43/64)\n",
      "epoch 1/11 14976/15287 train loss 1.5824 acc : 62.50% - (40/64)\n",
      "epoch 1/11 15040/15287 train loss 1.2679 acc : 71.88% - (46/64)\n",
      "epoch 1/11 15104/15287 train loss 1.4970 acc : 65.62% - (42/64)\n",
      "epoch 1/11 15168/15287 train loss 1.3587 acc : 71.88% - (46/64)\n",
      "epoch 1/11 15232/15287 train loss 1.4198 acc : 67.19% - (43/64)\n",
      "epoch 1/11 15287/15287 train loss 1.3759 acc : 63.64% - (35/55)\n",
      "\n",
      " epoch 1 train end!!! \t train batch loss : 2.0173\t total acc : 56.24% - (8598/15287) \n",
      "\n",
      "epoch 1 val end!!! val loss : 1.323 \t f1 score : 0.593 acc : 55.62% - (945/1699) \n",
      "\n",
      "\n",
      "epoch 2/11 64/15287 train loss 1.3077 acc : 70.31% - (45/64)\n",
      "epoch 2/11 128/15287 train loss 0.9996 acc : 81.25% - (52/64)\n",
      "epoch 2/11 192/15287 train loss 1.1418 acc : 76.56% - (49/64)\n",
      "epoch 2/11 256/15287 train loss 1.2107 acc : 71.88% - (46/64)\n",
      "epoch 2/11 320/15287 train loss 1.1531 acc : 64.06% - (41/64)\n",
      "epoch 2/11 384/15287 train loss 0.9967 acc : 76.56% - (49/64)\n",
      "epoch 2/11 448/15287 train loss 1.0977 acc : 78.12% - (50/64)\n",
      "epoch 2/11 512/15287 train loss 1.2758 acc : 73.44% - (47/64)\n",
      "epoch 2/11 576/15287 train loss 0.9423 acc : 68.75% - (44/64)\n",
      "epoch 2/11 640/15287 train loss 1.3060 acc : 65.62% - (42/64)\n",
      "epoch 2/11 704/15287 train loss 1.7246 acc : 60.94% - (39/64)\n",
      "epoch 2/11 768/15287 train loss 1.2671 acc : 70.31% - (45/64)\n",
      "epoch 2/11 832/15287 train loss 1.1164 acc : 79.69% - (51/64)\n",
      "epoch 2/11 896/15287 train loss 1.3908 acc : 62.50% - (40/64)\n",
      "epoch 2/11 960/15287 train loss 1.1091 acc : 73.44% - (47/64)\n",
      "epoch 2/11 1024/15287 train loss 1.2464 acc : 70.31% - (45/64)\n",
      "epoch 2/11 1088/15287 train loss 1.4006 acc : 57.81% - (37/64)\n",
      "epoch 2/11 1152/15287 train loss 1.4807 acc : 65.62% - (42/64)\n",
      "epoch 2/11 1216/15287 train loss 0.9038 acc : 79.69% - (51/64)\n",
      "epoch 2/11 1280/15287 train loss 0.9627 acc : 76.56% - (49/64)\n",
      "epoch 2/11 1344/15287 train loss 1.4845 acc : 57.81% - (37/64)\n",
      "epoch 2/11 1408/15287 train loss 1.2198 acc : 71.88% - (46/64)\n",
      "epoch 2/11 1472/15287 train loss 1.4531 acc : 70.31% - (45/64)\n",
      "epoch 2/11 1536/15287 train loss 1.0804 acc : 71.88% - (46/64)\n",
      "epoch 2/11 1600/15287 train loss 1.1761 acc : 76.56% - (49/64)\n",
      "epoch 2/11 1664/15287 train loss 1.0768 acc : 73.44% - (47/64)\n",
      "epoch 2/11 1728/15287 train loss 1.0262 acc : 75.00% - (48/64)\n",
      "epoch 2/11 1792/15287 train loss 1.1635 acc : 68.75% - (44/64)\n",
      "epoch 2/11 1856/15287 train loss 1.4564 acc : 65.62% - (42/64)\n",
      "epoch 2/11 1920/15287 train loss 1.2887 acc : 68.75% - (44/64)\n",
      "epoch 2/11 1984/15287 train loss 1.3427 acc : 65.62% - (42/64)\n",
      "epoch 2/11 2048/15287 train loss 1.1479 acc : 78.12% - (50/64)\n",
      "epoch 2/11 2112/15287 train loss 0.8884 acc : 75.00% - (48/64)\n",
      "epoch 2/11 2176/15287 train loss 1.3891 acc : 64.06% - (41/64)\n",
      "epoch 2/11 2240/15287 train loss 1.0784 acc : 76.56% - (49/64)\n",
      "epoch 2/11 2304/15287 train loss 1.3174 acc : 68.75% - (44/64)\n",
      "epoch 2/11 2368/15287 train loss 1.0735 acc : 75.00% - (48/64)\n",
      "epoch 2/11 2432/15287 train loss 0.8016 acc : 82.81% - (53/64)\n",
      "epoch 2/11 2496/15287 train loss 1.4848 acc : 60.94% - (39/64)\n",
      "epoch 2/11 2560/15287 train loss 1.1625 acc : 71.88% - (46/64)\n",
      "epoch 2/11 2624/15287 train loss 1.0967 acc : 71.88% - (46/64)\n",
      "epoch 2/11 2688/15287 train loss 1.0896 acc : 71.88% - (46/64)\n",
      "epoch 2/11 2752/15287 train loss 1.4565 acc : 64.06% - (41/64)\n",
      "epoch 2/11 2816/15287 train loss 1.0117 acc : 76.56% - (49/64)\n",
      "epoch 2/11 2880/15287 train loss 1.0930 acc : 73.44% - (47/64)\n",
      "epoch 2/11 2944/15287 train loss 1.0070 acc : 78.12% - (50/64)\n",
      "epoch 2/11 3008/15287 train loss 1.0918 acc : 67.19% - (43/64)\n",
      "epoch 2/11 3072/15287 train loss 1.0179 acc : 76.56% - (49/64)\n",
      "epoch 2/11 3136/15287 train loss 1.0167 acc : 73.44% - (47/64)\n",
      "epoch 2/11 3200/15287 train loss 1.1738 acc : 70.31% - (45/64)\n",
      "epoch 2/11 3264/15287 train loss 0.9417 acc : 78.12% - (50/64)\n",
      "epoch 2/11 3328/15287 train loss 1.1138 acc : 70.31% - (45/64)\n",
      "epoch 2/11 3392/15287 train loss 0.9995 acc : 75.00% - (48/64)\n",
      "epoch 2/11 3456/15287 train loss 1.0152 acc : 75.00% - (48/64)\n",
      "epoch 2/11 3520/15287 train loss 0.9973 acc : 76.56% - (49/64)\n",
      "epoch 2/11 3584/15287 train loss 1.2933 acc : 67.19% - (43/64)\n",
      "epoch 2/11 3648/15287 train loss 1.0225 acc : 71.88% - (46/64)\n",
      "epoch 2/11 3712/15287 train loss 1.0228 acc : 75.00% - (48/64)\n",
      "epoch 2/11 3776/15287 train loss 1.3715 acc : 65.62% - (42/64)\n",
      "epoch 2/11 3840/15287 train loss 1.0250 acc : 79.69% - (51/64)\n",
      "epoch 2/11 3904/15287 train loss 0.9690 acc : 73.44% - (47/64)\n",
      "epoch 2/11 3968/15287 train loss 1.2123 acc : 70.31% - (45/64)\n",
      "epoch 2/11 4032/15287 train loss 0.9526 acc : 67.19% - (43/64)\n",
      "epoch 2/11 4096/15287 train loss 1.1498 acc : 73.44% - (47/64)\n",
      "epoch 2/11 4160/15287 train loss 1.1102 acc : 75.00% - (48/64)\n",
      "epoch 2/11 4224/15287 train loss 0.7438 acc : 81.25% - (52/64)\n",
      "epoch 2/11 4288/15287 train loss 1.0645 acc : 78.12% - (50/64)\n",
      "epoch 2/11 4352/15287 train loss 0.8368 acc : 79.69% - (51/64)\n",
      "epoch 2/11 4416/15287 train loss 1.4815 acc : 62.50% - (40/64)\n",
      "epoch 2/11 4480/15287 train loss 0.8684 acc : 79.69% - (51/64)\n",
      "epoch 2/11 4544/15287 train loss 1.2353 acc : 64.06% - (41/64)\n",
      "epoch 2/11 4608/15287 train loss 0.9859 acc : 76.56% - (49/64)\n",
      "epoch 2/11 4672/15287 train loss 1.3681 acc : 67.19% - (43/64)\n",
      "epoch 2/11 4736/15287 train loss 0.8454 acc : 76.56% - (49/64)\n",
      "epoch 2/11 4800/15287 train loss 1.2150 acc : 75.00% - (48/64)\n",
      "epoch 2/11 4864/15287 train loss 1.0434 acc : 75.00% - (48/64)\n",
      "epoch 2/11 4928/15287 train loss 0.8092 acc : 78.12% - (50/64)\n",
      "epoch 2/11 4992/15287 train loss 1.2226 acc : 64.06% - (41/64)\n",
      "epoch 2/11 5056/15287 train loss 0.8228 acc : 81.25% - (52/64)\n",
      "epoch 2/11 5120/15287 train loss 0.9433 acc : 75.00% - (48/64)\n",
      "epoch 2/11 5184/15287 train loss 0.9184 acc : 73.44% - (47/64)\n",
      "epoch 2/11 5248/15287 train loss 0.8427 acc : 84.38% - (54/64)\n",
      "epoch 2/11 5312/15287 train loss 0.9203 acc : 75.00% - (48/64)\n",
      "epoch 2/11 5376/15287 train loss 1.1348 acc : 73.44% - (47/64)\n",
      "epoch 2/11 5440/15287 train loss 0.7746 acc : 87.50% - (56/64)\n",
      "epoch 2/11 5504/15287 train loss 0.9342 acc : 65.62% - (42/64)\n",
      "epoch 2/11 5568/15287 train loss 1.3619 acc : 70.31% - (45/64)\n",
      "epoch 2/11 5632/15287 train loss 0.9253 acc : 71.88% - (46/64)\n",
      "epoch 2/11 5696/15287 train loss 1.1460 acc : 65.62% - (42/64)\n",
      "epoch 2/11 5760/15287 train loss 0.9877 acc : 73.44% - (47/64)\n",
      "epoch 2/11 5824/15287 train loss 0.7816 acc : 79.69% - (51/64)\n",
      "epoch 2/11 5888/15287 train loss 1.3038 acc : 70.31% - (45/64)\n",
      "epoch 2/11 5952/15287 train loss 0.9937 acc : 78.12% - (50/64)\n",
      "epoch 2/11 6016/15287 train loss 1.1874 acc : 68.75% - (44/64)\n",
      "epoch 2/11 6080/15287 train loss 0.9087 acc : 75.00% - (48/64)\n",
      "epoch 2/11 6144/15287 train loss 1.0878 acc : 71.88% - (46/64)\n",
      "epoch 2/11 6208/15287 train loss 1.3066 acc : 70.31% - (45/64)\n",
      "epoch 2/11 6272/15287 train loss 0.7713 acc : 79.69% - (51/64)\n",
      "epoch 2/11 6336/15287 train loss 0.9843 acc : 78.12% - (50/64)\n",
      "epoch 2/11 6400/15287 train loss 0.9373 acc : 79.69% - (51/64)\n",
      "epoch 2/11 6464/15287 train loss 1.2136 acc : 70.31% - (45/64)\n",
      "epoch 2/11 6528/15287 train loss 0.8422 acc : 79.69% - (51/64)\n",
      "epoch 2/11 6592/15287 train loss 1.1004 acc : 78.12% - (50/64)\n",
      "epoch 2/11 6656/15287 train loss 1.0924 acc : 75.00% - (48/64)\n",
      "epoch 2/11 6720/15287 train loss 0.9263 acc : 71.88% - (46/64)\n",
      "epoch 2/11 6784/15287 train loss 1.0513 acc : 71.88% - (46/64)\n",
      "epoch 2/11 6848/15287 train loss 1.0780 acc : 71.88% - (46/64)\n",
      "epoch 2/11 6912/15287 train loss 0.9350 acc : 73.44% - (47/64)\n",
      "epoch 2/11 6976/15287 train loss 0.9969 acc : 78.12% - (50/64)\n",
      "epoch 2/11 7040/15287 train loss 1.0283 acc : 73.44% - (47/64)\n",
      "epoch 2/11 7104/15287 train loss 0.7956 acc : 79.69% - (51/64)\n",
      "epoch 2/11 7168/15287 train loss 0.8317 acc : 78.12% - (50/64)\n",
      "epoch 2/11 7232/15287 train loss 0.8345 acc : 79.69% - (51/64)\n",
      "epoch 2/11 7296/15287 train loss 0.8012 acc : 75.00% - (48/64)\n",
      "epoch 2/11 7360/15287 train loss 0.9415 acc : 73.44% - (47/64)\n",
      "epoch 2/11 7424/15287 train loss 1.1114 acc : 71.88% - (46/64)\n",
      "epoch 2/11 7488/15287 train loss 0.8971 acc : 75.00% - (48/64)\n",
      "epoch 2/11 7552/15287 train loss 0.9306 acc : 71.88% - (46/64)\n",
      "epoch 2/11 7616/15287 train loss 0.6444 acc : 82.81% - (53/64)\n",
      "epoch 2/11 7680/15287 train loss 0.8053 acc : 78.12% - (50/64)\n",
      "epoch 2/11 7744/15287 train loss 0.8857 acc : 76.56% - (49/64)\n",
      "epoch 2/11 7808/15287 train loss 1.3423 acc : 71.88% - (46/64)\n",
      "epoch 2/11 7872/15287 train loss 0.7058 acc : 82.81% - (53/64)\n",
      "epoch 2/11 7936/15287 train loss 1.0153 acc : 78.12% - (50/64)\n",
      "epoch 2/11 8000/15287 train loss 0.9644 acc : 81.25% - (52/64)\n",
      "epoch 2/11 8064/15287 train loss 0.9380 acc : 76.56% - (49/64)\n",
      "epoch 2/11 8128/15287 train loss 1.0571 acc : 71.88% - (46/64)\n",
      "epoch 2/11 8192/15287 train loss 0.7807 acc : 81.25% - (52/64)\n",
      "epoch 2/11 8256/15287 train loss 0.9643 acc : 79.69% - (51/64)\n",
      "epoch 2/11 8320/15287 train loss 1.0996 acc : 67.19% - (43/64)\n",
      "epoch 2/11 8384/15287 train loss 0.9103 acc : 70.31% - (45/64)\n",
      "epoch 2/11 8448/15287 train loss 0.8651 acc : 82.81% - (53/64)\n",
      "epoch 2/11 8512/15287 train loss 0.8160 acc : 79.69% - (51/64)\n",
      "epoch 2/11 8576/15287 train loss 0.7580 acc : 79.69% - (51/64)\n",
      "epoch 2/11 8640/15287 train loss 1.4258 acc : 60.94% - (39/64)\n",
      "epoch 2/11 8704/15287 train loss 0.9389 acc : 76.56% - (49/64)\n",
      "epoch 2/11 8768/15287 train loss 0.7198 acc : 81.25% - (52/64)\n",
      "epoch 2/11 8832/15287 train loss 0.8125 acc : 79.69% - (51/64)\n",
      "epoch 2/11 8896/15287 train loss 0.6766 acc : 76.56% - (49/64)\n",
      "epoch 2/11 8960/15287 train loss 0.9988 acc : 75.00% - (48/64)\n",
      "epoch 2/11 9024/15287 train loss 1.1005 acc : 70.31% - (45/64)\n",
      "epoch 2/11 9088/15287 train loss 0.9863 acc : 76.56% - (49/64)\n",
      "epoch 2/11 9152/15287 train loss 0.9016 acc : 70.31% - (45/64)\n",
      "epoch 2/11 9216/15287 train loss 0.7460 acc : 82.81% - (53/64)\n",
      "epoch 2/11 9280/15287 train loss 0.8163 acc : 75.00% - (48/64)\n",
      "epoch 2/11 9344/15287 train loss 0.8726 acc : 76.56% - (49/64)\n",
      "epoch 2/11 9408/15287 train loss 0.9333 acc : 75.00% - (48/64)\n",
      "epoch 2/11 9472/15287 train loss 0.7927 acc : 81.25% - (52/64)\n",
      "epoch 2/11 9536/15287 train loss 1.0155 acc : 73.44% - (47/64)\n",
      "epoch 2/11 9600/15287 train loss 0.9755 acc : 84.38% - (54/64)\n",
      "epoch 2/11 9664/15287 train loss 1.0076 acc : 78.12% - (50/64)\n",
      "epoch 2/11 9728/15287 train loss 0.7310 acc : 79.69% - (51/64)\n",
      "epoch 2/11 9792/15287 train loss 1.2191 acc : 68.75% - (44/64)\n",
      "epoch 2/11 9856/15287 train loss 0.8540 acc : 81.25% - (52/64)\n",
      "epoch 2/11 9920/15287 train loss 0.8814 acc : 71.88% - (46/64)\n",
      "epoch 2/11 9984/15287 train loss 0.7283 acc : 78.12% - (50/64)\n",
      "epoch 2/11 10048/15287 train loss 0.9818 acc : 71.88% - (46/64)\n",
      "epoch 2/11 10112/15287 train loss 0.8248 acc : 82.81% - (53/64)\n",
      "epoch 2/11 10176/15287 train loss 1.0533 acc : 71.88% - (46/64)\n",
      "epoch 2/11 10240/15287 train loss 0.7283 acc : 84.38% - (54/64)\n",
      "epoch 2/11 10304/15287 train loss 1.0224 acc : 78.12% - (50/64)\n",
      "epoch 2/11 10368/15287 train loss 1.0280 acc : 65.62% - (42/64)\n",
      "epoch 2/11 10432/15287 train loss 1.1531 acc : 68.75% - (44/64)\n",
      "epoch 2/11 10496/15287 train loss 0.6125 acc : 85.94% - (55/64)\n",
      "epoch 2/11 10560/15287 train loss 1.2612 acc : 71.88% - (46/64)\n",
      "epoch 2/11 10624/15287 train loss 1.0546 acc : 78.12% - (50/64)\n",
      "epoch 2/11 10688/15287 train loss 0.5911 acc : 84.38% - (54/64)\n",
      "epoch 2/11 10752/15287 train loss 0.7314 acc : 79.69% - (51/64)\n",
      "epoch 2/11 10816/15287 train loss 1.1915 acc : 68.75% - (44/64)\n",
      "epoch 2/11 10880/15287 train loss 0.8751 acc : 79.69% - (51/64)\n",
      "epoch 2/11 10944/15287 train loss 0.8996 acc : 76.56% - (49/64)\n",
      "epoch 2/11 11008/15287 train loss 0.8990 acc : 75.00% - (48/64)\n",
      "epoch 2/11 11072/15287 train loss 0.7390 acc : 75.00% - (48/64)\n",
      "epoch 2/11 11136/15287 train loss 0.8051 acc : 78.12% - (50/64)\n",
      "epoch 2/11 11200/15287 train loss 0.7861 acc : 84.38% - (54/64)\n",
      "epoch 2/11 11264/15287 train loss 0.7939 acc : 76.56% - (49/64)\n",
      "epoch 2/11 11328/15287 train loss 0.7773 acc : 76.56% - (49/64)\n",
      "epoch 2/11 11392/15287 train loss 1.1070 acc : 81.25% - (52/64)\n",
      "epoch 2/11 11456/15287 train loss 0.7791 acc : 79.69% - (51/64)\n",
      "epoch 2/11 11520/15287 train loss 0.6949 acc : 84.38% - (54/64)\n",
      "epoch 2/11 11584/15287 train loss 0.8213 acc : 76.56% - (49/64)\n",
      "epoch 2/11 11648/15287 train loss 0.8702 acc : 78.12% - (50/64)\n",
      "epoch 2/11 11712/15287 train loss 0.6496 acc : 78.12% - (50/64)\n",
      "epoch 2/11 11776/15287 train loss 0.9303 acc : 75.00% - (48/64)\n",
      "epoch 2/11 11840/15287 train loss 1.0941 acc : 71.88% - (46/64)\n",
      "epoch 2/11 11904/15287 train loss 1.0607 acc : 68.75% - (44/64)\n",
      "epoch 2/11 11968/15287 train loss 0.8159 acc : 79.69% - (51/64)\n",
      "epoch 2/11 12032/15287 train loss 0.7768 acc : 73.44% - (47/64)\n",
      "epoch 2/11 12096/15287 train loss 0.8909 acc : 79.69% - (51/64)\n",
      "epoch 2/11 12160/15287 train loss 0.8648 acc : 70.31% - (45/64)\n",
      "epoch 2/11 12224/15287 train loss 1.1070 acc : 68.75% - (44/64)\n",
      "epoch 2/11 12288/15287 train loss 1.2285 acc : 71.88% - (46/64)\n",
      "epoch 2/11 12352/15287 train loss 0.8200 acc : 81.25% - (52/64)\n",
      "epoch 2/11 12416/15287 train loss 1.0246 acc : 73.44% - (47/64)\n",
      "epoch 2/11 12480/15287 train loss 1.0153 acc : 75.00% - (48/64)\n",
      "epoch 2/11 12544/15287 train loss 0.9744 acc : 75.00% - (48/64)\n",
      "epoch 2/11 12608/15287 train loss 0.8561 acc : 79.69% - (51/64)\n",
      "epoch 2/11 12672/15287 train loss 0.6504 acc : 78.12% - (50/64)\n",
      "epoch 2/11 12736/15287 train loss 0.8363 acc : 79.69% - (51/64)\n",
      "epoch 2/11 12800/15287 train loss 1.0759 acc : 71.88% - (46/64)\n",
      "epoch 2/11 12864/15287 train loss 0.7947 acc : 75.00% - (48/64)\n",
      "epoch 2/11 12928/15287 train loss 0.8402 acc : 78.12% - (50/64)\n",
      "epoch 2/11 12992/15287 train loss 0.8317 acc : 81.25% - (52/64)\n",
      "epoch 2/11 13056/15287 train loss 0.7149 acc : 84.38% - (54/64)\n",
      "epoch 2/11 13120/15287 train loss 1.1947 acc : 73.44% - (47/64)\n",
      "epoch 2/11 13184/15287 train loss 0.9441 acc : 70.31% - (45/64)\n",
      "epoch 2/11 13248/15287 train loss 0.9239 acc : 82.81% - (53/64)\n",
      "epoch 2/11 13312/15287 train loss 0.7880 acc : 78.12% - (50/64)\n",
      "epoch 2/11 13376/15287 train loss 0.7562 acc : 78.12% - (50/64)\n",
      "epoch 2/11 13440/15287 train loss 0.9392 acc : 71.88% - (46/64)\n",
      "epoch 2/11 13504/15287 train loss 0.9243 acc : 81.25% - (52/64)\n",
      "epoch 2/11 13568/15287 train loss 0.7302 acc : 81.25% - (52/64)\n",
      "epoch 2/11 13632/15287 train loss 1.0755 acc : 73.44% - (47/64)\n",
      "epoch 2/11 13696/15287 train loss 1.0375 acc : 73.44% - (47/64)\n",
      "epoch 2/11 13760/15287 train loss 0.8325 acc : 78.12% - (50/64)\n",
      "epoch 2/11 13824/15287 train loss 0.8839 acc : 76.56% - (49/64)\n",
      "epoch 2/11 13888/15287 train loss 0.9825 acc : 75.00% - (48/64)\n",
      "epoch 2/11 13952/15287 train loss 0.8187 acc : 81.25% - (52/64)\n",
      "epoch 2/11 14016/15287 train loss 0.7157 acc : 82.81% - (53/64)\n",
      "epoch 2/11 14080/15287 train loss 0.7547 acc : 81.25% - (52/64)\n",
      "epoch 2/11 14144/15287 train loss 0.8250 acc : 79.69% - (51/64)\n",
      "epoch 2/11 14208/15287 train loss 0.8816 acc : 79.69% - (51/64)\n",
      "epoch 2/11 14272/15287 train loss 0.8553 acc : 75.00% - (48/64)\n",
      "epoch 2/11 14336/15287 train loss 0.7281 acc : 82.81% - (53/64)\n",
      "epoch 2/11 14400/15287 train loss 0.7518 acc : 85.94% - (55/64)\n",
      "epoch 2/11 14464/15287 train loss 0.8459 acc : 75.00% - (48/64)\n",
      "epoch 2/11 14528/15287 train loss 0.9145 acc : 71.88% - (46/64)\n",
      "epoch 2/11 14592/15287 train loss 0.9741 acc : 75.00% - (48/64)\n",
      "epoch 2/11 14656/15287 train loss 0.8273 acc : 78.12% - (50/64)\n",
      "epoch 2/11 14720/15287 train loss 0.8440 acc : 76.56% - (49/64)\n",
      "epoch 2/11 14784/15287 train loss 0.7868 acc : 84.38% - (54/64)\n",
      "epoch 2/11 14848/15287 train loss 1.0417 acc : 73.44% - (47/64)\n",
      "epoch 2/11 14912/15287 train loss 0.9917 acc : 75.00% - (48/64)\n",
      "epoch 2/11 14976/15287 train loss 1.0179 acc : 75.00% - (48/64)\n",
      "epoch 2/11 15040/15287 train loss 0.8026 acc : 79.69% - (51/64)\n",
      "epoch 2/11 15104/15287 train loss 0.9896 acc : 73.44% - (47/64)\n",
      "epoch 2/11 15168/15287 train loss 0.8572 acc : 84.38% - (54/64)\n",
      "epoch 2/11 15232/15287 train loss 0.7990 acc : 78.12% - (50/64)\n",
      "epoch 2/11 15287/15287 train loss 0.8007 acc : 76.36% - (42/55)\n",
      "\n",
      " epoch 2 train end!!! \t train batch loss : 0.9914\t total acc : 75.02% - (11469/15287) \n",
      "\n",
      "epoch 2 val end!!! val loss : 0.758 \t f1 score : 0.759 acc : 68.33% - (1161/1699) \n",
      "\n",
      "\n",
      "epoch 3/11 64/15287 train loss 0.7767 acc : 81.25% - (52/64)\n",
      "epoch 3/11 128/15287 train loss 0.7223 acc : 82.81% - (53/64)\n",
      "epoch 3/11 192/15287 train loss 0.7149 acc : 82.81% - (53/64)\n",
      "epoch 3/11 256/15287 train loss 0.7520 acc : 79.69% - (51/64)\n",
      "epoch 3/11 320/15287 train loss 0.7438 acc : 78.12% - (50/64)\n",
      "epoch 3/11 384/15287 train loss 0.6444 acc : 85.94% - (55/64)\n",
      "epoch 3/11 448/15287 train loss 0.6743 acc : 81.25% - (52/64)\n",
      "epoch 3/11 512/15287 train loss 0.8582 acc : 82.81% - (53/64)\n",
      "epoch 3/11 576/15287 train loss 0.6496 acc : 81.25% - (52/64)\n",
      "epoch 3/11 640/15287 train loss 0.7339 acc : 79.69% - (51/64)\n",
      "epoch 3/11 704/15287 train loss 1.0497 acc : 75.00% - (48/64)\n",
      "epoch 3/11 768/15287 train loss 0.7711 acc : 81.25% - (52/64)\n",
      "epoch 3/11 832/15287 train loss 0.7369 acc : 84.38% - (54/64)\n",
      "epoch 3/11 896/15287 train loss 0.8258 acc : 79.69% - (51/64)\n",
      "epoch 3/11 960/15287 train loss 0.6998 acc : 85.94% - (55/64)\n",
      "epoch 3/11 1024/15287 train loss 0.7353 acc : 76.56% - (49/64)\n",
      "epoch 3/11 1088/15287 train loss 0.7510 acc : 75.00% - (48/64)\n",
      "epoch 3/11 1152/15287 train loss 0.9618 acc : 76.56% - (49/64)\n",
      "epoch 3/11 1216/15287 train loss 0.5998 acc : 84.38% - (54/64)\n",
      "epoch 3/11 1280/15287 train loss 0.6614 acc : 81.25% - (52/64)\n",
      "epoch 3/11 1344/15287 train loss 0.9075 acc : 71.88% - (46/64)\n",
      "epoch 3/11 1408/15287 train loss 0.8646 acc : 78.12% - (50/64)\n",
      "epoch 3/11 1472/15287 train loss 0.8853 acc : 85.94% - (55/64)\n",
      "epoch 3/11 1536/15287 train loss 0.7321 acc : 81.25% - (52/64)\n",
      "epoch 3/11 1600/15287 train loss 0.7384 acc : 82.81% - (53/64)\n",
      "epoch 3/11 1664/15287 train loss 0.6191 acc : 84.38% - (54/64)\n",
      "epoch 3/11 1728/15287 train loss 0.6008 acc : 84.38% - (54/64)\n",
      "epoch 3/11 1792/15287 train loss 0.7483 acc : 79.69% - (51/64)\n",
      "epoch 3/11 1856/15287 train loss 0.8758 acc : 73.44% - (47/64)\n",
      "epoch 3/11 1920/15287 train loss 0.7467 acc : 81.25% - (52/64)\n",
      "epoch 3/11 1984/15287 train loss 0.8523 acc : 75.00% - (48/64)\n",
      "epoch 3/11 2048/15287 train loss 0.6764 acc : 87.50% - (56/64)\n",
      "epoch 3/11 2112/15287 train loss 0.4458 acc : 90.62% - (58/64)\n",
      "epoch 3/11 2176/15287 train loss 0.9675 acc : 71.88% - (46/64)\n",
      "epoch 3/11 2240/15287 train loss 0.6623 acc : 84.38% - (54/64)\n",
      "epoch 3/11 2304/15287 train loss 0.8974 acc : 76.56% - (49/64)\n",
      "epoch 3/11 2368/15287 train loss 0.6525 acc : 81.25% - (52/64)\n",
      "epoch 3/11 2432/15287 train loss 0.4963 acc : 87.50% - (56/64)\n",
      "epoch 3/11 2496/15287 train loss 0.9807 acc : 70.31% - (45/64)\n",
      "epoch 3/11 2560/15287 train loss 0.6959 acc : 81.25% - (52/64)\n",
      "epoch 3/11 2624/15287 train loss 0.7086 acc : 84.38% - (54/64)\n",
      "epoch 3/11 2688/15287 train loss 0.6132 acc : 85.94% - (55/64)\n",
      "epoch 3/11 2752/15287 train loss 0.9068 acc : 73.44% - (47/64)\n",
      "epoch 3/11 2816/15287 train loss 0.5620 acc : 87.50% - (56/64)\n",
      "epoch 3/11 2880/15287 train loss 0.7053 acc : 78.12% - (50/64)\n",
      "epoch 3/11 2944/15287 train loss 0.6296 acc : 87.50% - (56/64)\n",
      "epoch 3/11 3008/15287 train loss 0.6783 acc : 79.69% - (51/64)\n",
      "epoch 3/11 3072/15287 train loss 0.6189 acc : 84.38% - (54/64)\n",
      "epoch 3/11 3136/15287 train loss 0.7324 acc : 78.12% - (50/64)\n",
      "epoch 3/11 3200/15287 train loss 0.7178 acc : 79.69% - (51/64)\n",
      "epoch 3/11 3264/15287 train loss 0.5905 acc : 90.62% - (58/64)\n",
      "epoch 3/11 3328/15287 train loss 0.6924 acc : 79.69% - (51/64)\n",
      "epoch 3/11 3392/15287 train loss 0.6091 acc : 82.81% - (53/64)\n",
      "epoch 3/11 3456/15287 train loss 0.6670 acc : 84.38% - (54/64)\n",
      "epoch 3/11 3520/15287 train loss 0.5312 acc : 84.38% - (54/64)\n",
      "epoch 3/11 3584/15287 train loss 0.8121 acc : 82.81% - (53/64)\n",
      "epoch 3/11 3648/15287 train loss 0.6768 acc : 82.81% - (53/64)\n",
      "epoch 3/11 3712/15287 train loss 0.5963 acc : 82.81% - (53/64)\n",
      "epoch 3/11 3776/15287 train loss 0.8985 acc : 70.31% - (45/64)\n",
      "epoch 3/11 3840/15287 train loss 0.7235 acc : 87.50% - (56/64)\n",
      "epoch 3/11 3904/15287 train loss 0.5624 acc : 82.81% - (53/64)\n",
      "epoch 3/11 3968/15287 train loss 0.7790 acc : 79.69% - (51/64)\n",
      "epoch 3/11 4032/15287 train loss 0.5612 acc : 85.94% - (55/64)\n",
      "epoch 3/11 4096/15287 train loss 0.7384 acc : 81.25% - (52/64)\n",
      "epoch 3/11 4160/15287 train loss 0.7342 acc : 82.81% - (53/64)\n",
      "epoch 3/11 4224/15287 train loss 0.4330 acc : 89.06% - (57/64)\n",
      "epoch 3/11 4288/15287 train loss 0.6190 acc : 84.38% - (54/64)\n",
      "epoch 3/11 4352/15287 train loss 0.4760 acc : 85.94% - (55/64)\n",
      "epoch 3/11 4416/15287 train loss 0.8494 acc : 75.00% - (48/64)\n",
      "epoch 3/11 4480/15287 train loss 0.5462 acc : 84.38% - (54/64)\n",
      "epoch 3/11 4544/15287 train loss 0.6838 acc : 82.81% - (53/64)\n",
      "epoch 3/11 4608/15287 train loss 0.6608 acc : 82.81% - (53/64)\n",
      "epoch 3/11 4672/15287 train loss 0.9550 acc : 78.12% - (50/64)\n",
      "epoch 3/11 4736/15287 train loss 0.5598 acc : 85.94% - (55/64)\n",
      "epoch 3/11 4800/15287 train loss 0.7025 acc : 81.25% - (52/64)\n",
      "epoch 3/11 4864/15287 train loss 0.6218 acc : 84.38% - (54/64)\n",
      "epoch 3/11 4928/15287 train loss 0.5622 acc : 79.69% - (51/64)\n",
      "epoch 3/11 4992/15287 train loss 0.8087 acc : 75.00% - (48/64)\n",
      "epoch 3/11 5056/15287 train loss 0.5835 acc : 84.38% - (54/64)\n",
      "epoch 3/11 5120/15287 train loss 0.5888 acc : 79.69% - (51/64)\n",
      "epoch 3/11 5184/15287 train loss 0.5307 acc : 89.06% - (57/64)\n",
      "epoch 3/11 5248/15287 train loss 0.5407 acc : 82.81% - (53/64)\n",
      "epoch 3/11 5312/15287 train loss 0.6228 acc : 81.25% - (52/64)\n",
      "epoch 3/11 5376/15287 train loss 0.7631 acc : 81.25% - (52/64)\n",
      "epoch 3/11 5440/15287 train loss 0.5201 acc : 87.50% - (56/64)\n",
      "epoch 3/11 5504/15287 train loss 0.5349 acc : 84.38% - (54/64)\n",
      "epoch 3/11 5568/15287 train loss 0.9416 acc : 76.56% - (49/64)\n",
      "epoch 3/11 5632/15287 train loss 0.5234 acc : 78.12% - (50/64)\n",
      "epoch 3/11 5696/15287 train loss 0.6955 acc : 78.12% - (50/64)\n",
      "epoch 3/11 5760/15287 train loss 0.6317 acc : 84.38% - (54/64)\n",
      "epoch 3/11 5824/15287 train loss 0.4427 acc : 87.50% - (56/64)\n",
      "epoch 3/11 5888/15287 train loss 0.7746 acc : 82.81% - (53/64)\n",
      "epoch 3/11 5952/15287 train loss 0.5393 acc : 82.81% - (53/64)\n",
      "epoch 3/11 6016/15287 train loss 0.7447 acc : 79.69% - (51/64)\n",
      "epoch 3/11 6080/15287 train loss 0.6025 acc : 84.38% - (54/64)\n",
      "epoch 3/11 6144/15287 train loss 0.6980 acc : 81.25% - (52/64)\n",
      "epoch 3/11 6208/15287 train loss 0.7459 acc : 76.56% - (49/64)\n",
      "epoch 3/11 6272/15287 train loss 0.4254 acc : 90.62% - (58/64)\n",
      "epoch 3/11 6336/15287 train loss 0.5983 acc : 85.94% - (55/64)\n",
      "epoch 3/11 6400/15287 train loss 0.6217 acc : 89.06% - (57/64)\n",
      "epoch 3/11 6464/15287 train loss 0.7497 acc : 81.25% - (52/64)\n",
      "epoch 3/11 6528/15287 train loss 0.5552 acc : 84.38% - (54/64)\n",
      "epoch 3/11 6592/15287 train loss 0.6727 acc : 85.94% - (55/64)\n",
      "epoch 3/11 6656/15287 train loss 0.7062 acc : 79.69% - (51/64)\n",
      "epoch 3/11 6720/15287 train loss 0.5684 acc : 84.38% - (54/64)\n",
      "epoch 3/11 6784/15287 train loss 0.6814 acc : 85.94% - (55/64)\n",
      "epoch 3/11 6848/15287 train loss 0.7290 acc : 81.25% - (52/64)\n",
      "epoch 3/11 6912/15287 train loss 0.5093 acc : 84.38% - (54/64)\n",
      "epoch 3/11 6976/15287 train loss 0.6566 acc : 85.94% - (55/64)\n",
      "epoch 3/11 7040/15287 train loss 0.7005 acc : 82.81% - (53/64)\n",
      "epoch 3/11 7104/15287 train loss 0.4663 acc : 87.50% - (56/64)\n",
      "epoch 3/11 7168/15287 train loss 0.5562 acc : 89.06% - (57/64)\n",
      "epoch 3/11 7232/15287 train loss 0.5541 acc : 84.38% - (54/64)\n",
      "epoch 3/11 7296/15287 train loss 0.5483 acc : 85.94% - (55/64)\n",
      "epoch 3/11 7360/15287 train loss 0.5481 acc : 85.94% - (55/64)\n",
      "epoch 3/11 7424/15287 train loss 0.6879 acc : 78.12% - (50/64)\n",
      "epoch 3/11 7488/15287 train loss 0.5824 acc : 84.38% - (54/64)\n",
      "epoch 3/11 7552/15287 train loss 0.6018 acc : 79.69% - (51/64)\n",
      "epoch 3/11 7616/15287 train loss 0.3476 acc : 93.75% - (60/64)\n",
      "epoch 3/11 7680/15287 train loss 0.4629 acc : 87.50% - (56/64)\n",
      "epoch 3/11 7744/15287 train loss 0.5934 acc : 85.94% - (55/64)\n",
      "epoch 3/11 7808/15287 train loss 0.7890 acc : 81.25% - (52/64)\n",
      "epoch 3/11 7872/15287 train loss 0.3997 acc : 93.75% - (60/64)\n",
      "epoch 3/11 7936/15287 train loss 0.6461 acc : 84.38% - (54/64)\n",
      "epoch 3/11 8000/15287 train loss 0.5918 acc : 87.50% - (56/64)\n",
      "epoch 3/11 8064/15287 train loss 0.5368 acc : 84.38% - (54/64)\n",
      "epoch 3/11 8128/15287 train loss 0.7406 acc : 82.81% - (53/64)\n",
      "epoch 3/11 8192/15287 train loss 0.4275 acc : 92.19% - (59/64)\n",
      "epoch 3/11 8256/15287 train loss 0.6378 acc : 84.38% - (54/64)\n",
      "epoch 3/11 8320/15287 train loss 0.6835 acc : 84.38% - (54/64)\n",
      "epoch 3/11 8384/15287 train loss 0.6128 acc : 84.38% - (54/64)\n",
      "epoch 3/11 8448/15287 train loss 0.5348 acc : 89.06% - (57/64)\n",
      "epoch 3/11 8512/15287 train loss 0.5017 acc : 85.94% - (55/64)\n",
      "epoch 3/11 8576/15287 train loss 0.5061 acc : 82.81% - (53/64)\n",
      "epoch 3/11 8640/15287 train loss 0.9629 acc : 68.75% - (44/64)\n",
      "epoch 3/11 8704/15287 train loss 0.5220 acc : 87.50% - (56/64)\n",
      "epoch 3/11 8768/15287 train loss 0.4016 acc : 87.50% - (56/64)\n",
      "epoch 3/11 8832/15287 train loss 0.5643 acc : 85.94% - (55/64)\n",
      "epoch 3/11 8896/15287 train loss 0.3847 acc : 89.06% - (57/64)\n",
      "epoch 3/11 8960/15287 train loss 0.6344 acc : 82.81% - (53/64)\n",
      "epoch 3/11 9024/15287 train loss 0.6695 acc : 78.12% - (50/64)\n",
      "epoch 3/11 9088/15287 train loss 0.5104 acc : 84.38% - (54/64)\n",
      "epoch 3/11 9152/15287 train loss 0.5776 acc : 82.81% - (53/64)\n",
      "epoch 3/11 9216/15287 train loss 0.4159 acc : 89.06% - (57/64)\n",
      "epoch 3/11 9280/15287 train loss 0.4830 acc : 93.75% - (60/64)\n",
      "epoch 3/11 9344/15287 train loss 0.4889 acc : 89.06% - (57/64)\n",
      "epoch 3/11 9408/15287 train loss 0.5294 acc : 85.94% - (55/64)\n",
      "epoch 3/11 9472/15287 train loss 0.5389 acc : 84.38% - (54/64)\n",
      "epoch 3/11 9536/15287 train loss 0.6482 acc : 81.25% - (52/64)\n",
      "epoch 3/11 9600/15287 train loss 0.5624 acc : 85.94% - (55/64)\n",
      "epoch 3/11 9664/15287 train loss 0.6219 acc : 82.81% - (53/64)\n",
      "epoch 3/11 9728/15287 train loss 0.4117 acc : 85.94% - (55/64)\n",
      "epoch 3/11 9792/15287 train loss 0.7510 acc : 76.56% - (49/64)\n",
      "epoch 3/11 9856/15287 train loss 0.4957 acc : 87.50% - (56/64)\n",
      "epoch 3/11 9920/15287 train loss 0.5577 acc : 81.25% - (52/64)\n",
      "epoch 3/11 9984/15287 train loss 0.4132 acc : 87.50% - (56/64)\n",
      "epoch 3/11 10048/15287 train loss 0.6420 acc : 79.69% - (51/64)\n",
      "epoch 3/11 10112/15287 train loss 0.5165 acc : 87.50% - (56/64)\n",
      "epoch 3/11 10176/15287 train loss 0.6709 acc : 78.12% - (50/64)\n",
      "epoch 3/11 10240/15287 train loss 0.4907 acc : 93.75% - (60/64)\n",
      "epoch 3/11 10304/15287 train loss 0.6159 acc : 87.50% - (56/64)\n",
      "epoch 3/11 10368/15287 train loss 0.6333 acc : 76.56% - (49/64)\n",
      "epoch 3/11 10432/15287 train loss 0.7166 acc : 81.25% - (52/64)\n",
      "epoch 3/11 10496/15287 train loss 0.3690 acc : 89.06% - (57/64)\n",
      "epoch 3/11 10560/15287 train loss 0.8500 acc : 79.69% - (51/64)\n",
      "epoch 3/11 10624/15287 train loss 0.6999 acc : 81.25% - (52/64)\n",
      "epoch 3/11 10688/15287 train loss 0.3916 acc : 89.06% - (57/64)\n",
      "epoch 3/11 10752/15287 train loss 0.4042 acc : 87.50% - (56/64)\n",
      "epoch 3/11 10816/15287 train loss 0.7555 acc : 87.50% - (56/64)\n",
      "epoch 3/11 10880/15287 train loss 0.6259 acc : 82.81% - (53/64)\n",
      "epoch 3/11 10944/15287 train loss 0.6622 acc : 79.69% - (51/64)\n",
      "epoch 3/11 11008/15287 train loss 0.5310 acc : 84.38% - (54/64)\n",
      "epoch 3/11 11072/15287 train loss 0.4096 acc : 85.94% - (55/64)\n",
      "epoch 3/11 11136/15287 train loss 0.3912 acc : 89.06% - (57/64)\n",
      "epoch 3/11 11200/15287 train loss 0.4841 acc : 87.50% - (56/64)\n",
      "epoch 3/11 11264/15287 train loss 0.4750 acc : 85.94% - (55/64)\n",
      "epoch 3/11 11328/15287 train loss 0.5023 acc : 84.38% - (54/64)\n",
      "epoch 3/11 11392/15287 train loss 0.6580 acc : 87.50% - (56/64)\n",
      "epoch 3/11 11456/15287 train loss 0.5067 acc : 89.06% - (57/64)\n",
      "epoch 3/11 11520/15287 train loss 0.3496 acc : 90.62% - (58/64)\n",
      "epoch 3/11 11584/15287 train loss 0.4573 acc : 87.50% - (56/64)\n",
      "epoch 3/11 11648/15287 train loss 0.5586 acc : 89.06% - (57/64)\n",
      "epoch 3/11 11712/15287 train loss 0.3181 acc : 93.75% - (60/64)\n",
      "epoch 3/11 11776/15287 train loss 0.5076 acc : 90.62% - (58/64)\n",
      "epoch 3/11 11840/15287 train loss 0.6543 acc : 81.25% - (52/64)\n",
      "epoch 3/11 11904/15287 train loss 0.6473 acc : 84.38% - (54/64)\n",
      "epoch 3/11 11968/15287 train loss 0.5417 acc : 85.94% - (55/64)\n",
      "epoch 3/11 12032/15287 train loss 0.4866 acc : 87.50% - (56/64)\n",
      "epoch 3/11 12096/15287 train loss 0.6193 acc : 84.38% - (54/64)\n",
      "epoch 3/11 12160/15287 train loss 0.5080 acc : 84.38% - (54/64)\n",
      "epoch 3/11 12224/15287 train loss 0.8145 acc : 76.56% - (49/64)\n",
      "epoch 3/11 12288/15287 train loss 0.8210 acc : 76.56% - (49/64)\n",
      "epoch 3/11 12352/15287 train loss 0.4887 acc : 93.75% - (60/64)\n",
      "epoch 3/11 12416/15287 train loss 0.6790 acc : 78.12% - (50/64)\n",
      "epoch 3/11 12480/15287 train loss 0.6119 acc : 81.25% - (52/64)\n",
      "epoch 3/11 12544/15287 train loss 0.6493 acc : 84.38% - (54/64)\n",
      "epoch 3/11 12608/15287 train loss 0.4900 acc : 87.50% - (56/64)\n",
      "epoch 3/11 12672/15287 train loss 0.3683 acc : 90.62% - (58/64)\n",
      "epoch 3/11 12736/15287 train loss 0.5037 acc : 85.94% - (55/64)\n",
      "epoch 3/11 12800/15287 train loss 0.6065 acc : 81.25% - (52/64)\n",
      "epoch 3/11 12864/15287 train loss 0.4959 acc : 89.06% - (57/64)\n",
      "epoch 3/11 12928/15287 train loss 0.5245 acc : 85.94% - (55/64)\n",
      "epoch 3/11 12992/15287 train loss 0.5228 acc : 87.50% - (56/64)\n",
      "epoch 3/11 13056/15287 train loss 0.4178 acc : 90.62% - (58/64)\n",
      "epoch 3/11 13120/15287 train loss 0.8277 acc : 76.56% - (49/64)\n",
      "epoch 3/11 13184/15287 train loss 0.5655 acc : 89.06% - (57/64)\n",
      "epoch 3/11 13248/15287 train loss 0.6149 acc : 89.06% - (57/64)\n",
      "epoch 3/11 13312/15287 train loss 0.4657 acc : 87.50% - (56/64)\n",
      "epoch 3/11 13376/15287 train loss 0.3577 acc : 92.19% - (59/64)\n",
      "epoch 3/11 13440/15287 train loss 0.5675 acc : 85.94% - (55/64)\n",
      "epoch 3/11 13504/15287 train loss 0.5845 acc : 82.81% - (53/64)\n",
      "epoch 3/11 13568/15287 train loss 0.4516 acc : 87.50% - (56/64)\n",
      "epoch 3/11 13632/15287 train loss 0.7115 acc : 81.25% - (52/64)\n",
      "epoch 3/11 13696/15287 train loss 0.6671 acc : 84.38% - (54/64)\n",
      "epoch 3/11 13760/15287 train loss 0.6167 acc : 85.94% - (55/64)\n",
      "epoch 3/11 13824/15287 train loss 0.5944 acc : 81.25% - (52/64)\n",
      "epoch 3/11 13888/15287 train loss 0.7143 acc : 78.12% - (50/64)\n",
      "epoch 3/11 13952/15287 train loss 0.5536 acc : 87.50% - (56/64)\n",
      "epoch 3/11 14016/15287 train loss 0.3891 acc : 93.75% - (60/64)\n",
      "epoch 3/11 14080/15287 train loss 0.4163 acc : 89.06% - (57/64)\n",
      "epoch 3/11 14144/15287 train loss 0.4704 acc : 90.62% - (58/64)\n",
      "epoch 3/11 14208/15287 train loss 0.6452 acc : 84.38% - (54/64)\n",
      "epoch 3/11 14272/15287 train loss 0.5432 acc : 81.25% - (52/64)\n",
      "epoch 3/11 14336/15287 train loss 0.5304 acc : 87.50% - (56/64)\n",
      "epoch 3/11 14400/15287 train loss 0.4489 acc : 87.50% - (56/64)\n",
      "epoch 3/11 14464/15287 train loss 0.5397 acc : 85.94% - (55/64)\n",
      "epoch 3/11 14528/15287 train loss 0.5539 acc : 76.56% - (49/64)\n",
      "epoch 3/11 14592/15287 train loss 0.5912 acc : 85.94% - (55/64)\n",
      "epoch 3/11 14656/15287 train loss 0.4826 acc : 85.94% - (55/64)\n",
      "epoch 3/11 14720/15287 train loss 0.5337 acc : 85.94% - (55/64)\n",
      "epoch 3/11 14784/15287 train loss 0.4678 acc : 92.19% - (59/64)\n",
      "epoch 3/11 14848/15287 train loss 0.7233 acc : 81.25% - (52/64)\n",
      "epoch 3/11 14912/15287 train loss 0.5999 acc : 84.38% - (54/64)\n",
      "epoch 3/11 14976/15287 train loss 0.6841 acc : 82.81% - (53/64)\n",
      "epoch 3/11 15040/15287 train loss 0.5148 acc : 89.06% - (57/64)\n",
      "epoch 3/11 15104/15287 train loss 0.6774 acc : 82.81% - (53/64)\n",
      "epoch 3/11 15168/15287 train loss 0.5950 acc : 87.50% - (56/64)\n",
      "epoch 3/11 15232/15287 train loss 0.4714 acc : 84.38% - (54/64)\n",
      "epoch 3/11 15287/15287 train loss 0.4812 acc : 85.45% - (47/55)\n",
      "\n",
      " epoch 3 train end!!! \t train batch loss : 0.6185\t total acc : 83.81% - (12812/15287) \n",
      "\n",
      "epoch 3 val end!!! val loss : 0.453 \t f1 score : 0.837 acc : 74.69% - (1269/1699) \n",
      "\n",
      "\n",
      "epoch 4/11 64/15287 train loss 0.4371 acc : 89.06% - (57/64)\n",
      "epoch 4/11 128/15287 train loss 0.4824 acc : 87.50% - (56/64)\n",
      "epoch 4/11 192/15287 train loss 0.4514 acc : 87.50% - (56/64)\n",
      "epoch 4/11 256/15287 train loss 0.4265 acc : 87.50% - (56/64)\n",
      "epoch 4/11 320/15287 train loss 0.4945 acc : 89.06% - (57/64)\n",
      "epoch 4/11 384/15287 train loss 0.4106 acc : 90.62% - (58/64)\n",
      "epoch 4/11 448/15287 train loss 0.4303 acc : 89.06% - (57/64)\n",
      "epoch 4/11 512/15287 train loss 0.6078 acc : 89.06% - (57/64)\n",
      "epoch 4/11 576/15287 train loss 0.4834 acc : 85.94% - (55/64)\n",
      "epoch 4/11 640/15287 train loss 0.4762 acc : 85.94% - (55/64)\n",
      "epoch 4/11 704/15287 train loss 0.6662 acc : 85.94% - (55/64)\n",
      "epoch 4/11 768/15287 train loss 0.4553 acc : 85.94% - (55/64)\n",
      "epoch 4/11 832/15287 train loss 0.4539 acc : 87.50% - (56/64)\n",
      "epoch 4/11 896/15287 train loss 0.5237 acc : 82.81% - (53/64)\n",
      "epoch 4/11 960/15287 train loss 0.4881 acc : 92.19% - (59/64)\n",
      "epoch 4/11 1024/15287 train loss 0.4623 acc : 87.50% - (56/64)\n",
      "epoch 4/11 1088/15287 train loss 0.4038 acc : 89.06% - (57/64)\n",
      "epoch 4/11 1152/15287 train loss 0.6233 acc : 85.94% - (55/64)\n",
      "epoch 4/11 1216/15287 train loss 0.3863 acc : 92.19% - (59/64)\n",
      "epoch 4/11 1280/15287 train loss 0.4585 acc : 84.38% - (54/64)\n",
      "epoch 4/11 1344/15287 train loss 0.5827 acc : 84.38% - (54/64)\n",
      "epoch 4/11 1408/15287 train loss 0.5951 acc : 81.25% - (52/64)\n",
      "epoch 4/11 1472/15287 train loss 0.5596 acc : 92.19% - (59/64)\n",
      "epoch 4/11 1536/15287 train loss 0.4815 acc : 85.94% - (55/64)\n",
      "epoch 4/11 1600/15287 train loss 0.5186 acc : 87.50% - (56/64)\n",
      "epoch 4/11 1664/15287 train loss 0.3897 acc : 92.19% - (59/64)\n",
      "epoch 4/11 1728/15287 train loss 0.3850 acc : 92.19% - (59/64)\n",
      "epoch 4/11 1792/15287 train loss 0.4717 acc : 85.94% - (55/64)\n",
      "epoch 4/11 1856/15287 train loss 0.4930 acc : 82.81% - (53/64)\n",
      "epoch 4/11 1920/15287 train loss 0.4095 acc : 90.62% - (58/64)\n",
      "epoch 4/11 1984/15287 train loss 0.5514 acc : 84.38% - (54/64)\n",
      "epoch 4/11 2048/15287 train loss 0.4385 acc : 89.06% - (57/64)\n",
      "epoch 4/11 2112/15287 train loss 0.2380 acc : 96.88% - (62/64)\n",
      "epoch 4/11 2176/15287 train loss 0.6776 acc : 81.25% - (52/64)\n",
      "epoch 4/11 2240/15287 train loss 0.3903 acc : 90.62% - (58/64)\n",
      "epoch 4/11 2304/15287 train loss 0.5960 acc : 84.38% - (54/64)\n",
      "epoch 4/11 2368/15287 train loss 0.4194 acc : 89.06% - (57/64)\n",
      "epoch 4/11 2432/15287 train loss 0.3249 acc : 87.50% - (56/64)\n",
      "epoch 4/11 2496/15287 train loss 0.5743 acc : 84.38% - (54/64)\n",
      "epoch 4/11 2560/15287 train loss 0.4094 acc : 89.06% - (57/64)\n",
      "epoch 4/11 2624/15287 train loss 0.3906 acc : 92.19% - (59/64)\n",
      "epoch 4/11 2688/15287 train loss 0.3554 acc : 92.19% - (59/64)\n",
      "epoch 4/11 2752/15287 train loss 0.5386 acc : 85.94% - (55/64)\n",
      "epoch 4/11 2816/15287 train loss 0.3223 acc : 95.31% - (61/64)\n",
      "epoch 4/11 2880/15287 train loss 0.4370 acc : 87.50% - (56/64)\n",
      "epoch 4/11 2944/15287 train loss 0.4597 acc : 89.06% - (57/64)\n",
      "epoch 4/11 3008/15287 train loss 0.3949 acc : 87.50% - (56/64)\n",
      "epoch 4/11 3072/15287 train loss 0.3243 acc : 90.62% - (58/64)\n",
      "epoch 4/11 3136/15287 train loss 0.4815 acc : 84.38% - (54/64)\n",
      "epoch 4/11 3200/15287 train loss 0.4408 acc : 87.50% - (56/64)\n",
      "epoch 4/11 3264/15287 train loss 0.3761 acc : 90.62% - (58/64)\n",
      "epoch 4/11 3328/15287 train loss 0.3962 acc : 87.50% - (56/64)\n",
      "epoch 4/11 3392/15287 train loss 0.3900 acc : 85.94% - (55/64)\n",
      "epoch 4/11 3456/15287 train loss 0.4509 acc : 87.50% - (56/64)\n",
      "epoch 4/11 3520/15287 train loss 0.3315 acc : 90.62% - (58/64)\n",
      "epoch 4/11 3584/15287 train loss 0.5033 acc : 89.06% - (57/64)\n",
      "epoch 4/11 3648/15287 train loss 0.4953 acc : 89.06% - (57/64)\n",
      "epoch 4/11 3712/15287 train loss 0.3024 acc : 92.19% - (59/64)\n",
      "epoch 4/11 3776/15287 train loss 0.5817 acc : 79.69% - (51/64)\n",
      "epoch 4/11 3840/15287 train loss 0.4795 acc : 93.75% - (60/64)\n",
      "epoch 4/11 3904/15287 train loss 0.3420 acc : 89.06% - (57/64)\n",
      "epoch 4/11 3968/15287 train loss 0.4964 acc : 84.38% - (54/64)\n",
      "epoch 4/11 4032/15287 train loss 0.3436 acc : 89.06% - (57/64)\n",
      "epoch 4/11 4096/15287 train loss 0.4785 acc : 85.94% - (55/64)\n",
      "epoch 4/11 4160/15287 train loss 0.5079 acc : 85.94% - (55/64)\n",
      "epoch 4/11 4224/15287 train loss 0.2945 acc : 93.75% - (60/64)\n",
      "epoch 4/11 4288/15287 train loss 0.3557 acc : 92.19% - (59/64)\n",
      "epoch 4/11 4352/15287 train loss 0.2496 acc : 95.31% - (61/64)\n",
      "epoch 4/11 4416/15287 train loss 0.4498 acc : 85.94% - (55/64)\n",
      "epoch 4/11 4480/15287 train loss 0.3530 acc : 90.62% - (58/64)\n",
      "epoch 4/11 4544/15287 train loss 0.3627 acc : 90.62% - (58/64)\n",
      "epoch 4/11 4608/15287 train loss 0.4806 acc : 90.62% - (58/64)\n",
      "epoch 4/11 4672/15287 train loss 0.5918 acc : 81.25% - (52/64)\n",
      "epoch 4/11 4736/15287 train loss 0.4066 acc : 90.62% - (58/64)\n",
      "epoch 4/11 4800/15287 train loss 0.3710 acc : 90.62% - (58/64)\n",
      "epoch 4/11 4864/15287 train loss 0.4174 acc : 84.38% - (54/64)\n",
      "epoch 4/11 4928/15287 train loss 0.3876 acc : 87.50% - (56/64)\n",
      "epoch 4/11 4992/15287 train loss 0.5453 acc : 85.94% - (55/64)\n",
      "epoch 4/11 5056/15287 train loss 0.4352 acc : 87.50% - (56/64)\n",
      "epoch 4/11 5120/15287 train loss 0.3780 acc : 87.50% - (56/64)\n",
      "epoch 4/11 5184/15287 train loss 0.3260 acc : 89.06% - (57/64)\n",
      "epoch 4/11 5248/15287 train loss 0.3290 acc : 90.62% - (58/64)\n",
      "epoch 4/11 5312/15287 train loss 0.4301 acc : 90.62% - (58/64)\n",
      "epoch 4/11 5376/15287 train loss 0.5001 acc : 89.06% - (57/64)\n",
      "epoch 4/11 5440/15287 train loss 0.3838 acc : 90.62% - (58/64)\n",
      "epoch 4/11 5504/15287 train loss 0.3310 acc : 92.19% - (59/64)\n",
      "epoch 4/11 5568/15287 train loss 0.6587 acc : 82.81% - (53/64)\n",
      "epoch 4/11 5632/15287 train loss 0.2783 acc : 95.31% - (61/64)\n",
      "epoch 4/11 5696/15287 train loss 0.4257 acc : 84.38% - (54/64)\n",
      "epoch 4/11 5760/15287 train loss 0.3896 acc : 89.06% - (57/64)\n",
      "epoch 4/11 5824/15287 train loss 0.2971 acc : 93.75% - (60/64)\n",
      "epoch 4/11 5888/15287 train loss 0.4846 acc : 87.50% - (56/64)\n",
      "epoch 4/11 5952/15287 train loss 0.2917 acc : 92.19% - (59/64)\n",
      "epoch 4/11 6016/15287 train loss 0.4925 acc : 90.62% - (58/64)\n",
      "epoch 4/11 6080/15287 train loss 0.4444 acc : 85.94% - (55/64)\n",
      "epoch 4/11 6144/15287 train loss 0.4691 acc : 85.94% - (55/64)\n",
      "epoch 4/11 6208/15287 train loss 0.4206 acc : 87.50% - (56/64)\n",
      "epoch 4/11 6272/15287 train loss 0.2223 acc : 96.88% - (62/64)\n",
      "epoch 4/11 6336/15287 train loss 0.3342 acc : 93.75% - (60/64)\n",
      "epoch 4/11 6400/15287 train loss 0.4379 acc : 90.62% - (58/64)\n",
      "epoch 4/11 6464/15287 train loss 0.3837 acc : 90.62% - (58/64)\n",
      "epoch 4/11 6528/15287 train loss 0.3830 acc : 90.62% - (58/64)\n",
      "epoch 4/11 6592/15287 train loss 0.3665 acc : 89.06% - (57/64)\n",
      "epoch 4/11 6656/15287 train loss 0.4141 acc : 89.06% - (57/64)\n",
      "epoch 4/11 6720/15287 train loss 0.3168 acc : 87.50% - (56/64)\n",
      "epoch 4/11 6784/15287 train loss 0.4227 acc : 93.75% - (60/64)\n",
      "epoch 4/11 6848/15287 train loss 0.4386 acc : 90.62% - (58/64)\n",
      "epoch 4/11 6912/15287 train loss 0.2750 acc : 92.19% - (59/64)\n",
      "epoch 4/11 6976/15287 train loss 0.3876 acc : 93.75% - (60/64)\n",
      "epoch 4/11 7040/15287 train loss 0.4213 acc : 89.06% - (57/64)\n",
      "epoch 4/11 7104/15287 train loss 0.2538 acc : 95.31% - (61/64)\n",
      "epoch 4/11 7168/15287 train loss 0.3789 acc : 92.19% - (59/64)\n",
      "epoch 4/11 7232/15287 train loss 0.3223 acc : 89.06% - (57/64)\n",
      "epoch 4/11 7296/15287 train loss 0.3616 acc : 89.06% - (57/64)\n",
      "epoch 4/11 7360/15287 train loss 0.3413 acc : 92.19% - (59/64)\n",
      "epoch 4/11 7424/15287 train loss 0.4237 acc : 85.94% - (55/64)\n",
      "epoch 4/11 7488/15287 train loss 0.3699 acc : 90.62% - (58/64)\n",
      "epoch 4/11 7552/15287 train loss 0.3571 acc : 89.06% - (57/64)\n",
      "epoch 4/11 7616/15287 train loss 0.1915 acc : 100.00% - (64/64)\n",
      "epoch 4/11 7680/15287 train loss 0.2586 acc : 92.19% - (59/64)\n",
      "epoch 4/11 7744/15287 train loss 0.3884 acc : 90.62% - (58/64)\n",
      "epoch 4/11 7808/15287 train loss 0.4617 acc : 87.50% - (56/64)\n",
      "epoch 4/11 7872/15287 train loss 0.2257 acc : 95.31% - (61/64)\n",
      "epoch 4/11 7936/15287 train loss 0.3748 acc : 92.19% - (59/64)\n",
      "epoch 4/11 8000/15287 train loss 0.3685 acc : 92.19% - (59/64)\n",
      "epoch 4/11 8064/15287 train loss 0.3231 acc : 92.19% - (59/64)\n",
      "epoch 4/11 8128/15287 train loss 0.5053 acc : 85.94% - (55/64)\n",
      "epoch 4/11 8192/15287 train loss 0.2537 acc : 93.75% - (60/64)\n",
      "epoch 4/11 8256/15287 train loss 0.3860 acc : 89.06% - (57/64)\n",
      "epoch 4/11 8320/15287 train loss 0.4209 acc : 90.62% - (58/64)\n",
      "epoch 4/11 8384/15287 train loss 0.4179 acc : 92.19% - (59/64)\n",
      "epoch 4/11 8448/15287 train loss 0.2712 acc : 93.75% - (60/64)\n",
      "epoch 4/11 8512/15287 train loss 0.2796 acc : 93.75% - (60/64)\n",
      "epoch 4/11 8576/15287 train loss 0.3461 acc : 93.75% - (60/64)\n",
      "epoch 4/11 8640/15287 train loss 0.5616 acc : 82.81% - (53/64)\n",
      "epoch 4/11 8704/15287 train loss 0.3054 acc : 92.19% - (59/64)\n",
      "epoch 4/11 8768/15287 train loss 0.2015 acc : 96.88% - (62/64)\n",
      "epoch 4/11 8832/15287 train loss 0.4008 acc : 92.19% - (59/64)\n",
      "epoch 4/11 8896/15287 train loss 0.2352 acc : 95.31% - (61/64)\n",
      "epoch 4/11 8960/15287 train loss 0.4061 acc : 85.94% - (55/64)\n",
      "epoch 4/11 9024/15287 train loss 0.4168 acc : 87.50% - (56/64)\n",
      "epoch 4/11 9088/15287 train loss 0.2461 acc : 95.31% - (61/64)\n",
      "epoch 4/11 9152/15287 train loss 0.3790 acc : 87.50% - (56/64)\n",
      "epoch 4/11 9216/15287 train loss 0.2423 acc : 93.75% - (60/64)\n",
      "epoch 4/11 9280/15287 train loss 0.2685 acc : 98.44% - (63/64)\n",
      "epoch 4/11 9344/15287 train loss 0.2558 acc : 96.88% - (62/64)\n",
      "epoch 4/11 9408/15287 train loss 0.2847 acc : 90.62% - (58/64)\n",
      "epoch 4/11 9472/15287 train loss 0.3485 acc : 92.19% - (59/64)\n",
      "epoch 4/11 9536/15287 train loss 0.4598 acc : 82.81% - (53/64)\n",
      "epoch 4/11 9600/15287 train loss 0.3113 acc : 93.75% - (60/64)\n",
      "epoch 4/11 9664/15287 train loss 0.3755 acc : 93.75% - (60/64)\n",
      "epoch 4/11 9728/15287 train loss 0.2185 acc : 96.88% - (62/64)\n",
      "epoch 4/11 9792/15287 train loss 0.4173 acc : 87.50% - (56/64)\n",
      "epoch 4/11 9856/15287 train loss 0.2976 acc : 95.31% - (61/64)\n",
      "epoch 4/11 9920/15287 train loss 0.3231 acc : 92.19% - (59/64)\n",
      "epoch 4/11 9984/15287 train loss 0.2127 acc : 96.88% - (62/64)\n",
      "epoch 4/11 10048/15287 train loss 0.4000 acc : 84.38% - (54/64)\n",
      "epoch 4/11 10112/15287 train loss 0.2963 acc : 89.06% - (57/64)\n",
      "epoch 4/11 10176/15287 train loss 0.4012 acc : 84.38% - (54/64)\n",
      "epoch 4/11 10240/15287 train loss 0.3331 acc : 93.75% - (60/64)\n",
      "epoch 4/11 10304/15287 train loss 0.3366 acc : 93.75% - (60/64)\n",
      "epoch 4/11 10368/15287 train loss 0.3707 acc : 89.06% - (57/64)\n",
      "epoch 4/11 10432/15287 train loss 0.4142 acc : 89.06% - (57/64)\n",
      "epoch 4/11 10496/15287 train loss 0.2081 acc : 95.31% - (61/64)\n",
      "epoch 4/11 10560/15287 train loss 0.5213 acc : 84.38% - (54/64)\n",
      "epoch 4/11 10624/15287 train loss 0.4677 acc : 87.50% - (56/64)\n",
      "epoch 4/11 10688/15287 train loss 0.2371 acc : 95.31% - (61/64)\n",
      "epoch 4/11 10752/15287 train loss 0.2203 acc : 95.31% - (61/64)\n",
      "epoch 4/11 10816/15287 train loss 0.4766 acc : 90.62% - (58/64)\n",
      "epoch 4/11 10880/15287 train loss 0.4158 acc : 89.06% - (57/64)\n",
      "epoch 4/11 10944/15287 train loss 0.4746 acc : 85.94% - (55/64)\n",
      "epoch 4/11 11008/15287 train loss 0.2735 acc : 96.88% - (62/64)\n",
      "epoch 4/11 11072/15287 train loss 0.2250 acc : 93.75% - (60/64)\n",
      "epoch 4/11 11136/15287 train loss 0.2159 acc : 95.31% - (61/64)\n",
      "epoch 4/11 11200/15287 train loss 0.2981 acc : 93.75% - (60/64)\n",
      "epoch 4/11 11264/15287 train loss 0.2699 acc : 90.62% - (58/64)\n",
      "epoch 4/11 11328/15287 train loss 0.2835 acc : 93.75% - (60/64)\n",
      "epoch 4/11 11392/15287 train loss 0.3390 acc : 93.75% - (60/64)\n",
      "epoch 4/11 11456/15287 train loss 0.3145 acc : 95.31% - (61/64)\n",
      "epoch 4/11 11520/15287 train loss 0.1854 acc : 95.31% - (61/64)\n",
      "epoch 4/11 11584/15287 train loss 0.2479 acc : 93.75% - (60/64)\n",
      "epoch 4/11 11648/15287 train loss 0.3200 acc : 93.75% - (60/64)\n",
      "epoch 4/11 11712/15287 train loss 0.1491 acc : 95.31% - (61/64)\n",
      "epoch 4/11 11776/15287 train loss 0.2933 acc : 95.31% - (61/64)\n",
      "epoch 4/11 11840/15287 train loss 0.3675 acc : 85.94% - (55/64)\n",
      "epoch 4/11 11904/15287 train loss 0.3887 acc : 93.75% - (60/64)\n",
      "epoch 4/11 11968/15287 train loss 0.3308 acc : 92.19% - (59/64)\n",
      "epoch 4/11 12032/15287 train loss 0.2910 acc : 95.31% - (61/64)\n",
      "epoch 4/11 12096/15287 train loss 0.4092 acc : 90.62% - (58/64)\n",
      "epoch 4/11 12160/15287 train loss 0.2832 acc : 95.31% - (61/64)\n",
      "epoch 4/11 12224/15287 train loss 0.5792 acc : 82.81% - (53/64)\n",
      "epoch 4/11 12288/15287 train loss 0.5166 acc : 90.62% - (58/64)\n",
      "epoch 4/11 12352/15287 train loss 0.3041 acc : 98.44% - (63/64)\n",
      "epoch 4/11 12416/15287 train loss 0.3687 acc : 95.31% - (61/64)\n",
      "epoch 4/11 12480/15287 train loss 0.3168 acc : 92.19% - (59/64)\n",
      "epoch 4/11 12544/15287 train loss 0.4159 acc : 89.06% - (57/64)\n",
      "epoch 4/11 12608/15287 train loss 0.2724 acc : 93.75% - (60/64)\n",
      "epoch 4/11 12672/15287 train loss 0.2222 acc : 93.75% - (60/64)\n",
      "epoch 4/11 12736/15287 train loss 0.2721 acc : 95.31% - (61/64)\n",
      "epoch 4/11 12800/15287 train loss 0.3012 acc : 93.75% - (60/64)\n",
      "epoch 4/11 12864/15287 train loss 0.3188 acc : 90.62% - (58/64)\n",
      "epoch 4/11 12928/15287 train loss 0.3025 acc : 96.88% - (62/64)\n",
      "epoch 4/11 12992/15287 train loss 0.3088 acc : 95.31% - (61/64)\n",
      "epoch 4/11 13056/15287 train loss 0.2362 acc : 92.19% - (59/64)\n",
      "epoch 4/11 13120/15287 train loss 0.5283 acc : 84.38% - (54/64)\n",
      "epoch 4/11 13184/15287 train loss 0.3185 acc : 90.62% - (58/64)\n",
      "epoch 4/11 13248/15287 train loss 0.3989 acc : 92.19% - (59/64)\n",
      "epoch 4/11 13312/15287 train loss 0.2284 acc : 93.75% - (60/64)\n",
      "epoch 4/11 13376/15287 train loss 0.1829 acc : 98.44% - (63/64)\n",
      "epoch 4/11 13440/15287 train loss 0.3166 acc : 92.19% - (59/64)\n",
      "epoch 4/11 13504/15287 train loss 0.3671 acc : 90.62% - (58/64)\n",
      "epoch 4/11 13568/15287 train loss 0.2803 acc : 92.19% - (59/64)\n",
      "epoch 4/11 13632/15287 train loss 0.4060 acc : 90.62% - (58/64)\n",
      "epoch 4/11 13696/15287 train loss 0.4036 acc : 90.62% - (58/64)\n",
      "epoch 4/11 13760/15287 train loss 0.4433 acc : 89.06% - (57/64)\n",
      "epoch 4/11 13824/15287 train loss 0.3679 acc : 90.62% - (58/64)\n",
      "epoch 4/11 13888/15287 train loss 0.4637 acc : 87.50% - (56/64)\n",
      "epoch 4/11 13952/15287 train loss 0.3611 acc : 89.06% - (57/64)\n",
      "epoch 4/11 14016/15287 train loss 0.2271 acc : 96.88% - (62/64)\n",
      "epoch 4/11 14080/15287 train loss 0.2328 acc : 96.88% - (62/64)\n",
      "epoch 4/11 14144/15287 train loss 0.2676 acc : 96.88% - (62/64)\n",
      "epoch 4/11 14208/15287 train loss 0.4099 acc : 87.50% - (56/64)\n",
      "epoch 4/11 14272/15287 train loss 0.3046 acc : 93.75% - (60/64)\n",
      "epoch 4/11 14336/15287 train loss 0.3731 acc : 89.06% - (57/64)\n",
      "epoch 4/11 14400/15287 train loss 0.2711 acc : 92.19% - (59/64)\n",
      "epoch 4/11 14464/15287 train loss 0.3551 acc : 90.62% - (58/64)\n",
      "epoch 4/11 14528/15287 train loss 0.2994 acc : 92.19% - (59/64)\n",
      "epoch 4/11 14592/15287 train loss 0.3571 acc : 89.06% - (57/64)\n",
      "epoch 4/11 14656/15287 train loss 0.2557 acc : 95.31% - (61/64)\n",
      "epoch 4/11 14720/15287 train loss 0.3268 acc : 92.19% - (59/64)\n",
      "epoch 4/11 14784/15287 train loss 0.2839 acc : 92.19% - (59/64)\n",
      "epoch 4/11 14848/15287 train loss 0.4652 acc : 90.62% - (58/64)\n",
      "epoch 4/11 14912/15287 train loss 0.3554 acc : 93.75% - (60/64)\n",
      "epoch 4/11 14976/15287 train loss 0.4969 acc : 87.50% - (56/64)\n",
      "epoch 4/11 15040/15287 train loss 0.3013 acc : 93.75% - (60/64)\n",
      "epoch 4/11 15104/15287 train loss 0.4719 acc : 87.50% - (56/64)\n",
      "epoch 4/11 15168/15287 train loss 0.3894 acc : 89.06% - (57/64)\n",
      "epoch 4/11 15232/15287 train loss 0.2353 acc : 96.88% - (62/64)\n",
      "epoch 4/11 15287/15287 train loss 0.2306 acc : 90.91% - (50/55)\n",
      "\n",
      " epoch 4 train end!!! \t train batch loss : 0.3799\t total acc : 90.44% - (13826/15287) \n",
      "\n",
      "epoch 4 val end!!! val loss : 0.234 \t f1 score : 0.923 acc : 81.05% - (1377/1699) \n",
      "\n",
      "\n",
      "epoch 5/11 64/15287 train loss 0.2276 acc : 93.75% - (60/64)\n",
      "epoch 5/11 128/15287 train loss 0.3035 acc : 89.06% - (57/64)\n",
      "epoch 5/11 192/15287 train loss 0.2974 acc : 96.88% - (62/64)\n",
      "epoch 5/11 256/15287 train loss 0.2251 acc : 92.19% - (59/64)\n",
      "epoch 5/11 320/15287 train loss 0.3730 acc : 95.31% - (61/64)\n",
      "epoch 5/11 384/15287 train loss 0.2487 acc : 96.88% - (62/64)\n",
      "epoch 5/11 448/15287 train loss 0.2589 acc : 93.75% - (60/64)\n",
      "epoch 5/11 512/15287 train loss 0.4002 acc : 92.19% - (59/64)\n",
      "epoch 5/11 576/15287 train loss 0.3549 acc : 92.19% - (59/64)\n",
      "epoch 5/11 640/15287 train loss 0.2891 acc : 90.62% - (58/64)\n",
      "epoch 5/11 704/15287 train loss 0.4007 acc : 92.19% - (59/64)\n",
      "epoch 5/11 768/15287 train loss 0.2234 acc : 95.31% - (61/64)\n",
      "epoch 5/11 832/15287 train loss 0.2291 acc : 95.31% - (61/64)\n",
      "epoch 5/11 896/15287 train loss 0.3194 acc : 90.62% - (58/64)\n",
      "epoch 5/11 960/15287 train loss 0.3476 acc : 95.31% - (61/64)\n",
      "epoch 5/11 1024/15287 train loss 0.2443 acc : 95.31% - (61/64)\n",
      "epoch 5/11 1088/15287 train loss 0.1995 acc : 96.88% - (62/64)\n",
      "epoch 5/11 1152/15287 train loss 0.4089 acc : 89.06% - (57/64)\n",
      "epoch 5/11 1216/15287 train loss 0.2376 acc : 95.31% - (61/64)\n",
      "epoch 5/11 1280/15287 train loss 0.2829 acc : 92.19% - (59/64)\n",
      "epoch 5/11 1344/15287 train loss 0.3973 acc : 89.06% - (57/64)\n",
      "epoch 5/11 1408/15287 train loss 0.3545 acc : 92.19% - (59/64)\n",
      "epoch 5/11 1472/15287 train loss 0.2841 acc : 93.75% - (60/64)\n",
      "epoch 5/11 1536/15287 train loss 0.2558 acc : 95.31% - (61/64)\n",
      "epoch 5/11 1600/15287 train loss 0.3449 acc : 93.75% - (60/64)\n",
      "epoch 5/11 1664/15287 train loss 0.1949 acc : 95.31% - (61/64)\n",
      "epoch 5/11 1728/15287 train loss 0.2351 acc : 93.75% - (60/64)\n",
      "epoch 5/11 1792/15287 train loss 0.2498 acc : 96.88% - (62/64)\n",
      "epoch 5/11 1856/15287 train loss 0.2500 acc : 93.75% - (60/64)\n",
      "epoch 5/11 1920/15287 train loss 0.2143 acc : 95.31% - (61/64)\n",
      "epoch 5/11 1984/15287 train loss 0.3264 acc : 92.19% - (59/64)\n",
      "epoch 5/11 2048/15287 train loss 0.2855 acc : 93.75% - (60/64)\n",
      "epoch 5/11 2112/15287 train loss 0.1280 acc : 98.44% - (63/64)\n",
      "epoch 5/11 2176/15287 train loss 0.4418 acc : 89.06% - (57/64)\n",
      "epoch 5/11 2240/15287 train loss 0.2576 acc : 95.31% - (61/64)\n",
      "epoch 5/11 2304/15287 train loss 0.3597 acc : 92.19% - (59/64)\n",
      "epoch 5/11 2368/15287 train loss 0.2601 acc : 95.31% - (61/64)\n",
      "epoch 5/11 2432/15287 train loss 0.2089 acc : 96.88% - (62/64)\n",
      "epoch 5/11 2496/15287 train loss 0.2950 acc : 90.62% - (58/64)\n",
      "epoch 5/11 2560/15287 train loss 0.2219 acc : 95.31% - (61/64)\n",
      "epoch 5/11 2624/15287 train loss 0.2185 acc : 93.75% - (60/64)\n",
      "epoch 5/11 2688/15287 train loss 0.2103 acc : 95.31% - (61/64)\n",
      "epoch 5/11 2752/15287 train loss 0.2958 acc : 93.75% - (60/64)\n",
      "epoch 5/11 2816/15287 train loss 0.1632 acc : 96.88% - (62/64)\n",
      "epoch 5/11 2880/15287 train loss 0.2890 acc : 89.06% - (57/64)\n",
      "epoch 5/11 2944/15287 train loss 0.3269 acc : 95.31% - (61/64)\n",
      "epoch 5/11 3008/15287 train loss 0.2261 acc : 95.31% - (61/64)\n",
      "epoch 5/11 3072/15287 train loss 0.1489 acc : 98.44% - (63/64)\n",
      "epoch 5/11 3136/15287 train loss 0.2667 acc : 92.19% - (59/64)\n",
      "epoch 5/11 3200/15287 train loss 0.2444 acc : 93.75% - (60/64)\n",
      "epoch 5/11 3264/15287 train loss 0.2507 acc : 95.31% - (61/64)\n",
      "epoch 5/11 3328/15287 train loss 0.1767 acc : 98.44% - (63/64)\n",
      "epoch 5/11 3392/15287 train loss 0.2184 acc : 90.62% - (58/64)\n",
      "epoch 5/11 3456/15287 train loss 0.2805 acc : 92.19% - (59/64)\n",
      "epoch 5/11 3520/15287 train loss 0.1861 acc : 96.88% - (62/64)\n",
      "epoch 5/11 3584/15287 train loss 0.3042 acc : 93.75% - (60/64)\n",
      "epoch 5/11 3648/15287 train loss 0.3115 acc : 92.19% - (59/64)\n",
      "epoch 5/11 3712/15287 train loss 0.1676 acc : 96.88% - (62/64)\n",
      "epoch 5/11 3776/15287 train loss 0.3413 acc : 89.06% - (57/64)\n",
      "epoch 5/11 3840/15287 train loss 0.2790 acc : 95.31% - (61/64)\n",
      "epoch 5/11 3904/15287 train loss 0.1906 acc : 93.75% - (60/64)\n",
      "epoch 5/11 3968/15287 train loss 0.2778 acc : 92.19% - (59/64)\n",
      "epoch 5/11 4032/15287 train loss 0.2081 acc : 93.75% - (60/64)\n",
      "epoch 5/11 4096/15287 train loss 0.2893 acc : 93.75% - (60/64)\n",
      "epoch 5/11 4160/15287 train loss 0.3281 acc : 93.75% - (60/64)\n",
      "epoch 5/11 4224/15287 train loss 0.1807 acc : 95.31% - (61/64)\n",
      "epoch 5/11 4288/15287 train loss 0.1794 acc : 98.44% - (63/64)\n",
      "epoch 5/11 4352/15287 train loss 0.1277 acc : 96.88% - (62/64)\n",
      "epoch 5/11 4416/15287 train loss 0.2394 acc : 92.19% - (59/64)\n",
      "epoch 5/11 4480/15287 train loss 0.1974 acc : 96.88% - (62/64)\n",
      "epoch 5/11 4544/15287 train loss 0.1852 acc : 96.88% - (62/64)\n",
      "epoch 5/11 4608/15287 train loss 0.3414 acc : 92.19% - (59/64)\n",
      "epoch 5/11 4672/15287 train loss 0.3375 acc : 90.62% - (58/64)\n",
      "epoch 5/11 4736/15287 train loss 0.2540 acc : 92.19% - (59/64)\n",
      "epoch 5/11 4800/15287 train loss 0.1530 acc : 95.31% - (61/64)\n",
      "epoch 5/11 4864/15287 train loss 0.2312 acc : 93.75% - (60/64)\n",
      "epoch 5/11 4928/15287 train loss 0.2233 acc : 95.31% - (61/64)\n",
      "epoch 5/11 4992/15287 train loss 0.2985 acc : 96.88% - (62/64)\n",
      "epoch 5/11 5056/15287 train loss 0.2957 acc : 92.19% - (59/64)\n",
      "epoch 5/11 5120/15287 train loss 0.2298 acc : 95.31% - (61/64)\n",
      "epoch 5/11 5184/15287 train loss 0.1570 acc : 98.44% - (63/64)\n",
      "epoch 5/11 5248/15287 train loss 0.2042 acc : 95.31% - (61/64)\n",
      "epoch 5/11 5312/15287 train loss 0.2878 acc : 95.31% - (61/64)\n",
      "epoch 5/11 5376/15287 train loss 0.3057 acc : 95.31% - (61/64)\n",
      "epoch 5/11 5440/15287 train loss 0.2417 acc : 92.19% - (59/64)\n",
      "epoch 5/11 5504/15287 train loss 0.2144 acc : 95.31% - (61/64)\n",
      "epoch 5/11 5568/15287 train loss 0.4469 acc : 87.50% - (56/64)\n",
      "epoch 5/11 5632/15287 train loss 0.1406 acc : 100.00% - (64/64)\n",
      "epoch 5/11 5696/15287 train loss 0.2269 acc : 93.75% - (60/64)\n",
      "epoch 5/11 5760/15287 train loss 0.2130 acc : 95.31% - (61/64)\n",
      "epoch 5/11 5824/15287 train loss 0.1678 acc : 98.44% - (63/64)\n",
      "epoch 5/11 5888/15287 train loss 0.2468 acc : 93.75% - (60/64)\n",
      "epoch 5/11 5952/15287 train loss 0.1281 acc : 100.00% - (64/64)\n",
      "epoch 5/11 6016/15287 train loss 0.3381 acc : 95.31% - (61/64)\n",
      "epoch 5/11 6080/15287 train loss 0.2929 acc : 93.75% - (60/64)\n",
      "epoch 5/11 6144/15287 train loss 0.2521 acc : 92.19% - (59/64)\n",
      "epoch 5/11 6208/15287 train loss 0.2303 acc : 95.31% - (61/64)\n",
      "epoch 5/11 6272/15287 train loss 0.1297 acc : 98.44% - (63/64)\n",
      "epoch 5/11 6336/15287 train loss 0.1957 acc : 96.88% - (62/64)\n",
      "epoch 5/11 6400/15287 train loss 0.2873 acc : 95.31% - (61/64)\n",
      "epoch 5/11 6464/15287 train loss 0.1862 acc : 96.88% - (62/64)\n",
      "epoch 5/11 6528/15287 train loss 0.2702 acc : 95.31% - (61/64)\n",
      "epoch 5/11 6592/15287 train loss 0.2025 acc : 95.31% - (61/64)\n",
      "epoch 5/11 6656/15287 train loss 0.2205 acc : 93.75% - (60/64)\n",
      "epoch 5/11 6720/15287 train loss 0.1686 acc : 96.88% - (62/64)\n",
      "epoch 5/11 6784/15287 train loss 0.2206 acc : 95.31% - (61/64)\n",
      "epoch 5/11 6848/15287 train loss 0.2409 acc : 95.31% - (61/64)\n",
      "epoch 5/11 6912/15287 train loss 0.1497 acc : 98.44% - (63/64)\n",
      "epoch 5/11 6976/15287 train loss 0.1981 acc : 96.88% - (62/64)\n",
      "epoch 5/11 7040/15287 train loss 0.2199 acc : 95.31% - (61/64)\n",
      "epoch 5/11 7104/15287 train loss 0.1065 acc : 98.44% - (63/64)\n",
      "epoch 5/11 7168/15287 train loss 0.2545 acc : 95.31% - (61/64)\n",
      "epoch 5/11 7232/15287 train loss 0.1709 acc : 96.88% - (62/64)\n",
      "epoch 5/11 7296/15287 train loss 0.2178 acc : 90.62% - (58/64)\n",
      "epoch 5/11 7360/15287 train loss 0.1672 acc : 98.44% - (63/64)\n",
      "epoch 5/11 7424/15287 train loss 0.2531 acc : 93.75% - (60/64)\n",
      "epoch 5/11 7488/15287 train loss 0.2123 acc : 96.88% - (62/64)\n",
      "epoch 5/11 7552/15287 train loss 0.1950 acc : 93.75% - (60/64)\n",
      "epoch 5/11 7616/15287 train loss 0.0956 acc : 100.00% - (64/64)\n",
      "epoch 5/11 7680/15287 train loss 0.1440 acc : 98.44% - (63/64)\n",
      "epoch 5/11 7744/15287 train loss 0.2430 acc : 93.75% - (60/64)\n",
      "epoch 5/11 7808/15287 train loss 0.2756 acc : 92.19% - (59/64)\n",
      "epoch 5/11 7872/15287 train loss 0.1345 acc : 96.88% - (62/64)\n",
      "epoch 5/11 7936/15287 train loss 0.1817 acc : 98.44% - (63/64)\n",
      "epoch 5/11 8000/15287 train loss 0.2074 acc : 95.31% - (61/64)\n",
      "epoch 5/11 8064/15287 train loss 0.2008 acc : 95.31% - (61/64)\n",
      "epoch 5/11 8128/15287 train loss 0.3111 acc : 89.06% - (57/64)\n",
      "epoch 5/11 8192/15287 train loss 0.1529 acc : 96.88% - (62/64)\n",
      "epoch 5/11 8256/15287 train loss 0.2030 acc : 95.31% - (61/64)\n",
      "epoch 5/11 8320/15287 train loss 0.2032 acc : 96.88% - (62/64)\n",
      "epoch 5/11 8384/15287 train loss 0.2692 acc : 93.75% - (60/64)\n",
      "epoch 5/11 8448/15287 train loss 0.1356 acc : 96.88% - (62/64)\n",
      "epoch 5/11 8512/15287 train loss 0.1364 acc : 100.00% - (64/64)\n",
      "epoch 5/11 8576/15287 train loss 0.2197 acc : 95.31% - (61/64)\n",
      "epoch 5/11 8640/15287 train loss 0.2921 acc : 92.19% - (59/64)\n",
      "epoch 5/11 8704/15287 train loss 0.1666 acc : 93.75% - (60/64)\n",
      "epoch 5/11 8768/15287 train loss 0.0987 acc : 98.44% - (63/64)\n",
      "epoch 5/11 8832/15287 train loss 0.2526 acc : 95.31% - (61/64)\n",
      "epoch 5/11 8896/15287 train loss 0.1205 acc : 100.00% - (64/64)\n",
      "epoch 5/11 8960/15287 train loss 0.2215 acc : 95.31% - (61/64)\n",
      "epoch 5/11 9024/15287 train loss 0.2449 acc : 93.75% - (60/64)\n",
      "epoch 5/11 9088/15287 train loss 0.1161 acc : 100.00% - (64/64)\n",
      "epoch 5/11 9152/15287 train loss 0.2221 acc : 92.19% - (59/64)\n",
      "epoch 5/11 9216/15287 train loss 0.1442 acc : 98.44% - (63/64)\n",
      "epoch 5/11 9280/15287 train loss 0.1790 acc : 95.31% - (61/64)\n",
      "epoch 5/11 9344/15287 train loss 0.1513 acc : 96.88% - (62/64)\n",
      "epoch 5/11 9408/15287 train loss 0.1459 acc : 98.44% - (63/64)\n",
      "epoch 5/11 9472/15287 train loss 0.1794 acc : 96.88% - (62/64)\n",
      "epoch 5/11 9536/15287 train loss 0.2814 acc : 90.62% - (58/64)\n",
      "epoch 5/11 9600/15287 train loss 0.1810 acc : 96.88% - (62/64)\n",
      "epoch 5/11 9664/15287 train loss 0.1961 acc : 96.88% - (62/64)\n",
      "epoch 5/11 9728/15287 train loss 0.1170 acc : 98.44% - (63/64)\n",
      "epoch 5/11 9792/15287 train loss 0.2099 acc : 96.88% - (62/64)\n",
      "epoch 5/11 9856/15287 train loss 0.1959 acc : 96.88% - (62/64)\n",
      "epoch 5/11 9920/15287 train loss 0.1717 acc : 96.88% - (62/64)\n",
      "epoch 5/11 9984/15287 train loss 0.1093 acc : 98.44% - (63/64)\n",
      "epoch 5/11 10048/15287 train loss 0.2406 acc : 93.75% - (60/64)\n",
      "epoch 5/11 10112/15287 train loss 0.1594 acc : 98.44% - (63/64)\n",
      "epoch 5/11 10176/15287 train loss 0.2322 acc : 92.19% - (59/64)\n",
      "epoch 5/11 10240/15287 train loss 0.1898 acc : 93.75% - (60/64)\n",
      "epoch 5/11 10304/15287 train loss 0.1737 acc : 98.44% - (63/64)\n",
      "epoch 5/11 10368/15287 train loss 0.2008 acc : 98.44% - (63/64)\n",
      "epoch 5/11 10432/15287 train loss 0.2300 acc : 95.31% - (61/64)\n",
      "epoch 5/11 10496/15287 train loss 0.1168 acc : 96.88% - (62/64)\n",
      "epoch 5/11 10560/15287 train loss 0.2752 acc : 93.75% - (60/64)\n",
      "epoch 5/11 10624/15287 train loss 0.2741 acc : 93.75% - (60/64)\n",
      "epoch 5/11 10688/15287 train loss 0.1370 acc : 96.88% - (62/64)\n",
      "epoch 5/11 10752/15287 train loss 0.1223 acc : 100.00% - (64/64)\n",
      "epoch 5/11 10816/15287 train loss 0.2428 acc : 93.75% - (60/64)\n",
      "epoch 5/11 10880/15287 train loss 0.2209 acc : 96.88% - (62/64)\n",
      "epoch 5/11 10944/15287 train loss 0.2738 acc : 93.75% - (60/64)\n",
      "epoch 5/11 11008/15287 train loss 0.1275 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11072/15287 train loss 0.1085 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11136/15287 train loss 0.1119 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11200/15287 train loss 0.1572 acc : 96.88% - (62/64)\n",
      "epoch 5/11 11264/15287 train loss 0.1343 acc : 96.88% - (62/64)\n",
      "epoch 5/11 11328/15287 train loss 0.1285 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11392/15287 train loss 0.1457 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11456/15287 train loss 0.1530 acc : 96.88% - (62/64)\n",
      "epoch 5/11 11520/15287 train loss 0.1058 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11584/15287 train loss 0.1123 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11648/15287 train loss 0.1478 acc : 98.44% - (63/64)\n",
      "epoch 5/11 11712/15287 train loss 0.0716 acc : 100.00% - (64/64)\n",
      "epoch 5/11 11776/15287 train loss 0.1618 acc : 96.88% - (62/64)\n",
      "epoch 5/11 11840/15287 train loss 0.1554 acc : 95.31% - (61/64)\n",
      "epoch 5/11 11904/15287 train loss 0.2162 acc : 95.31% - (61/64)\n",
      "epoch 5/11 11968/15287 train loss 0.1948 acc : 96.88% - (62/64)\n",
      "epoch 5/11 12032/15287 train loss 0.1556 acc : 96.88% - (62/64)\n",
      "epoch 5/11 12096/15287 train loss 0.2309 acc : 93.75% - (60/64)\n",
      "epoch 5/11 12160/15287 train loss 0.1560 acc : 98.44% - (63/64)\n",
      "epoch 5/11 12224/15287 train loss 0.3255 acc : 90.62% - (58/64)\n",
      "epoch 5/11 12288/15287 train loss 0.3025 acc : 92.19% - (59/64)\n",
      "epoch 5/11 12352/15287 train loss 0.2038 acc : 98.44% - (63/64)\n",
      "epoch 5/11 12416/15287 train loss 0.1982 acc : 96.88% - (62/64)\n",
      "epoch 5/11 12480/15287 train loss 0.1550 acc : 98.44% - (63/64)\n",
      "epoch 5/11 12544/15287 train loss 0.2207 acc : 95.31% - (61/64)\n",
      "epoch 5/11 12608/15287 train loss 0.1178 acc : 100.00% - (64/64)\n",
      "epoch 5/11 12672/15287 train loss 0.1211 acc : 100.00% - (64/64)\n",
      "epoch 5/11 12736/15287 train loss 0.1233 acc : 98.44% - (63/64)\n",
      "epoch 5/11 12800/15287 train loss 0.1411 acc : 100.00% - (64/64)\n",
      "epoch 5/11 12864/15287 train loss 0.1846 acc : 96.88% - (62/64)\n",
      "epoch 5/11 12928/15287 train loss 0.1564 acc : 98.44% - (63/64)\n",
      "epoch 5/11 12992/15287 train loss 0.1779 acc : 98.44% - (63/64)\n",
      "epoch 5/11 13056/15287 train loss 0.1018 acc : 98.44% - (63/64)\n",
      "epoch 5/11 13120/15287 train loss 0.2875 acc : 89.06% - (57/64)\n",
      "epoch 5/11 13184/15287 train loss 0.1798 acc : 96.88% - (62/64)\n",
      "epoch 5/11 13248/15287 train loss 0.2218 acc : 95.31% - (61/64)\n",
      "epoch 5/11 13312/15287 train loss 0.1041 acc : 100.00% - (64/64)\n",
      "epoch 5/11 13376/15287 train loss 0.0803 acc : 100.00% - (64/64)\n",
      "epoch 5/11 13440/15287 train loss 0.1523 acc : 98.44% - (63/64)\n",
      "epoch 5/11 13504/15287 train loss 0.1802 acc : 95.31% - (61/64)\n",
      "epoch 5/11 13568/15287 train loss 0.1489 acc : 96.88% - (62/64)\n",
      "epoch 5/11 13632/15287 train loss 0.1919 acc : 98.44% - (63/64)\n",
      "epoch 5/11 13696/15287 train loss 0.2093 acc : 96.88% - (62/64)\n",
      "epoch 5/11 13760/15287 train loss 0.2773 acc : 92.19% - (59/64)\n",
      "epoch 5/11 13824/15287 train loss 0.1907 acc : 95.31% - (61/64)\n",
      "epoch 5/11 13888/15287 train loss 0.2691 acc : 93.75% - (60/64)\n",
      "epoch 5/11 13952/15287 train loss 0.1981 acc : 95.31% - (61/64)\n",
      "epoch 5/11 14016/15287 train loss 0.1406 acc : 96.88% - (62/64)\n",
      "epoch 5/11 14080/15287 train loss 0.1021 acc : 100.00% - (64/64)\n",
      "epoch 5/11 14144/15287 train loss 0.1767 acc : 96.88% - (62/64)\n",
      "epoch 5/11 14208/15287 train loss 0.1763 acc : 95.31% - (61/64)\n",
      "epoch 5/11 14272/15287 train loss 0.1508 acc : 95.31% - (61/64)\n",
      "epoch 5/11 14336/15287 train loss 0.2037 acc : 92.19% - (59/64)\n",
      "epoch 5/11 14400/15287 train loss 0.1574 acc : 96.88% - (62/64)\n",
      "epoch 5/11 14464/15287 train loss 0.2010 acc : 96.88% - (62/64)\n",
      "epoch 5/11 14528/15287 train loss 0.1537 acc : 96.88% - (62/64)\n",
      "epoch 5/11 14592/15287 train loss 0.1923 acc : 95.31% - (61/64)\n",
      "epoch 5/11 14656/15287 train loss 0.1105 acc : 98.44% - (63/64)\n",
      "epoch 5/11 14720/15287 train loss 0.1722 acc : 98.44% - (63/64)\n",
      "epoch 5/11 14784/15287 train loss 0.1483 acc : 98.44% - (63/64)\n",
      "epoch 5/11 14848/15287 train loss 0.2532 acc : 95.31% - (61/64)\n",
      "epoch 5/11 14912/15287 train loss 0.1841 acc : 98.44% - (63/64)\n",
      "epoch 5/11 14976/15287 train loss 0.3202 acc : 90.62% - (58/64)\n",
      "epoch 5/11 15040/15287 train loss 0.1652 acc : 96.88% - (62/64)\n",
      "epoch 5/11 15104/15287 train loss 0.3238 acc : 93.75% - (60/64)\n",
      "epoch 5/11 15168/15287 train loss 0.1887 acc : 96.88% - (62/64)\n",
      "epoch 5/11 15232/15287 train loss 0.0994 acc : 100.00% - (64/64)\n",
      "epoch 5/11 15287/15287 train loss 0.0972 acc : 100.00% - (55/55)\n",
      "\n",
      " epoch 5 train end!!! \t train batch loss : 0.2150\t total acc : 95.51% - (14601/15287) \n",
      "\n",
      "epoch 5 val end!!! val loss : 0.116 \t f1 score : 1.000 acc : 87.40% - (1485/1699) \n",
      "\n",
      "\n",
      "epoch 6/11 64/15287 train loss 0.1020 acc : 98.44% - (63/64)\n",
      "epoch 6/11 128/15287 train loss 0.1585 acc : 95.31% - (61/64)\n",
      "epoch 6/11 192/15287 train loss 0.1896 acc : 98.44% - (63/64)\n",
      "epoch 6/11 256/15287 train loss 0.1077 acc : 96.88% - (62/64)\n",
      "epoch 6/11 320/15287 train loss 0.2805 acc : 96.88% - (62/64)\n",
      "epoch 6/11 384/15287 train loss 0.1186 acc : 98.44% - (63/64)\n",
      "epoch 6/11 448/15287 train loss 0.1440 acc : 96.88% - (62/64)\n",
      "epoch 6/11 512/15287 train loss 0.1941 acc : 95.31% - (61/64)\n",
      "epoch 6/11 576/15287 train loss 0.2242 acc : 96.88% - (62/64)\n",
      "epoch 6/11 640/15287 train loss 0.1272 acc : 96.88% - (62/64)\n",
      "epoch 6/11 704/15287 train loss 0.1866 acc : 96.88% - (62/64)\n",
      "epoch 6/11 768/15287 train loss 0.1181 acc : 100.00% - (64/64)\n",
      "epoch 6/11 832/15287 train loss 0.1147 acc : 98.44% - (63/64)\n",
      "epoch 6/11 896/15287 train loss 0.1985 acc : 93.75% - (60/64)\n",
      "epoch 6/11 960/15287 train loss 0.2507 acc : 98.44% - (63/64)\n",
      "epoch 6/11 1024/15287 train loss 0.1266 acc : 98.44% - (63/64)\n",
      "epoch 6/11 1088/15287 train loss 0.1114 acc : 100.00% - (64/64)\n",
      "epoch 6/11 1152/15287 train loss 0.1947 acc : 95.31% - (61/64)\n",
      "epoch 6/11 1216/15287 train loss 0.1402 acc : 95.31% - (61/64)\n",
      "epoch 6/11 1280/15287 train loss 0.1726 acc : 96.88% - (62/64)\n",
      "epoch 6/11 1344/15287 train loss 0.2159 acc : 93.75% - (60/64)\n",
      "epoch 6/11 1408/15287 train loss 0.2015 acc : 95.31% - (61/64)\n",
      "epoch 6/11 1472/15287 train loss 0.0917 acc : 98.44% - (63/64)\n",
      "epoch 6/11 1536/15287 train loss 0.1305 acc : 98.44% - (63/64)\n",
      "epoch 6/11 1600/15287 train loss 0.2081 acc : 93.75% - (60/64)\n",
      "epoch 6/11 1664/15287 train loss 0.0893 acc : 100.00% - (64/64)\n",
      "epoch 6/11 1728/15287 train loss 0.1401 acc : 96.88% - (62/64)\n",
      "epoch 6/11 1792/15287 train loss 0.1452 acc : 98.44% - (63/64)\n",
      "epoch 6/11 1856/15287 train loss 0.1273 acc : 100.00% - (64/64)\n",
      "epoch 6/11 1920/15287 train loss 0.1115 acc : 98.44% - (63/64)\n",
      "epoch 6/11 1984/15287 train loss 0.1955 acc : 95.31% - (61/64)\n",
      "epoch 6/11 2048/15287 train loss 0.1661 acc : 96.88% - (62/64)\n",
      "epoch 6/11 2112/15287 train loss 0.0645 acc : 100.00% - (64/64)\n",
      "epoch 6/11 2176/15287 train loss 0.2352 acc : 90.62% - (58/64)\n",
      "epoch 6/11 2240/15287 train loss 0.2109 acc : 96.88% - (62/64)\n",
      "epoch 6/11 2304/15287 train loss 0.1601 acc : 96.88% - (62/64)\n",
      "epoch 6/11 2368/15287 train loss 0.1411 acc : 95.31% - (61/64)\n",
      "epoch 6/11 2432/15287 train loss 0.1114 acc : 98.44% - (63/64)\n",
      "epoch 6/11 2496/15287 train loss 0.1356 acc : 98.44% - (63/64)\n",
      "epoch 6/11 2560/15287 train loss 0.1191 acc : 98.44% - (63/64)\n",
      "epoch 6/11 2624/15287 train loss 0.1193 acc : 95.31% - (61/64)\n",
      "epoch 6/11 2688/15287 train loss 0.0977 acc : 98.44% - (63/64)\n",
      "epoch 6/11 2752/15287 train loss 0.1260 acc : 96.88% - (62/64)\n",
      "epoch 6/11 2816/15287 train loss 0.0808 acc : 100.00% - (64/64)\n",
      "epoch 6/11 2880/15287 train loss 0.1728 acc : 93.75% - (60/64)\n",
      "epoch 6/11 2944/15287 train loss 0.2111 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3008/15287 train loss 0.1230 acc : 96.88% - (62/64)\n",
      "epoch 6/11 3072/15287 train loss 0.0737 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3136/15287 train loss 0.1642 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3200/15287 train loss 0.1094 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3264/15287 train loss 0.1784 acc : 95.31% - (61/64)\n",
      "epoch 6/11 3328/15287 train loss 0.0687 acc : 100.00% - (64/64)\n",
      "epoch 6/11 3392/15287 train loss 0.0987 acc : 96.88% - (62/64)\n",
      "epoch 6/11 3456/15287 train loss 0.1514 acc : 95.31% - (61/64)\n",
      "epoch 6/11 3520/15287 train loss 0.1126 acc : 96.88% - (62/64)\n",
      "epoch 6/11 3584/15287 train loss 0.1432 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3648/15287 train loss 0.1592 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3712/15287 train loss 0.1051 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3776/15287 train loss 0.1618 acc : 96.88% - (62/64)\n",
      "epoch 6/11 3840/15287 train loss 0.1240 acc : 98.44% - (63/64)\n",
      "epoch 6/11 3904/15287 train loss 0.0926 acc : 100.00% - (64/64)\n",
      "epoch 6/11 3968/15287 train loss 0.1214 acc : 96.88% - (62/64)\n",
      "epoch 6/11 4032/15287 train loss 0.1035 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4096/15287 train loss 0.1379 acc : 98.44% - (63/64)\n",
      "epoch 6/11 4160/15287 train loss 0.1995 acc : 96.88% - (62/64)\n",
      "epoch 6/11 4224/15287 train loss 0.1061 acc : 98.44% - (63/64)\n",
      "epoch 6/11 4288/15287 train loss 0.0696 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4352/15287 train loss 0.0595 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4416/15287 train loss 0.1226 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4480/15287 train loss 0.0916 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4544/15287 train loss 0.0911 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4608/15287 train loss 0.1707 acc : 95.31% - (61/64)\n",
      "epoch 6/11 4672/15287 train loss 0.1492 acc : 98.44% - (63/64)\n",
      "epoch 6/11 4736/15287 train loss 0.1117 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4800/15287 train loss 0.0609 acc : 100.00% - (64/64)\n",
      "epoch 6/11 4864/15287 train loss 0.1090 acc : 98.44% - (63/64)\n",
      "epoch 6/11 4928/15287 train loss 0.0934 acc : 98.44% - (63/64)\n",
      "epoch 6/11 4992/15287 train loss 0.1506 acc : 96.88% - (62/64)\n",
      "epoch 6/11 5056/15287 train loss 0.1360 acc : 96.88% - (62/64)\n",
      "epoch 6/11 5120/15287 train loss 0.1125 acc : 98.44% - (63/64)\n",
      "epoch 6/11 5184/15287 train loss 0.0697 acc : 100.00% - (64/64)\n",
      "epoch 6/11 5248/15287 train loss 0.1127 acc : 96.88% - (62/64)\n",
      "epoch 6/11 5312/15287 train loss 0.1810 acc : 95.31% - (61/64)\n",
      "epoch 6/11 5376/15287 train loss 0.1129 acc : 100.00% - (64/64)\n",
      "epoch 6/11 5440/15287 train loss 0.1373 acc : 96.88% - (62/64)\n",
      "epoch 6/11 5504/15287 train loss 0.1315 acc : 98.44% - (63/64)\n",
      "epoch 6/11 5568/15287 train loss 0.2585 acc : 92.19% - (59/64)\n",
      "epoch 6/11 5632/15287 train loss 0.0702 acc : 100.00% - (64/64)\n",
      "epoch 6/11 5696/15287 train loss 0.1124 acc : 100.00% - (64/64)\n",
      "epoch 6/11 5760/15287 train loss 0.1094 acc : 95.31% - (61/64)\n",
      "epoch 6/11 5824/15287 train loss 0.0848 acc : 98.44% - (63/64)\n",
      "epoch 6/11 5888/15287 train loss 0.1152 acc : 98.44% - (63/64)\n",
      "epoch 6/11 5952/15287 train loss 0.0586 acc : 100.00% - (64/64)\n",
      "epoch 6/11 6016/15287 train loss 0.2289 acc : 95.31% - (61/64)\n",
      "epoch 6/11 6080/15287 train loss 0.1294 acc : 96.88% - (62/64)\n",
      "epoch 6/11 6144/15287 train loss 0.1080 acc : 98.44% - (63/64)\n",
      "epoch 6/11 6208/15287 train loss 0.1097 acc : 100.00% - (64/64)\n",
      "epoch 6/11 6272/15287 train loss 0.0569 acc : 100.00% - (64/64)\n",
      "epoch 6/11 6336/15287 train loss 0.1239 acc : 98.44% - (63/64)\n",
      "epoch 6/11 6400/15287 train loss 0.1373 acc : 98.44% - (63/64)\n",
      "epoch 6/11 6464/15287 train loss 0.0912 acc : 98.44% - (63/64)\n",
      "epoch 6/11 6528/15287 train loss 0.1730 acc : 96.88% - (62/64)\n",
      "epoch 6/11 6592/15287 train loss 0.0964 acc : 98.44% - (63/64)\n",
      "epoch 6/11 6656/15287 train loss 0.0923 acc : 98.44% - (63/64)\n",
      "epoch 6/11 6720/15287 train loss 0.0771 acc : 100.00% - (64/64)\n",
      "epoch 6/11 6784/15287 train loss 0.1071 acc : 96.88% - (62/64)\n",
      "epoch 6/11 6848/15287 train loss 0.1276 acc : 98.44% - (63/64)\n",
      "epoch 6/11 6912/15287 train loss 0.0732 acc : 100.00% - (64/64)\n",
      "epoch 6/11 6976/15287 train loss 0.1050 acc : 98.44% - (63/64)\n",
      "epoch 6/11 7040/15287 train loss 0.1062 acc : 98.44% - (63/64)\n",
      "epoch 6/11 7104/15287 train loss 0.0563 acc : 100.00% - (64/64)\n",
      "epoch 6/11 7168/15287 train loss 0.1496 acc : 96.88% - (62/64)\n",
      "epoch 6/11 7232/15287 train loss 0.0756 acc : 100.00% - (64/64)\n",
      "epoch 6/11 7296/15287 train loss 0.1163 acc : 96.88% - (62/64)\n",
      "epoch 6/11 7360/15287 train loss 0.0706 acc : 100.00% - (64/64)\n",
      "epoch 6/11 7424/15287 train loss 0.1450 acc : 95.31% - (61/64)\n",
      "epoch 6/11 7488/15287 train loss 0.1333 acc : 98.44% - (63/64)\n",
      "epoch 6/11 7552/15287 train loss 0.1009 acc : 100.00% - (64/64)\n",
      "epoch 6/11 7616/15287 train loss 0.0546 acc : 100.00% - (64/64)\n",
      "epoch 6/11 7680/15287 train loss 0.0642 acc : 100.00% - (64/64)\n",
      "epoch 6/11 7744/15287 train loss 0.1313 acc : 95.31% - (61/64)\n",
      "epoch 6/11 7808/15287 train loss 0.1404 acc : 98.44% - (63/64)\n",
      "epoch 6/11 7872/15287 train loss 0.0898 acc : 98.44% - (63/64)\n",
      "epoch 6/11 7936/15287 train loss 0.0808 acc : 100.00% - (64/64)\n",
      "epoch 6/11 8000/15287 train loss 0.1303 acc : 96.88% - (62/64)\n",
      "epoch 6/11 8064/15287 train loss 0.1084 acc : 98.44% - (63/64)\n",
      "epoch 6/11 8128/15287 train loss 0.1517 acc : 96.88% - (62/64)\n",
      "epoch 6/11 8192/15287 train loss 0.0915 acc : 98.44% - (63/64)\n",
      "epoch 6/11 8256/15287 train loss 0.0821 acc : 98.44% - (63/64)\n",
      "epoch 6/11 8320/15287 train loss 0.0805 acc : 100.00% - (64/64)\n",
      "epoch 6/11 8384/15287 train loss 0.1486 acc : 98.44% - (63/64)\n",
      "epoch 6/11 8448/15287 train loss 0.0716 acc : 98.44% - (63/64)\n",
      "epoch 6/11 8512/15287 train loss 0.0606 acc : 100.00% - (64/64)\n",
      "epoch 6/11 8576/15287 train loss 0.1055 acc : 98.44% - (63/64)\n",
      "epoch 6/11 8640/15287 train loss 0.1329 acc : 98.44% - (63/64)\n",
      "epoch 6/11 8704/15287 train loss 0.0788 acc : 100.00% - (64/64)\n",
      "epoch 6/11 8768/15287 train loss 0.0430 acc : 100.00% - (64/64)\n",
      "epoch 6/11 8832/15287 train loss 0.1661 acc : 96.88% - (62/64)\n",
      "epoch 6/11 8896/15287 train loss 0.0634 acc : 100.00% - (64/64)\n",
      "epoch 6/11 8960/15287 train loss 0.0994 acc : 100.00% - (64/64)\n",
      "epoch 6/11 9024/15287 train loss 0.1309 acc : 100.00% - (64/64)\n",
      "epoch 6/11 9088/15287 train loss 0.0644 acc : 100.00% - (64/64)\n",
      "epoch 6/11 9152/15287 train loss 0.1039 acc : 98.44% - (63/64)\n",
      "epoch 6/11 9216/15287 train loss 0.0781 acc : 98.44% - (63/64)\n",
      "epoch 6/11 9280/15287 train loss 0.1102 acc : 98.44% - (63/64)\n",
      "epoch 6/11 9344/15287 train loss 0.0966 acc : 98.44% - (63/64)\n",
      "epoch 6/11 9408/15287 train loss 0.0852 acc : 100.00% - (64/64)\n",
      "epoch 6/11 9472/15287 train loss 0.0779 acc : 100.00% - (64/64)\n",
      "epoch 6/11 9536/15287 train loss 0.1562 acc : 93.75% - (60/64)\n",
      "epoch 6/11 9600/15287 train loss 0.1393 acc : 96.88% - (62/64)\n",
      "epoch 6/11 9664/15287 train loss 0.0827 acc : 100.00% - (64/64)\n",
      "epoch 6/11 9728/15287 train loss 0.0505 acc : 100.00% - (64/64)\n",
      "epoch 6/11 9792/15287 train loss 0.0829 acc : 98.44% - (63/64)\n",
      "epoch 6/11 9856/15287 train loss 0.1351 acc : 98.44% - (63/64)\n",
      "epoch 6/11 9920/15287 train loss 0.0810 acc : 98.44% - (63/64)\n",
      "epoch 6/11 9984/15287 train loss 0.0556 acc : 100.00% - (64/64)\n",
      "epoch 6/11 10048/15287 train loss 0.1214 acc : 98.44% - (63/64)\n",
      "epoch 6/11 10112/15287 train loss 0.0725 acc : 98.44% - (63/64)\n",
      "epoch 6/11 10176/15287 train loss 0.0994 acc : 100.00% - (64/64)\n",
      "epoch 6/11 10240/15287 train loss 0.0704 acc : 100.00% - (64/64)\n",
      "epoch 6/11 10304/15287 train loss 0.0939 acc : 98.44% - (63/64)\n",
      "epoch 6/11 10368/15287 train loss 0.1171 acc : 98.44% - (63/64)\n",
      "epoch 6/11 10432/15287 train loss 0.1072 acc : 98.44% - (63/64)\n",
      "epoch 6/11 10496/15287 train loss 0.0659 acc : 100.00% - (64/64)\n",
      "epoch 6/11 10560/15287 train loss 0.1427 acc : 96.88% - (62/64)\n",
      "epoch 6/11 10624/15287 train loss 0.1426 acc : 96.88% - (62/64)\n",
      "epoch 6/11 10688/15287 train loss 0.0674 acc : 98.44% - (63/64)\n",
      "epoch 6/11 10752/15287 train loss 0.0685 acc : 100.00% - (64/64)\n",
      "epoch 6/11 10816/15287 train loss 0.1001 acc : 96.88% - (62/64)\n",
      "epoch 6/11 10880/15287 train loss 0.1154 acc : 98.44% - (63/64)\n",
      "epoch 6/11 10944/15287 train loss 0.1349 acc : 98.44% - (63/64)\n",
      "epoch 6/11 11008/15287 train loss 0.0621 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11072/15287 train loss 0.0546 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11136/15287 train loss 0.0532 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11200/15287 train loss 0.0844 acc : 98.44% - (63/64)\n",
      "epoch 6/11 11264/15287 train loss 0.0688 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11328/15287 train loss 0.0594 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11392/15287 train loss 0.0672 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11456/15287 train loss 0.0592 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11520/15287 train loss 0.0632 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11584/15287 train loss 0.0477 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11648/15287 train loss 0.0655 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11712/15287 train loss 0.0477 acc : 98.44% - (63/64)\n",
      "epoch 6/11 11776/15287 train loss 0.0816 acc : 98.44% - (63/64)\n",
      "epoch 6/11 11840/15287 train loss 0.0698 acc : 100.00% - (64/64)\n",
      "epoch 6/11 11904/15287 train loss 0.0983 acc : 98.44% - (63/64)\n",
      "epoch 6/11 11968/15287 train loss 0.1033 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12032/15287 train loss 0.0709 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12096/15287 train loss 0.0914 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12160/15287 train loss 0.1097 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12224/15287 train loss 0.1389 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12288/15287 train loss 0.1346 acc : 96.88% - (62/64)\n",
      "epoch 6/11 12352/15287 train loss 0.1280 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12416/15287 train loss 0.1390 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12480/15287 train loss 0.0803 acc : 100.00% - (64/64)\n",
      "epoch 6/11 12544/15287 train loss 0.0966 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12608/15287 train loss 0.0536 acc : 100.00% - (64/64)\n",
      "epoch 6/11 12672/15287 train loss 0.0652 acc : 100.00% - (64/64)\n",
      "epoch 6/11 12736/15287 train loss 0.0544 acc : 100.00% - (64/64)\n",
      "epoch 6/11 12800/15287 train loss 0.0740 acc : 100.00% - (64/64)\n",
      "epoch 6/11 12864/15287 train loss 0.1027 acc : 96.88% - (62/64)\n",
      "epoch 6/11 12928/15287 train loss 0.0717 acc : 98.44% - (63/64)\n",
      "epoch 6/11 12992/15287 train loss 0.1217 acc : 98.44% - (63/64)\n",
      "epoch 6/11 13056/15287 train loss 0.0468 acc : 100.00% - (64/64)\n",
      "epoch 6/11 13120/15287 train loss 0.1356 acc : 98.44% - (63/64)\n",
      "epoch 6/11 13184/15287 train loss 0.0639 acc : 98.44% - (63/64)\n",
      "epoch 6/11 13248/15287 train loss 0.0929 acc : 100.00% - (64/64)\n",
      "epoch 6/11 13312/15287 train loss 0.0635 acc : 100.00% - (64/64)\n",
      "epoch 6/11 13376/15287 train loss 0.0455 acc : 100.00% - (64/64)\n",
      "epoch 6/11 13440/15287 train loss 0.0697 acc : 100.00% - (64/64)\n",
      "epoch 6/11 13504/15287 train loss 0.0793 acc : 100.00% - (64/64)\n",
      "epoch 6/11 13568/15287 train loss 0.0714 acc : 98.44% - (63/64)\n",
      "epoch 6/11 13632/15287 train loss 0.0768 acc : 100.00% - (64/64)\n",
      "epoch 6/11 13696/15287 train loss 0.0905 acc : 98.44% - (63/64)\n",
      "epoch 6/11 13760/15287 train loss 0.1531 acc : 98.44% - (63/64)\n",
      "epoch 6/11 13824/15287 train loss 0.0824 acc : 98.44% - (63/64)\n",
      "epoch 6/11 13888/15287 train loss 0.1398 acc : 96.88% - (62/64)\n",
      "epoch 6/11 13952/15287 train loss 0.0795 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14016/15287 train loss 0.0822 acc : 96.88% - (62/64)\n",
      "epoch 6/11 14080/15287 train loss 0.0542 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14144/15287 train loss 0.0971 acc : 98.44% - (63/64)\n",
      "epoch 6/11 14208/15287 train loss 0.0641 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14272/15287 train loss 0.0571 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14336/15287 train loss 0.0615 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14400/15287 train loss 0.0575 acc : 98.44% - (63/64)\n",
      "epoch 6/11 14464/15287 train loss 0.0763 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14528/15287 train loss 0.0852 acc : 98.44% - (63/64)\n",
      "epoch 6/11 14592/15287 train loss 0.0822 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14656/15287 train loss 0.0601 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14720/15287 train loss 0.1249 acc : 100.00% - (64/64)\n",
      "epoch 6/11 14784/15287 train loss 0.0847 acc : 98.44% - (63/64)\n",
      "epoch 6/11 14848/15287 train loss 0.1087 acc : 98.44% - (63/64)\n",
      "epoch 6/11 14912/15287 train loss 0.0852 acc : 98.44% - (63/64)\n",
      "epoch 6/11 14976/15287 train loss 0.1758 acc : 95.31% - (61/64)\n",
      "epoch 6/11 15040/15287 train loss 0.1144 acc : 98.44% - (63/64)\n",
      "epoch 6/11 15104/15287 train loss 0.2004 acc : 96.88% - (62/64)\n",
      "epoch 6/11 15168/15287 train loss 0.0720 acc : 100.00% - (64/64)\n",
      "epoch 6/11 15232/15287 train loss 0.0464 acc : 100.00% - (64/64)\n",
      "epoch 6/11 15287/15287 train loss 0.0447 acc : 100.00% - (55/55)\n",
      "\n",
      " epoch 6 train end!!! \t train batch loss : 0.1107\t total acc : 98.35% - (15035/15287) \n",
      "\n",
      "epoch 6 val end!!! val loss : 0.060 \t f1 score : 1.000 acc : 87.40% - (1485/1699) \n",
      "\n",
      "\n",
      "epoch 7/11 64/15287 train loss 0.0503 acc : 100.00% - (64/64)\n",
      "epoch 7/11 128/15287 train loss 0.0918 acc : 96.88% - (62/64)\n",
      "epoch 7/11 192/15287 train loss 0.1189 acc : 98.44% - (63/64)\n",
      "epoch 7/11 256/15287 train loss 0.0566 acc : 100.00% - (64/64)\n",
      "epoch 7/11 320/15287 train loss 0.2404 acc : 98.44% - (63/64)\n",
      "epoch 7/11 384/15287 train loss 0.0391 acc : 100.00% - (64/64)\n",
      "epoch 7/11 448/15287 train loss 0.0732 acc : 100.00% - (64/64)\n",
      "epoch 7/11 512/15287 train loss 0.0978 acc : 98.44% - (63/64)\n",
      "epoch 7/11 576/15287 train loss 0.1197 acc : 98.44% - (63/64)\n",
      "epoch 7/11 640/15287 train loss 0.0537 acc : 100.00% - (64/64)\n",
      "epoch 7/11 704/15287 train loss 0.0861 acc : 98.44% - (63/64)\n",
      "epoch 7/11 768/15287 train loss 0.0567 acc : 100.00% - (64/64)\n",
      "epoch 7/11 832/15287 train loss 0.0572 acc : 100.00% - (64/64)\n",
      "epoch 7/11 896/15287 train loss 0.1008 acc : 98.44% - (63/64)\n",
      "epoch 7/11 960/15287 train loss 0.1457 acc : 98.44% - (63/64)\n",
      "epoch 7/11 1024/15287 train loss 0.0719 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1088/15287 train loss 0.0741 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1152/15287 train loss 0.0848 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1216/15287 train loss 0.0646 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1280/15287 train loss 0.1210 acc : 98.44% - (63/64)\n",
      "epoch 7/11 1344/15287 train loss 0.1147 acc : 95.31% - (61/64)\n",
      "epoch 7/11 1408/15287 train loss 0.0901 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1472/15287 train loss 0.0390 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1536/15287 train loss 0.0687 acc : 98.44% - (63/64)\n",
      "epoch 7/11 1600/15287 train loss 0.0810 acc : 98.44% - (63/64)\n",
      "epoch 7/11 1664/15287 train loss 0.0604 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1728/15287 train loss 0.0587 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1792/15287 train loss 0.0831 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1856/15287 train loss 0.0514 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1920/15287 train loss 0.0546 acc : 100.00% - (64/64)\n",
      "epoch 7/11 1984/15287 train loss 0.0882 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2048/15287 train loss 0.0717 acc : 98.44% - (63/64)\n",
      "epoch 7/11 2112/15287 train loss 0.0337 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2176/15287 train loss 0.1240 acc : 96.88% - (62/64)\n",
      "epoch 7/11 2240/15287 train loss 0.1845 acc : 98.44% - (63/64)\n",
      "epoch 7/11 2304/15287 train loss 0.0654 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2368/15287 train loss 0.0607 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2432/15287 train loss 0.0544 acc : 98.44% - (63/64)\n",
      "epoch 7/11 2496/15287 train loss 0.0600 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2560/15287 train loss 0.0644 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2624/15287 train loss 0.0583 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2688/15287 train loss 0.0478 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2752/15287 train loss 0.0517 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2816/15287 train loss 0.0329 acc : 100.00% - (64/64)\n",
      "epoch 7/11 2880/15287 train loss 0.1064 acc : 96.88% - (62/64)\n",
      "epoch 7/11 2944/15287 train loss 0.1172 acc : 98.44% - (63/64)\n",
      "epoch 7/11 3008/15287 train loss 0.0550 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3072/15287 train loss 0.0331 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3136/15287 train loss 0.0820 acc : 98.44% - (63/64)\n",
      "epoch 7/11 3200/15287 train loss 0.0526 acc : 98.44% - (63/64)\n",
      "epoch 7/11 3264/15287 train loss 0.0770 acc : 96.88% - (62/64)\n",
      "epoch 7/11 3328/15287 train loss 0.0343 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3392/15287 train loss 0.0397 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3456/15287 train loss 0.0952 acc : 96.88% - (62/64)\n",
      "epoch 7/11 3520/15287 train loss 0.0569 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3584/15287 train loss 0.0617 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3648/15287 train loss 0.0742 acc : 98.44% - (63/64)\n",
      "epoch 7/11 3712/15287 train loss 0.0691 acc : 98.44% - (63/64)\n",
      "epoch 7/11 3776/15287 train loss 0.0704 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3840/15287 train loss 0.0548 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3904/15287 train loss 0.0337 acc : 100.00% - (64/64)\n",
      "epoch 7/11 3968/15287 train loss 0.0701 acc : 98.44% - (63/64)\n",
      "epoch 7/11 4032/15287 train loss 0.0556 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4096/15287 train loss 0.0783 acc : 98.44% - (63/64)\n",
      "epoch 7/11 4160/15287 train loss 0.0853 acc : 98.44% - (63/64)\n",
      "epoch 7/11 4224/15287 train loss 0.0678 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4288/15287 train loss 0.0293 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4352/15287 train loss 0.0329 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4416/15287 train loss 0.0620 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4480/15287 train loss 0.0428 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4544/15287 train loss 0.0470 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4608/15287 train loss 0.0663 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4672/15287 train loss 0.0757 acc : 98.44% - (63/64)\n",
      "epoch 7/11 4736/15287 train loss 0.0487 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4800/15287 train loss 0.0341 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4864/15287 train loss 0.0714 acc : 98.44% - (63/64)\n",
      "epoch 7/11 4928/15287 train loss 0.0425 acc : 100.00% - (64/64)\n",
      "epoch 7/11 4992/15287 train loss 0.0725 acc : 98.44% - (63/64)\n",
      "epoch 7/11 5056/15287 train loss 0.0560 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5120/15287 train loss 0.0672 acc : 98.44% - (63/64)\n",
      "epoch 7/11 5184/15287 train loss 0.0334 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5248/15287 train loss 0.0449 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5312/15287 train loss 0.0984 acc : 98.44% - (63/64)\n",
      "epoch 7/11 5376/15287 train loss 0.0460 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5440/15287 train loss 0.0533 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5504/15287 train loss 0.0725 acc : 98.44% - (63/64)\n",
      "epoch 7/11 5568/15287 train loss 0.1328 acc : 96.88% - (62/64)\n",
      "epoch 7/11 5632/15287 train loss 0.0413 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5696/15287 train loss 0.0515 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5760/15287 train loss 0.0482 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5824/15287 train loss 0.0443 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5888/15287 train loss 0.0585 acc : 100.00% - (64/64)\n",
      "epoch 7/11 5952/15287 train loss 0.0318 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6016/15287 train loss 0.1860 acc : 98.44% - (63/64)\n",
      "epoch 7/11 6080/15287 train loss 0.0566 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6144/15287 train loss 0.0552 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6208/15287 train loss 0.0556 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6272/15287 train loss 0.0340 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6336/15287 train loss 0.0713 acc : 98.44% - (63/64)\n",
      "epoch 7/11 6400/15287 train loss 0.0597 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6464/15287 train loss 0.0574 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6528/15287 train loss 0.1236 acc : 98.44% - (63/64)\n",
      "epoch 7/11 6592/15287 train loss 0.0479 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6656/15287 train loss 0.0448 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6720/15287 train loss 0.0457 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6784/15287 train loss 0.0466 acc : 98.44% - (63/64)\n",
      "epoch 7/11 6848/15287 train loss 0.0566 acc : 98.44% - (63/64)\n",
      "epoch 7/11 6912/15287 train loss 0.0577 acc : 100.00% - (64/64)\n",
      "epoch 7/11 6976/15287 train loss 0.0441 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7040/15287 train loss 0.0637 acc : 96.88% - (62/64)\n",
      "epoch 7/11 7104/15287 train loss 0.0331 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7168/15287 train loss 0.0828 acc : 98.44% - (63/64)\n",
      "epoch 7/11 7232/15287 train loss 0.0364 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7296/15287 train loss 0.0620 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7360/15287 train loss 0.0357 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7424/15287 train loss 0.0697 acc : 98.44% - (63/64)\n",
      "epoch 7/11 7488/15287 train loss 0.0840 acc : 98.44% - (63/64)\n",
      "epoch 7/11 7552/15287 train loss 0.0480 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7616/15287 train loss 0.0307 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7680/15287 train loss 0.0296 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7744/15287 train loss 0.0628 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7808/15287 train loss 0.0716 acc : 98.44% - (63/64)\n",
      "epoch 7/11 7872/15287 train loss 0.0367 acc : 100.00% - (64/64)\n",
      "epoch 7/11 7936/15287 train loss 0.0369 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8000/15287 train loss 0.0586 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8064/15287 train loss 0.0476 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8128/15287 train loss 0.0577 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8192/15287 train loss 0.0423 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8256/15287 train loss 0.0342 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8320/15287 train loss 0.0458 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8384/15287 train loss 0.0627 acc : 98.44% - (63/64)\n",
      "epoch 7/11 8448/15287 train loss 0.0338 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8512/15287 train loss 0.0356 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8576/15287 train loss 0.0445 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8640/15287 train loss 0.0552 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8704/15287 train loss 0.0397 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8768/15287 train loss 0.0243 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8832/15287 train loss 0.0871 acc : 98.44% - (63/64)\n",
      "epoch 7/11 8896/15287 train loss 0.0374 acc : 100.00% - (64/64)\n",
      "epoch 7/11 8960/15287 train loss 0.0670 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9024/15287 train loss 0.0585 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9088/15287 train loss 0.0360 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9152/15287 train loss 0.0479 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9216/15287 train loss 0.0295 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9280/15287 train loss 0.0789 acc : 98.44% - (63/64)\n",
      "epoch 7/11 9344/15287 train loss 0.0602 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9408/15287 train loss 0.0438 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9472/15287 train loss 0.0346 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9536/15287 train loss 0.0664 acc : 98.44% - (63/64)\n",
      "epoch 7/11 9600/15287 train loss 0.0876 acc : 98.44% - (63/64)\n",
      "epoch 7/11 9664/15287 train loss 0.0546 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9728/15287 train loss 0.0251 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9792/15287 train loss 0.0405 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9856/15287 train loss 0.1105 acc : 98.44% - (63/64)\n",
      "epoch 7/11 9920/15287 train loss 0.0432 acc : 100.00% - (64/64)\n",
      "epoch 7/11 9984/15287 train loss 0.0312 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10048/15287 train loss 0.0522 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10112/15287 train loss 0.0324 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10176/15287 train loss 0.0459 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10240/15287 train loss 0.0328 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10304/15287 train loss 0.0298 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10368/15287 train loss 0.0544 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10432/15287 train loss 0.0483 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10496/15287 train loss 0.0376 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10560/15287 train loss 0.0650 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10624/15287 train loss 0.0688 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10688/15287 train loss 0.0271 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10752/15287 train loss 0.0336 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10816/15287 train loss 0.0501 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10880/15287 train loss 0.0586 acc : 100.00% - (64/64)\n",
      "epoch 7/11 10944/15287 train loss 0.0505 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11008/15287 train loss 0.0331 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11072/15287 train loss 0.0326 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11136/15287 train loss 0.0295 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11200/15287 train loss 0.0400 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11264/15287 train loss 0.0398 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11328/15287 train loss 0.0355 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11392/15287 train loss 0.0355 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11456/15287 train loss 0.0355 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11520/15287 train loss 0.0320 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11584/15287 train loss 0.0293 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11648/15287 train loss 0.0352 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11712/15287 train loss 0.0223 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11776/15287 train loss 0.0354 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11840/15287 train loss 0.0404 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11904/15287 train loss 0.0437 acc : 100.00% - (64/64)\n",
      "epoch 7/11 11968/15287 train loss 0.0494 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12032/15287 train loss 0.0353 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12096/15287 train loss 0.0378 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12160/15287 train loss 0.0436 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12224/15287 train loss 0.0623 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12288/15287 train loss 0.0921 acc : 98.44% - (63/64)\n",
      "epoch 7/11 12352/15287 train loss 0.0741 acc : 98.44% - (63/64)\n",
      "epoch 7/11 12416/15287 train loss 0.1229 acc : 98.44% - (63/64)\n",
      "epoch 7/11 12480/15287 train loss 0.0358 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12544/15287 train loss 0.0352 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12608/15287 train loss 0.0280 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12672/15287 train loss 0.0432 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12736/15287 train loss 0.0317 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12800/15287 train loss 0.0329 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12864/15287 train loss 0.0513 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12928/15287 train loss 0.0306 acc : 100.00% - (64/64)\n",
      "epoch 7/11 12992/15287 train loss 0.0655 acc : 98.44% - (63/64)\n",
      "epoch 7/11 13056/15287 train loss 0.0212 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13120/15287 train loss 0.0482 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13184/15287 train loss 0.0278 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13248/15287 train loss 0.0429 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13312/15287 train loss 0.0390 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13376/15287 train loss 0.0246 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13440/15287 train loss 0.0350 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13504/15287 train loss 0.0374 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13568/15287 train loss 0.0283 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13632/15287 train loss 0.0390 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13696/15287 train loss 0.0409 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13760/15287 train loss 0.0892 acc : 98.44% - (63/64)\n",
      "epoch 7/11 13824/15287 train loss 0.0332 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13888/15287 train loss 0.0394 acc : 100.00% - (64/64)\n",
      "epoch 7/11 13952/15287 train loss 0.0309 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14016/15287 train loss 0.0305 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14080/15287 train loss 0.0363 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14144/15287 train loss 0.0437 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14208/15287 train loss 0.0336 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14272/15287 train loss 0.0304 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14336/15287 train loss 0.0372 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14400/15287 train loss 0.0418 acc : 98.44% - (63/64)\n",
      "epoch 7/11 14464/15287 train loss 0.0354 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14528/15287 train loss 0.0282 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14592/15287 train loss 0.0296 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14656/15287 train loss 0.0233 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14720/15287 train loss 0.0393 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14784/15287 train loss 0.0306 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14848/15287 train loss 0.0527 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14912/15287 train loss 0.0518 acc : 100.00% - (64/64)\n",
      "epoch 7/11 14976/15287 train loss 0.0745 acc : 100.00% - (64/64)\n",
      "epoch 7/11 15040/15287 train loss 0.1075 acc : 98.44% - (63/64)\n",
      "epoch 7/11 15104/15287 train loss 0.0828 acc : 98.44% - (63/64)\n",
      "epoch 7/11 15168/15287 train loss 0.0320 acc : 100.00% - (64/64)\n",
      "epoch 7/11 15232/15287 train loss 0.0311 acc : 100.00% - (64/64)\n",
      "epoch 7/11 15287/15287 train loss 0.0305 acc : 100.00% - (55/55)\n",
      "\n",
      " epoch 7 train end!!! \t train batch loss : 0.0564\t total acc : 99.56% - (15220/15287) \n",
      "\n",
      "epoch 7 val end!!! val loss : 0.038 \t f1 score : 1.000 acc : 87.40% - (1485/1699) \n",
      "\n",
      "\n",
      "epoch 8/11 64/15287 train loss 0.0281 acc : 100.00% - (64/64)\n",
      "epoch 8/11 128/15287 train loss 0.0342 acc : 100.00% - (64/64)\n",
      "epoch 8/11 192/15287 train loss 0.0550 acc : 100.00% - (64/64)\n",
      "epoch 8/11 256/15287 train loss 0.0334 acc : 100.00% - (64/64)\n",
      "epoch 8/11 320/15287 train loss 0.0417 acc : 100.00% - (64/64)\n",
      "epoch 8/11 384/15287 train loss 0.0214 acc : 100.00% - (64/64)\n",
      "epoch 8/11 448/15287 train loss 0.0586 acc : 98.44% - (63/64)\n",
      "epoch 8/11 512/15287 train loss 0.0394 acc : 100.00% - (64/64)\n",
      "epoch 8/11 576/15287 train loss 0.0515 acc : 98.44% - (63/64)\n",
      "epoch 8/11 640/15287 train loss 0.0239 acc : 100.00% - (64/64)\n",
      "epoch 8/11 704/15287 train loss 0.0323 acc : 100.00% - (64/64)\n",
      "epoch 8/11 768/15287 train loss 0.0260 acc : 100.00% - (64/64)\n",
      "epoch 8/11 832/15287 train loss 0.0327 acc : 100.00% - (64/64)\n",
      "epoch 8/11 896/15287 train loss 0.0445 acc : 100.00% - (64/64)\n",
      "epoch 8/11 960/15287 train loss 0.0399 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1024/15287 train loss 0.0296 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1088/15287 train loss 0.0385 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1152/15287 train loss 0.0431 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1216/15287 train loss 0.0292 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1280/15287 train loss 0.0799 acc : 98.44% - (63/64)\n",
      "epoch 8/11 1344/15287 train loss 0.0502 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1408/15287 train loss 0.0397 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1472/15287 train loss 0.0257 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1536/15287 train loss 0.0391 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1600/15287 train loss 0.0329 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1664/15287 train loss 0.0337 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1728/15287 train loss 0.0420 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1792/15287 train loss 0.0455 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1856/15287 train loss 0.0304 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1920/15287 train loss 0.0286 acc : 100.00% - (64/64)\n",
      "epoch 8/11 1984/15287 train loss 0.0445 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2048/15287 train loss 0.0362 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2112/15287 train loss 0.0204 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2176/15287 train loss 0.0635 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2240/15287 train loss 0.1858 acc : 98.44% - (63/64)\n",
      "epoch 8/11 2304/15287 train loss 0.0373 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2368/15287 train loss 0.0291 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2432/15287 train loss 0.0260 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2496/15287 train loss 0.0303 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2560/15287 train loss 0.0363 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2624/15287 train loss 0.0214 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2688/15287 train loss 0.0303 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2752/15287 train loss 0.0293 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2816/15287 train loss 0.0188 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2880/15287 train loss 0.0413 acc : 100.00% - (64/64)\n",
      "epoch 8/11 2944/15287 train loss 0.0513 acc : 98.44% - (63/64)\n",
      "epoch 8/11 3008/15287 train loss 0.0333 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3072/15287 train loss 0.0223 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3136/15287 train loss 0.0372 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3200/15287 train loss 0.0245 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3264/15287 train loss 0.0753 acc : 98.44% - (63/64)\n",
      "epoch 8/11 3328/15287 train loss 0.0263 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3392/15287 train loss 0.0199 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3456/15287 train loss 0.0417 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3520/15287 train loss 0.0339 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3584/15287 train loss 0.0395 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3648/15287 train loss 0.0403 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3712/15287 train loss 0.0396 acc : 98.44% - (63/64)\n",
      "epoch 8/11 3776/15287 train loss 0.0355 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3840/15287 train loss 0.0260 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3904/15287 train loss 0.0195 acc : 100.00% - (64/64)\n",
      "epoch 8/11 3968/15287 train loss 0.0311 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4032/15287 train loss 0.0474 acc : 98.44% - (63/64)\n",
      "epoch 8/11 4096/15287 train loss 0.0346 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4160/15287 train loss 0.0355 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4224/15287 train loss 0.0370 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4288/15287 train loss 0.0138 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4352/15287 train loss 0.0219 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4416/15287 train loss 0.0333 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4480/15287 train loss 0.0311 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4544/15287 train loss 0.0264 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4608/15287 train loss 0.0357 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4672/15287 train loss 0.0464 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4736/15287 train loss 0.0225 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4800/15287 train loss 0.0194 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4864/15287 train loss 0.0266 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4928/15287 train loss 0.0278 acc : 100.00% - (64/64)\n",
      "epoch 8/11 4992/15287 train loss 0.0335 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5056/15287 train loss 0.0260 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5120/15287 train loss 0.0335 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5184/15287 train loss 0.0161 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5248/15287 train loss 0.0268 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5312/15287 train loss 0.0652 acc : 98.44% - (63/64)\n",
      "epoch 8/11 5376/15287 train loss 0.0541 acc : 98.44% - (63/64)\n",
      "epoch 8/11 5440/15287 train loss 0.0239 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5504/15287 train loss 0.0477 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5568/15287 train loss 0.0879 acc : 98.44% - (63/64)\n",
      "epoch 8/11 5632/15287 train loss 0.0301 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5696/15287 train loss 0.0285 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5760/15287 train loss 0.0278 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5824/15287 train loss 0.0210 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5888/15287 train loss 0.0362 acc : 100.00% - (64/64)\n",
      "epoch 8/11 5952/15287 train loss 0.0191 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6016/15287 train loss 0.1498 acc : 98.44% - (63/64)\n",
      "epoch 8/11 6080/15287 train loss 0.0262 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6144/15287 train loss 0.0275 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6208/15287 train loss 0.0271 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6272/15287 train loss 0.0201 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6336/15287 train loss 0.0371 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6400/15287 train loss 0.0237 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6464/15287 train loss 0.0284 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6528/15287 train loss 0.1614 acc : 98.44% - (63/64)\n",
      "epoch 8/11 6592/15287 train loss 0.0254 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6656/15287 train loss 0.0243 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6720/15287 train loss 0.0243 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6784/15287 train loss 0.0356 acc : 98.44% - (63/64)\n",
      "epoch 8/11 6848/15287 train loss 0.0245 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6912/15287 train loss 0.0308 acc : 100.00% - (64/64)\n",
      "epoch 8/11 6976/15287 train loss 0.0246 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7040/15287 train loss 0.0529 acc : 98.44% - (63/64)\n",
      "epoch 8/11 7104/15287 train loss 0.0192 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7168/15287 train loss 0.0465 acc : 98.44% - (63/64)\n",
      "epoch 8/11 7232/15287 train loss 0.0170 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7296/15287 train loss 0.0375 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7360/15287 train loss 0.0195 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7424/15287 train loss 0.0421 acc : 98.44% - (63/64)\n",
      "epoch 8/11 7488/15287 train loss 0.0481 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7552/15287 train loss 0.0264 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7616/15287 train loss 0.0152 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7680/15287 train loss 0.0322 acc : 98.44% - (63/64)\n",
      "epoch 8/11 7744/15287 train loss 0.0312 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7808/15287 train loss 0.0899 acc : 96.88% - (62/64)\n",
      "epoch 8/11 7872/15287 train loss 0.0195 acc : 100.00% - (64/64)\n",
      "epoch 8/11 7936/15287 train loss 0.0184 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8000/15287 train loss 0.0310 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8064/15287 train loss 0.0267 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8128/15287 train loss 0.0288 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8192/15287 train loss 0.0428 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8256/15287 train loss 0.0181 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8320/15287 train loss 0.0229 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8384/15287 train loss 0.0251 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8448/15287 train loss 0.0201 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8512/15287 train loss 0.0225 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8576/15287 train loss 0.0851 acc : 96.88% - (62/64)\n",
      "epoch 8/11 8640/15287 train loss 0.0318 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8704/15287 train loss 0.0256 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8768/15287 train loss 0.0151 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8832/15287 train loss 0.0304 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8896/15287 train loss 0.0279 acc : 100.00% - (64/64)\n",
      "epoch 8/11 8960/15287 train loss 0.0366 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9024/15287 train loss 0.0314 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9088/15287 train loss 0.0259 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9152/15287 train loss 0.0244 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9216/15287 train loss 0.0159 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9280/15287 train loss 0.0817 acc : 98.44% - (63/64)\n",
      "epoch 8/11 9344/15287 train loss 0.0338 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9408/15287 train loss 0.0271 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9472/15287 train loss 0.0244 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9536/15287 train loss 0.0433 acc : 98.44% - (63/64)\n",
      "epoch 8/11 9600/15287 train loss 0.0353 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9664/15287 train loss 0.0330 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9728/15287 train loss 0.0583 acc : 98.44% - (63/64)\n",
      "epoch 8/11 9792/15287 train loss 0.0229 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9856/15287 train loss 0.0745 acc : 98.44% - (63/64)\n",
      "epoch 8/11 9920/15287 train loss 0.0245 acc : 100.00% - (64/64)\n",
      "epoch 8/11 9984/15287 train loss 0.0163 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10048/15287 train loss 0.0273 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10112/15287 train loss 0.0374 acc : 98.44% - (63/64)\n",
      "epoch 8/11 10176/15287 train loss 0.0322 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10240/15287 train loss 0.0176 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10304/15287 train loss 0.0228 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10368/15287 train loss 0.0312 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10432/15287 train loss 0.0236 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10496/15287 train loss 0.0246 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10560/15287 train loss 0.0413 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10624/15287 train loss 0.0355 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10688/15287 train loss 0.0162 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10752/15287 train loss 0.0153 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10816/15287 train loss 0.0326 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10880/15287 train loss 0.0335 acc : 100.00% - (64/64)\n",
      "epoch 8/11 10944/15287 train loss 0.0236 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11008/15287 train loss 0.0173 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11072/15287 train loss 0.0224 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11136/15287 train loss 0.0158 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11200/15287 train loss 0.0176 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11264/15287 train loss 0.0235 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11328/15287 train loss 0.0260 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11392/15287 train loss 0.0192 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11456/15287 train loss 0.0196 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11520/15287 train loss 0.0190 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11584/15287 train loss 0.0297 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11648/15287 train loss 0.0211 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11712/15287 train loss 0.0137 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11776/15287 train loss 0.0214 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11840/15287 train loss 0.0250 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11904/15287 train loss 0.0260 acc : 100.00% - (64/64)\n",
      "epoch 8/11 11968/15287 train loss 0.0208 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12032/15287 train loss 0.0183 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12096/15287 train loss 0.0249 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12160/15287 train loss 0.0344 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12224/15287 train loss 0.0424 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12288/15287 train loss 0.0193 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12352/15287 train loss 0.0289 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12416/15287 train loss 0.1091 acc : 98.44% - (63/64)\n",
      "epoch 8/11 12480/15287 train loss 0.0204 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12544/15287 train loss 0.0236 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12608/15287 train loss 0.0177 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12672/15287 train loss 0.0254 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12736/15287 train loss 0.0217 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12800/15287 train loss 0.0184 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12864/15287 train loss 0.0237 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12928/15287 train loss 0.0196 acc : 100.00% - (64/64)\n",
      "epoch 8/11 12992/15287 train loss 0.0271 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13056/15287 train loss 0.0134 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13120/15287 train loss 0.0272 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13184/15287 train loss 0.0194 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13248/15287 train loss 0.0266 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13312/15287 train loss 0.0223 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13376/15287 train loss 0.0145 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13440/15287 train loss 0.0228 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13504/15287 train loss 0.0244 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13568/15287 train loss 0.0222 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13632/15287 train loss 0.0234 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13696/15287 train loss 0.0245 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13760/15287 train loss 0.0451 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13824/15287 train loss 0.0183 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13888/15287 train loss 0.0216 acc : 100.00% - (64/64)\n",
      "epoch 8/11 13952/15287 train loss 0.0177 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14016/15287 train loss 0.0284 acc : 98.44% - (63/64)\n",
      "epoch 8/11 14080/15287 train loss 0.0208 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14144/15287 train loss 0.0123 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14208/15287 train loss 0.0169 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14272/15287 train loss 0.0194 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14336/15287 train loss 0.0168 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14400/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14464/15287 train loss 0.0176 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14528/15287 train loss 0.0280 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14592/15287 train loss 0.0171 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14656/15287 train loss 0.0176 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14720/15287 train loss 0.0286 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14784/15287 train loss 0.0166 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14848/15287 train loss 0.0151 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14912/15287 train loss 0.0200 acc : 100.00% - (64/64)\n",
      "epoch 8/11 14976/15287 train loss 0.0285 acc : 100.00% - (64/64)\n",
      "epoch 8/11 15040/15287 train loss 0.0563 acc : 98.44% - (63/64)\n",
      "epoch 8/11 15104/15287 train loss 0.0409 acc : 98.44% - (63/64)\n",
      "epoch 8/11 15168/15287 train loss 0.0231 acc : 100.00% - (64/64)\n",
      "epoch 8/11 15232/15287 train loss 0.0249 acc : 100.00% - (64/64)\n",
      "epoch 8/11 15287/15287 train loss 0.0220 acc : 100.00% - (55/55)\n",
      "\n",
      " epoch 8 train end!!! \t train batch loss : 0.0325\t total acc : 99.80% - (15256/15287) \n",
      "\n",
      "epoch 8 val end!!! val loss : 0.025 \t f1 score : 1.000 acc : 87.40% - (1485/1699) \n",
      "\n",
      "\n",
      "epoch 9/11 64/15287 train loss 0.0190 acc : 100.00% - (64/64)\n",
      "epoch 9/11 128/15287 train loss 0.0146 acc : 100.00% - (64/64)\n",
      "epoch 9/11 192/15287 train loss 0.0202 acc : 100.00% - (64/64)\n",
      "epoch 9/11 256/15287 train loss 0.0199 acc : 100.00% - (64/64)\n",
      "epoch 9/11 320/15287 train loss 0.0268 acc : 100.00% - (64/64)\n",
      "epoch 9/11 384/15287 train loss 0.0146 acc : 100.00% - (64/64)\n",
      "epoch 9/11 448/15287 train loss 0.0187 acc : 100.00% - (64/64)\n",
      "epoch 9/11 512/15287 train loss 0.0173 acc : 100.00% - (64/64)\n",
      "epoch 9/11 576/15287 train loss 0.0191 acc : 100.00% - (64/64)\n",
      "epoch 9/11 640/15287 train loss 0.0137 acc : 100.00% - (64/64)\n",
      "epoch 9/11 704/15287 train loss 0.0238 acc : 100.00% - (64/64)\n",
      "epoch 9/11 768/15287 train loss 0.0159 acc : 100.00% - (64/64)\n",
      "epoch 9/11 832/15287 train loss 0.0150 acc : 100.00% - (64/64)\n",
      "epoch 9/11 896/15287 train loss 0.0249 acc : 100.00% - (64/64)\n",
      "epoch 9/11 960/15287 train loss 0.0216 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1024/15287 train loss 0.0143 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1088/15287 train loss 0.0197 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1152/15287 train loss 0.0245 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1216/15287 train loss 0.0153 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1280/15287 train loss 0.0351 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1344/15287 train loss 0.0223 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1408/15287 train loss 0.0183 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1472/15287 train loss 0.0121 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1536/15287 train loss 0.0215 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1600/15287 train loss 0.0156 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1664/15287 train loss 0.0212 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1728/15287 train loss 0.0157 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1792/15287 train loss 0.0209 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1856/15287 train loss 0.0296 acc : 98.44% - (63/64)\n",
      "epoch 9/11 1920/15287 train loss 0.0255 acc : 100.00% - (64/64)\n",
      "epoch 9/11 1984/15287 train loss 0.0245 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2048/15287 train loss 0.0219 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2112/15287 train loss 0.0139 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2176/15287 train loss 0.0315 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2240/15287 train loss 0.2051 acc : 98.44% - (63/64)\n",
      "epoch 9/11 2304/15287 train loss 0.0180 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2368/15287 train loss 0.0169 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2432/15287 train loss 0.0150 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2496/15287 train loss 0.0203 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2560/15287 train loss 0.0202 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2624/15287 train loss 0.0123 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2688/15287 train loss 0.0166 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2752/15287 train loss 0.0251 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2816/15287 train loss 0.0123 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2880/15287 train loss 0.0192 acc : 100.00% - (64/64)\n",
      "epoch 9/11 2944/15287 train loss 0.0174 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3008/15287 train loss 0.0440 acc : 98.44% - (63/64)\n",
      "epoch 9/11 3072/15287 train loss 0.0096 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3136/15287 train loss 0.0186 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3200/15287 train loss 0.0135 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3264/15287 train loss 0.0199 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3328/15287 train loss 0.0145 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3392/15287 train loss 0.0107 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3456/15287 train loss 0.0190 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3520/15287 train loss 0.0167 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3584/15287 train loss 0.0208 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3648/15287 train loss 0.0204 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3712/15287 train loss 0.0252 acc : 98.44% - (63/64)\n",
      "epoch 9/11 3776/15287 train loss 0.0195 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3840/15287 train loss 0.0139 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3904/15287 train loss 0.0149 acc : 100.00% - (64/64)\n",
      "epoch 9/11 3968/15287 train loss 0.0184 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4032/15287 train loss 0.0256 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4096/15287 train loss 0.0190 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4160/15287 train loss 0.0168 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4224/15287 train loss 0.0158 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4288/15287 train loss 0.0144 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4352/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4416/15287 train loss 0.0170 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4480/15287 train loss 0.0171 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4544/15287 train loss 0.0171 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4608/15287 train loss 0.0277 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4672/15287 train loss 0.0247 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4736/15287 train loss 0.0123 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4800/15287 train loss 0.0103 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4864/15287 train loss 0.0127 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4928/15287 train loss 0.0141 acc : 100.00% - (64/64)\n",
      "epoch 9/11 4992/15287 train loss 0.0225 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5056/15287 train loss 0.0156 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5120/15287 train loss 0.0197 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5184/15287 train loss 0.0117 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5248/15287 train loss 0.0168 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5312/15287 train loss 0.0688 acc : 98.44% - (63/64)\n",
      "epoch 9/11 5376/15287 train loss 0.0185 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5440/15287 train loss 0.0217 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5504/15287 train loss 0.0151 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5568/15287 train loss 0.0184 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5632/15287 train loss 0.0169 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5696/15287 train loss 0.0161 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5760/15287 train loss 0.0107 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5824/15287 train loss 0.0385 acc : 98.44% - (63/64)\n",
      "epoch 9/11 5888/15287 train loss 0.0209 acc : 100.00% - (64/64)\n",
      "epoch 9/11 5952/15287 train loss 0.0136 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6016/15287 train loss 0.0187 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6080/15287 train loss 0.0157 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6144/15287 train loss 0.0190 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6208/15287 train loss 0.0140 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6272/15287 train loss 0.0125 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6336/15287 train loss 0.0168 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6400/15287 train loss 0.0174 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6464/15287 train loss 0.0212 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6528/15287 train loss 0.0130 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6592/15287 train loss 0.0187 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6656/15287 train loss 0.0178 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6720/15287 train loss 0.0189 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6784/15287 train loss 0.0378 acc : 98.44% - (63/64)\n",
      "epoch 9/11 6848/15287 train loss 0.0173 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6912/15287 train loss 0.0170 acc : 100.00% - (64/64)\n",
      "epoch 9/11 6976/15287 train loss 0.0334 acc : 98.44% - (63/64)\n",
      "epoch 9/11 7040/15287 train loss 0.0160 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7104/15287 train loss 0.0106 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7168/15287 train loss 0.0190 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7232/15287 train loss 0.0117 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7296/15287 train loss 0.0188 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7360/15287 train loss 0.0113 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7424/15287 train loss 0.0235 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7488/15287 train loss 0.0211 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7552/15287 train loss 0.0153 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7616/15287 train loss 0.0094 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7680/15287 train loss 0.0100 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7744/15287 train loss 0.0175 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7808/15287 train loss 0.0202 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7872/15287 train loss 0.0192 acc : 100.00% - (64/64)\n",
      "epoch 9/11 7936/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8000/15287 train loss 0.0239 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8064/15287 train loss 0.0156 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8128/15287 train loss 0.0205 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8192/15287 train loss 0.0199 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8256/15287 train loss 0.0169 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8320/15287 train loss 0.0127 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8384/15287 train loss 0.0174 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8448/15287 train loss 0.0095 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8512/15287 train loss 0.0260 acc : 98.44% - (63/64)\n",
      "epoch 9/11 8576/15287 train loss 0.0125 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8640/15287 train loss 0.0170 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8704/15287 train loss 0.0146 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8768/15287 train loss 0.0081 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8832/15287 train loss 0.0183 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8896/15287 train loss 0.0159 acc : 100.00% - (64/64)\n",
      "epoch 9/11 8960/15287 train loss 0.0188 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9024/15287 train loss 0.0201 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9088/15287 train loss 0.0128 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9152/15287 train loss 0.0144 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9216/15287 train loss 0.0112 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9280/15287 train loss 0.0944 acc : 98.44% - (63/64)\n",
      "epoch 9/11 9344/15287 train loss 0.0196 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9408/15287 train loss 0.0149 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9472/15287 train loss 0.0150 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9536/15287 train loss 0.0165 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9600/15287 train loss 0.0144 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9664/15287 train loss 0.0198 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9728/15287 train loss 0.0103 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9792/15287 train loss 0.0160 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9856/15287 train loss 0.0335 acc : 98.44% - (63/64)\n",
      "epoch 9/11 9920/15287 train loss 0.0131 acc : 100.00% - (64/64)\n",
      "epoch 9/11 9984/15287 train loss 0.0119 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10048/15287 train loss 0.0191 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10112/15287 train loss 0.0184 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10176/15287 train loss 0.0179 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10240/15287 train loss 0.0093 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10304/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10368/15287 train loss 0.0166 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10432/15287 train loss 0.0156 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10496/15287 train loss 0.0135 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10560/15287 train loss 0.0228 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10624/15287 train loss 0.0228 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10688/15287 train loss 0.0102 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10752/15287 train loss 0.0118 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10816/15287 train loss 0.0161 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10880/15287 train loss 0.0152 acc : 100.00% - (64/64)\n",
      "epoch 9/11 10944/15287 train loss 0.0162 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11008/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11072/15287 train loss 0.0114 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11136/15287 train loss 0.0104 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11200/15287 train loss 0.0161 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11264/15287 train loss 0.0165 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11328/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11392/15287 train loss 0.0130 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11456/15287 train loss 0.0164 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11520/15287 train loss 0.0101 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11584/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11648/15287 train loss 0.0127 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11712/15287 train loss 0.0093 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11776/15287 train loss 0.0136 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11840/15287 train loss 0.0157 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11904/15287 train loss 0.0149 acc : 100.00% - (64/64)\n",
      "epoch 9/11 11968/15287 train loss 0.0138 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12032/15287 train loss 0.0118 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12096/15287 train loss 0.0132 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12160/15287 train loss 0.0136 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12224/15287 train loss 0.0219 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12288/15287 train loss 0.0119 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12352/15287 train loss 0.0157 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12416/15287 train loss 0.0875 acc : 98.44% - (63/64)\n",
      "epoch 9/11 12480/15287 train loss 0.0153 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12544/15287 train loss 0.0165 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12608/15287 train loss 0.0122 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12672/15287 train loss 0.0193 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12736/15287 train loss 0.0127 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12800/15287 train loss 0.0132 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12864/15287 train loss 0.0156 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12928/15287 train loss 0.0090 acc : 100.00% - (64/64)\n",
      "epoch 9/11 12992/15287 train loss 0.0171 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13056/15287 train loss 0.0091 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13120/15287 train loss 0.0182 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13184/15287 train loss 0.0133 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13248/15287 train loss 0.0184 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13312/15287 train loss 0.0117 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13376/15287 train loss 0.0112 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13440/15287 train loss 0.0136 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13504/15287 train loss 0.0175 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13568/15287 train loss 0.0146 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13632/15287 train loss 0.0128 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13696/15287 train loss 0.0161 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13760/15287 train loss 0.0163 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13824/15287 train loss 0.0105 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13888/15287 train loss 0.0112 acc : 100.00% - (64/64)\n",
      "epoch 9/11 13952/15287 train loss 0.0131 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14016/15287 train loss 0.0093 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14080/15287 train loss 0.0130 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14144/15287 train loss 0.0092 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14208/15287 train loss 0.0104 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14272/15287 train loss 0.0098 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14336/15287 train loss 0.0121 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14400/15287 train loss 0.0086 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14464/15287 train loss 0.0111 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14528/15287 train loss 0.0136 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14592/15287 train loss 0.0111 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14656/15287 train loss 0.0090 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14720/15287 train loss 0.0140 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14784/15287 train loss 0.0088 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14848/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14912/15287 train loss 0.0111 acc : 100.00% - (64/64)\n",
      "epoch 9/11 14976/15287 train loss 0.0155 acc : 100.00% - (64/64)\n",
      "epoch 9/11 15040/15287 train loss 0.0176 acc : 100.00% - (64/64)\n",
      "epoch 9/11 15104/15287 train loss 0.0165 acc : 100.00% - (64/64)\n",
      "epoch 9/11 15168/15287 train loss 0.0141 acc : 100.00% - (64/64)\n",
      "epoch 9/11 15232/15287 train loss 0.0092 acc : 100.00% - (64/64)\n",
      "epoch 9/11 15287/15287 train loss 0.0109 acc : 100.00% - (55/55)\n",
      "\n",
      " epoch 9 train end!!! \t train batch loss : 0.0182\t total acc : 99.92% - (15275/15287) \n",
      "\n",
      "epoch 9 val end!!! val loss : 0.018 \t f1 score : 1.000 acc : 87.40% - (1485/1699) \n",
      "\n",
      "\n",
      "epoch 10/11 64/15287 train loss 0.0207 acc : 100.00% - (64/64)\n",
      "epoch 10/11 128/15287 train loss 0.0077 acc : 100.00% - (64/64)\n",
      "epoch 10/11 192/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 10/11 256/15287 train loss 0.0146 acc : 100.00% - (64/64)\n",
      "epoch 10/11 320/15287 train loss 0.0156 acc : 100.00% - (64/64)\n",
      "epoch 10/11 384/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 10/11 448/15287 train loss 0.0149 acc : 100.00% - (64/64)\n",
      "epoch 10/11 512/15287 train loss 0.0246 acc : 100.00% - (64/64)\n",
      "epoch 10/11 576/15287 train loss 0.0154 acc : 100.00% - (64/64)\n",
      "epoch 10/11 640/15287 train loss 0.0107 acc : 100.00% - (64/64)\n",
      "epoch 10/11 704/15287 train loss 0.0140 acc : 100.00% - (64/64)\n",
      "epoch 10/11 768/15287 train loss 0.0096 acc : 100.00% - (64/64)\n",
      "epoch 10/11 832/15287 train loss 0.0088 acc : 100.00% - (64/64)\n",
      "epoch 10/11 896/15287 train loss 0.0128 acc : 100.00% - (64/64)\n",
      "epoch 10/11 960/15287 train loss 0.0121 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1024/15287 train loss 0.0084 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1088/15287 train loss 0.0131 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1152/15287 train loss 0.0146 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1216/15287 train loss 0.0084 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1280/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1344/15287 train loss 0.0141 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1408/15287 train loss 0.0116 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1472/15287 train loss 0.0086 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1536/15287 train loss 0.0103 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1600/15287 train loss 0.0097 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1664/15287 train loss 0.0137 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1728/15287 train loss 0.0095 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1792/15287 train loss 0.0121 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1856/15287 train loss 0.0111 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1920/15287 train loss 0.0129 acc : 100.00% - (64/64)\n",
      "epoch 10/11 1984/15287 train loss 0.0147 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2048/15287 train loss 0.0125 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2112/15287 train loss 0.0081 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2176/15287 train loss 0.0160 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2240/15287 train loss 0.2270 acc : 98.44% - (63/64)\n",
      "epoch 10/11 2304/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2368/15287 train loss 0.0109 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2432/15287 train loss 0.0101 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2496/15287 train loss 0.0111 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2560/15287 train loss 0.0129 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2624/15287 train loss 0.0104 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2688/15287 train loss 0.0119 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2752/15287 train loss 0.0130 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2816/15287 train loss 0.0075 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2880/15287 train loss 0.0121 acc : 100.00% - (64/64)\n",
      "epoch 10/11 2944/15287 train loss 0.0105 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3008/15287 train loss 0.0147 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3072/15287 train loss 0.0079 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3136/15287 train loss 0.0101 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3200/15287 train loss 0.0199 acc : 98.44% - (63/64)\n",
      "epoch 10/11 3264/15287 train loss 0.0102 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3328/15287 train loss 0.0102 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3392/15287 train loss 0.0082 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3456/15287 train loss 0.0113 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3520/15287 train loss 0.0123 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3584/15287 train loss 0.0116 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3648/15287 train loss 0.0115 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3712/15287 train loss 0.0079 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3776/15287 train loss 0.0104 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3840/15287 train loss 0.0097 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3904/15287 train loss 0.0105 acc : 100.00% - (64/64)\n",
      "epoch 10/11 3968/15287 train loss 0.0100 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4032/15287 train loss 0.0111 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4096/15287 train loss 0.0115 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4160/15287 train loss 0.0106 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4224/15287 train loss 0.0096 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4288/15287 train loss 0.0062 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4352/15287 train loss 0.0104 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4416/15287 train loss 0.0118 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4480/15287 train loss 0.0131 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4544/15287 train loss 0.0117 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4608/15287 train loss 0.0123 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4672/15287 train loss 0.0154 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4736/15287 train loss 0.0078 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4800/15287 train loss 0.0070 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4864/15287 train loss 0.0115 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4928/15287 train loss 0.0085 acc : 100.00% - (64/64)\n",
      "epoch 10/11 4992/15287 train loss 0.0160 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5056/15287 train loss 0.0094 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5120/15287 train loss 0.0143 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5184/15287 train loss 0.0073 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5248/15287 train loss 0.0102 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5312/15287 train loss 0.0620 acc : 98.44% - (63/64)\n",
      "epoch 10/11 5376/15287 train loss 0.0100 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5440/15287 train loss 0.0117 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5504/15287 train loss 0.0093 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5568/15287 train loss 0.0099 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5632/15287 train loss 0.0113 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5696/15287 train loss 0.0099 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5760/15287 train loss 0.0075 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5824/15287 train loss 0.0121 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5888/15287 train loss 0.0111 acc : 100.00% - (64/64)\n",
      "epoch 10/11 5952/15287 train loss 0.0087 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6016/15287 train loss 0.0119 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6080/15287 train loss 0.0104 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6144/15287 train loss 0.0096 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6208/15287 train loss 0.0108 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6272/15287 train loss 0.0079 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6336/15287 train loss 0.0174 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6400/15287 train loss 0.0131 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6464/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6528/15287 train loss 0.0073 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6592/15287 train loss 0.0098 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6656/15287 train loss 0.0114 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6720/15287 train loss 0.0130 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6784/15287 train loss 0.0131 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6848/15287 train loss 0.0101 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6912/15287 train loss 0.0117 acc : 100.00% - (64/64)\n",
      "epoch 10/11 6976/15287 train loss 0.0075 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7040/15287 train loss 0.0094 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7104/15287 train loss 0.0073 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7168/15287 train loss 0.0103 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7232/15287 train loss 0.0075 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7296/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7360/15287 train loss 0.0075 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7424/15287 train loss 0.0097 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7488/15287 train loss 0.0126 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7552/15287 train loss 0.0097 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7616/15287 train loss 0.0056 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7680/15287 train loss 0.0072 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7744/15287 train loss 0.0106 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7808/15287 train loss 0.0114 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7872/15287 train loss 0.0080 acc : 100.00% - (64/64)\n",
      "epoch 10/11 7936/15287 train loss 0.0080 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8000/15287 train loss 0.0104 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8064/15287 train loss 0.0101 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8128/15287 train loss 0.0110 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8192/15287 train loss 0.0081 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8256/15287 train loss 0.0100 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8320/15287 train loss 0.0072 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8384/15287 train loss 0.0086 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8448/15287 train loss 0.0059 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8512/15287 train loss 0.0054 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8576/15287 train loss 0.0085 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8640/15287 train loss 0.0108 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8704/15287 train loss 0.0105 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8768/15287 train loss 0.0047 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8832/15287 train loss 0.0115 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8896/15287 train loss 0.0086 acc : 100.00% - (64/64)\n",
      "epoch 10/11 8960/15287 train loss 0.0128 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9024/15287 train loss 0.0132 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9088/15287 train loss 0.0086 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9152/15287 train loss 0.0083 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9216/15287 train loss 0.0053 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9280/15287 train loss 0.1049 acc : 98.44% - (63/64)\n",
      "epoch 10/11 9344/15287 train loss 0.0098 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9408/15287 train loss 0.0098 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9472/15287 train loss 0.0092 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9536/15287 train loss 0.0097 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9600/15287 train loss 0.0085 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9664/15287 train loss 0.0139 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9728/15287 train loss 0.0074 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9792/15287 train loss 0.0107 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9856/15287 train loss 0.0116 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9920/15287 train loss 0.0091 acc : 100.00% - (64/64)\n",
      "epoch 10/11 9984/15287 train loss 0.0065 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10048/15287 train loss 0.0105 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10112/15287 train loss 0.0058 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10176/15287 train loss 0.0109 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10240/15287 train loss 0.0069 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10304/15287 train loss 0.0083 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10368/15287 train loss 0.0116 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10432/15287 train loss 0.0095 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10496/15287 train loss 0.0089 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10560/15287 train loss 0.0149 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10624/15287 train loss 0.0164 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10688/15287 train loss 0.0058 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10752/15287 train loss 0.0077 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10816/15287 train loss 0.0112 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10880/15287 train loss 0.0109 acc : 100.00% - (64/64)\n",
      "epoch 10/11 10944/15287 train loss 0.0109 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11008/15287 train loss 0.0085 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11072/15287 train loss 0.0069 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11136/15287 train loss 0.0062 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11200/15287 train loss 0.0068 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11264/15287 train loss 0.0090 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11328/15287 train loss 0.0085 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11392/15287 train loss 0.0073 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11456/15287 train loss 0.0091 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11520/15287 train loss 0.0067 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11584/15287 train loss 0.0084 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11648/15287 train loss 0.0075 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11712/15287 train loss 0.0095 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11776/15287 train loss 0.0090 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11840/15287 train loss 0.0097 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11904/15287 train loss 0.0098 acc : 100.00% - (64/64)\n",
      "epoch 10/11 11968/15287 train loss 0.0100 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12032/15287 train loss 0.0074 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12096/15287 train loss 0.0083 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12160/15287 train loss 0.0089 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12224/15287 train loss 0.0132 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12288/15287 train loss 0.0096 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12352/15287 train loss 0.0091 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12416/15287 train loss 0.0609 acc : 98.44% - (63/64)\n",
      "epoch 10/11 12480/15287 train loss 0.0098 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12544/15287 train loss 0.0107 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12608/15287 train loss 0.0077 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12672/15287 train loss 0.0098 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12736/15287 train loss 0.0085 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12800/15287 train loss 0.0089 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12864/15287 train loss 0.0095 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12928/15287 train loss 0.0066 acc : 100.00% - (64/64)\n",
      "epoch 10/11 12992/15287 train loss 0.0116 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13056/15287 train loss 0.0061 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13120/15287 train loss 0.0121 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13184/15287 train loss 0.0097 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13248/15287 train loss 0.0124 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13312/15287 train loss 0.0070 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13376/15287 train loss 0.0074 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13440/15287 train loss 0.0090 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13504/15287 train loss 0.0110 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13568/15287 train loss 0.0101 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13632/15287 train loss 0.0082 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13696/15287 train loss 0.0092 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13760/15287 train loss 0.0099 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13824/15287 train loss 0.0064 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13888/15287 train loss 0.0074 acc : 100.00% - (64/64)\n",
      "epoch 10/11 13952/15287 train loss 0.0076 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14016/15287 train loss 0.0058 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14080/15287 train loss 0.0078 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14144/15287 train loss 0.0059 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14208/15287 train loss 0.0072 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14272/15287 train loss 0.0068 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14336/15287 train loss 0.0085 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14400/15287 train loss 0.0056 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14464/15287 train loss 0.0078 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14528/15287 train loss 0.0087 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14592/15287 train loss 0.0077 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14656/15287 train loss 0.0065 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14720/15287 train loss 0.0089 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14784/15287 train loss 0.0058 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14848/15287 train loss 0.0073 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14912/15287 train loss 0.0078 acc : 100.00% - (64/64)\n",
      "epoch 10/11 14976/15287 train loss 0.0103 acc : 100.00% - (64/64)\n",
      "epoch 10/11 15040/15287 train loss 0.0061 acc : 100.00% - (64/64)\n",
      "epoch 10/11 15104/15287 train loss 0.0102 acc : 100.00% - (64/64)\n",
      "epoch 10/11 15168/15287 train loss 0.0084 acc : 100.00% - (64/64)\n",
      "epoch 10/11 15232/15287 train loss 0.0061 acc : 100.00% - (64/64)\n",
      "epoch 10/11 15287/15287 train loss 0.0060 acc : 100.00% - (55/55)\n",
      "\n",
      " epoch 10 train end!!! \t train batch loss : 0.0118\t total acc : 99.97% - (15282/15287) \n",
      "\n",
      "epoch 10 val end!!! val loss : 0.011 \t f1 score : 1.000 acc : 87.40% - (1485/1699) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.metrics import f1_score\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
    "\n",
    "# 토치 텍스트 관련 참조자료\n",
    "# https://towardsdatascience.com/custom-datasets-in-pytorch-part-2-text-machine-translation-71c41a3e994e\n",
    "\n",
    "\n",
    "CFG = {\n",
    "    'IMG_SIZE':128,\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':64,\n",
    "    'SEED':41,\n",
    "    'MAX_VOCAB_SIZE':100000,\n",
    "    'TRAIN_RATE':0.9,\n",
    "    'NUM_WORKERS':4\n",
    "}\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, max_size,mecab):\n",
    "        self.itos = {0: '<unk>', 1:'<pad>'}\n",
    "        self.stoi = {k:j for j,k in self.itos.items()} \n",
    "        self.mecab = mecab\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "        self.bi_gram =True\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 2\n",
    "        self.max_len = 0\n",
    "\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            sentence = self.mecab.morphs(sentence)\n",
    "            \n",
    "            ## 바이그램으로 구성할지 여부\n",
    "            if self.bi_gram:\n",
    "                sentence = self.generate_bigrams(sentence)\n",
    "            \n",
    "            ## 문장 최대 길이\n",
    "            if len(sentence) > self.max_len:\n",
    "                self.max_len = len(sentence)\n",
    "            \n",
    "            for word in sentence:\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
    "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
    "        for word in frequencies.keys():\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx+=1\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.mecab.morphs(text)\n",
    "        numericalized_text = []\n",
    "        for token in tokenized_text:\n",
    "            if token in self.stoi.keys():\n",
    "                numericalized_text.append(self.stoi[token])\n",
    "            else:\n",
    "                numericalized_text.append(self.stoi['<unk>'])\n",
    "                \n",
    "        return numericalized_text\n",
    "    \n",
    "    ## bi그램 처리\n",
    "    def generate_bigrams(self,x):\n",
    "        n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "        for n_gram in n_grams:\n",
    "            x.append(' '.join(n_gram))\n",
    "        return x\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,csv_path,max_vocab_size,transforms,infer=False):\n",
    "        self.all_df = pd.read_csv(csv_path)        \n",
    "        \n",
    "        ## 텍스트 리스트에서 html문법 빼기\n",
    "        self.text_list = list(self.all_df['overview'])\n",
    "        self.img_path_list = list(self.all_df['img_path'])\n",
    "        \n",
    "        self.label_list = list(self.all_df['cat3'])\n",
    "        self.num2label = {i:label for i,label in enumerate(list(set(self.label_list)))}\n",
    "        self.lable2num = {label:i for i,label in enumerate(list(set(self.label_list)))}\n",
    "        \n",
    "        self.TEXT = Vocabulary(0,max_vocab_size,Mecab())\n",
    "        self.TEXT.build_vocabulary(self.text_list)        \n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.infer = infer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "       \n",
    "        text = self.text_list[index]\n",
    "        text_vector = self.TEXT.numericalize(text)\n",
    "        text_len = len(text_vector)\n",
    "        \n",
    "        ## Image\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        # Label\n",
    "        if self.infer:\n",
    "            return image, torch.Tensor(text_vector).view(-1)\n",
    "        else:\n",
    "            label = self.lable2num[self.label_list[index]]\n",
    "            return image, torch.Tensor(text_vector).view(-1).to(torch.long), torch.tensor([label],dtype=torch.long),torch.tensor([text_len],dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_df)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "\n",
    "print(\"데이터셋 구성 중\")\n",
    "train_all_dataset = CustomDataset('./train.csv',CFG['MAX_VOCAB_SIZE'],train_transform)\n",
    "vocab_size = len(train_all_dataset.TEXT.itos)\n",
    "label_info = train_all_dataset.num2label\n",
    "print(\"데이터셋 구성 완료\")\n",
    "\n",
    "dataset_size = len(train_all_dataset)\n",
    "train_size = int(dataset_size * CFG['TRAIN_RATE'])\n",
    "validation_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(train_all_dataset, [train_size, validation_size])\n",
    "\n",
    "\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        image = [item[0] for item in batch]  \n",
    "        text = [item[1] for item in batch]  \n",
    "        text = nn.utils.rnn.pad_sequence(text, batch_first=True, padding_value = self.pad_idx) \n",
    "        label = [item[2] for item in batch]  \n",
    "        text_len = [item[3] for item in batch]  \n",
    "        return {\"image\":torch.stack(image),\"text\":text,\"label\":torch.stack(label).squeeze(),\"text_len\":torch.stack(text_len)}\n",
    "    \n",
    "\n",
    "pad_idx = train_all_dataset.TEXT.stoi['<pad>']\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], num_workers = CFG['NUM_WORKERS'],pin_memory=True, collate_fn = MyCollate(pad_idx=pad_idx))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size = CFG['BATCH_SIZE'], num_workers = CFG['NUM_WORKERS'],pin_memory=True, collate_fn = MyCollate(pad_idx=pad_idx))\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,vocab_size, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = 1024\n",
    "        self.vocab_size = vocab_size\n",
    "        # Image\n",
    "        self.cnn_extract = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # 이미지만 3136\n",
    "        \n",
    "        # Text\n",
    "        self.bon_embed = nn.Embedding(vocab_size,self.embedding_dim,padding_idx=1)\n",
    "        self.fc = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        self.nlp_bn = nn.BatchNorm1d(self.embedding_dim)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4160, num_classes)\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, img, text,text_len):\n",
    "        img_feature = self.cnn_extract(img)\n",
    "        img_feature = torch.flatten(img_feature, start_dim=1)\n",
    "        \n",
    "        embed = self.bon_embed(text)\n",
    "        embed = torch.sum(embed, 1).squeeze(1)\n",
    "        batch_size = embed.size(0)\n",
    "        # text_len = text_len.float().unsqueeze(1)\n",
    "        text_len = text_len.expand(batch_size, self.embedding_dim)\n",
    "        embed /= text_len\n",
    "        embed = self.nlp_bn(embed)\n",
    "        text_feature = self.fc(embed)\n",
    "        \n",
    "        feature = torch.cat([img_feature, text_feature], axis=1)\n",
    "        \n",
    "        \n",
    "        output = self.classifier(feature)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = CustomModel(vocab_size,len(label_info))\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "best_score = 0\n",
    "best_model = None\n",
    "# optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"],weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "# 손실함수에 어떤 제약 조건을 적용해 오버피팅을 최소화하는 방법으로 L1 정형화와 L2 정형화가 있습니다. 오버피팅은 특정 가중치값이 커질수록 발생할 가능성이 높아지므로 이를 해소하기 위해 특정값을 손실함수에 더해주는 것이 정형화 중 가중치 감소(Weight Decay)이며, 더해주는 특정값을 결정하는 것이 L1 정형화와 L2 정형화입니다. 파이토치에서 이 Weight Decay는 다음 코드처럼 적용할 수 있습니다.\n",
    "# 결과적으로 weight_decay의 값이 커질수록 가중치 값이 작어지게 되고, 오버피팅 현상을 해소할 수 있지만, weight_decay 값을 너무 크게 하면 언더피팅 현상이 발생하므로 적당한 값을 사용해야 합니다.\n",
    "\n",
    "def score_function(real, pred):\n",
    "    return f1_score(real, pred, average=\"weighted\")\n",
    "\n",
    "\n",
    "for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
    "    model.train()\n",
    "    train_loss = []   \n",
    "    train_data_len = train_dataset.__len__()\n",
    "    total_train_correct = 0\n",
    "    \n",
    "    for i,data_batch in enumerate(train_loader):\n",
    "        img = data_batch['image']\n",
    "        text = data_batch['text']\n",
    "        label = data_batch['label']\n",
    "        text_len = data_batch['text_len']\n",
    "        \n",
    "        img = img.float().to(device)\n",
    "        text = text.to(device)\n",
    "        label = label.to(device)\n",
    "        text_len = text_len.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_pred = model(img, text,text_len)        \n",
    "        loss = criterion(model_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()                \n",
    "        _, predicted = torch.max(model_pred, 1) \n",
    "        correct = (predicted == label).sum().item()        \n",
    "        total_train_correct += correct        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        print(f'epoch {epoch}/{CFG[\"EPOCHS\"]+1} {(i*CFG[\"BATCH_SIZE\"])+len(label)}/{train_data_len} train loss {loss.item():.4f} acc : {100*correct/len(label):.2f}% - ({correct}/{len(label)})')\n",
    "        \n",
    "    tr_loss = np.mean(train_loss)\n",
    "    print(f\"\\n epoch {epoch} train end!!! \\t train batch loss : {tr_loss:.4f}\\t total acc : {100*total_train_correct/train_data_len:.2f}% - ({total_train_correct}/{train_data_len}) \\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    val_data_len = validation_dataset.__len__()\n",
    "    model.eval()   \n",
    "    model_preds = []\n",
    "    true_labels = []    \n",
    "    val_loss = []    \n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for img, text, label,text_len in validation_loader:\n",
    "            img = data_batch['image']\n",
    "            text = data_batch['text']\n",
    "            label = data_batch['label']\n",
    "            text_len = data_batch['text_len']\n",
    "        \n",
    "            img = img.float().to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            text_len = text_len.to(device)\n",
    "            \n",
    "            model_pred = model(img, text,text_len)\n",
    "            \n",
    "            loss = criterion(model_pred, label)\n",
    "            \n",
    "            _, predicted = torch.max(model_pred, 1) \n",
    "            correct = (predicted == label).sum().item() \n",
    "            total_val_correct+=correct\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "        \n",
    "    test_weighted_f1 = score_function(true_labels, model_preds)\n",
    "    \n",
    "    print(f\"epoch {epoch} val end!!! val loss : {np.mean(val_loss):.3f} \\t f1 score : {test_weighted_f1:.3f} acc : {100*total_val_correct/val_data_len:.2f}% - ({total_val_correct}/{val_data_len}) \\n\\n\")    \n",
    "\n",
    "    \n",
    "    writer.add_scalars(\"loss\",{\"tr_loss\":tr_loss,\"val loss\":np.mean(val_loss)},epoch)\n",
    "    writer.add_scalars(\"acc\",{\"tr_acc\":total_train_correct/train_data_len,\"val_acc\":total_val_correct/val_data_len},epoch)\n",
    "    torch.save(model.state_dict(),'./'+str(epoch)+\".pth\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ATV',\n",
       " 1: '기타행사',\n",
       " 2: '국립공원',\n",
       " 3: '공연장',\n",
       " 4: '식음료',\n",
       " 5: '전시관',\n",
       " 6: '전통공연',\n",
       " 7: '스노쿨링/스킨스쿠버다이빙',\n",
       " 8: '서양식',\n",
       " 9: '유람선/잠수함관광',\n",
       " 10: '등대',\n",
       " 11: '호수',\n",
       " 12: '안보관광',\n",
       " 13: '온천/욕장/스파',\n",
       " 14: '한식',\n",
       " 15: '유명건물',\n",
       " 16: '카약/카누',\n",
       " 17: '자연생태관광지',\n",
       " 18: '발전소',\n",
       " 19: '중식',\n",
       " 20: '항구/포구',\n",
       " 21: '바/까페',\n",
       " 22: '군립공원',\n",
       " 23: '해수욕장',\n",
       " 24: '수련시설',\n",
       " 25: '학교',\n",
       " 26: '문화원',\n",
       " 27: '썰매장',\n",
       " 28: '공예,공방',\n",
       " 29: '유스호스텔',\n",
       " 30: '민속마을',\n",
       " 31: '미술관/화랑',\n",
       " 32: '다리/대교',\n",
       " 33: '특산물판매점',\n",
       " 34: 'MTB',\n",
       " 35: '유적지/사적지',\n",
       " 36: '윈드서핑/제트스키',\n",
       " 37: '뮤지컬',\n",
       " 38: '채식전문점',\n",
       " 39: '클럽',\n",
       " 40: '스카이다이빙',\n",
       " 41: '면세점',\n",
       " 42: '기념관',\n",
       " 43: '스케이트',\n",
       " 44: '계곡',\n",
       " 45: '대형서점',\n",
       " 46: '복합 레포츠',\n",
       " 47: '희귀동.식물',\n",
       " 48: '이색체험',\n",
       " 49: '박람회',\n",
       " 50: '골프',\n",
       " 51: '이색찜질방',\n",
       " 52: '분수',\n",
       " 53: '약수터',\n",
       " 54: '사격장',\n",
       " 55: '홈스테이',\n",
       " 56: '번지점프',\n",
       " 57: '도서관',\n",
       " 58: '자전거하이킹',\n",
       " 59: '폭포',\n",
       " 60: '트래킹',\n",
       " 61: '문화전수시설',\n",
       " 62: '승마',\n",
       " 63: '콘도미니엄',\n",
       " 64: '기타',\n",
       " 65: '강',\n",
       " 66: '연극',\n",
       " 67: '인라인(실내 인라인 포함)',\n",
       " 68: '카트',\n",
       " 69: '상설시장',\n",
       " 70: '바다낚시',\n",
       " 71: '패밀리레스토랑',\n",
       " 72: '사찰',\n",
       " 73: '빙벽등반',\n",
       " 74: '자동차경주',\n",
       " 75: '전문상가',\n",
       " 76: '야영장,오토캠핑장',\n",
       " 77: '서비스드레지던스',\n",
       " 78: '래프팅',\n",
       " 79: '문',\n",
       " 80: '박물관',\n",
       " 81: '생가',\n",
       " 82: '농.산.어촌 체험',\n",
       " 83: '해안절경',\n",
       " 84: '모텔',\n",
       " 85: '영화관',\n",
       " 86: '헹글라이딩/패러글라이딩',\n",
       " 87: '민박',\n",
       " 88: '민물낚시',\n",
       " 89: '동상',\n",
       " 90: '스키/스노보드',\n",
       " 91: '산',\n",
       " 92: '클래식음악회',\n",
       " 93: '펜션',\n",
       " 94: '이색거리',\n",
       " 95: '고궁',\n",
       " 96: '대중콘서트',\n",
       " 97: '카지노',\n",
       " 98: '자연휴양림',\n",
       " 99: '동굴',\n",
       " 100: '컨벤션',\n",
       " 101: '성',\n",
       " 102: '외국문화원',\n",
       " 103: '문화관광축제',\n",
       " 104: '고택',\n",
       " 105: '수상레포츠',\n",
       " 106: '수영',\n",
       " 107: '관광단지',\n",
       " 108: '유원지',\n",
       " 109: '종교성지',\n",
       " 110: '도립공원',\n",
       " 111: '터널',\n",
       " 112: '5일장',\n",
       " 113: '스키(보드) 렌탈샵',\n",
       " 114: '공원',\n",
       " 115: '헬스투어',\n",
       " 116: '테마공원',\n",
       " 117: '수목원',\n",
       " 118: '요트',\n",
       " 119: '섬',\n",
       " 120: '컨벤션센터',\n",
       " 121: '기암괴석',\n",
       " 122: '한옥스테이',\n",
       " 123: '일반축제',\n",
       " 124: '게스트하우스',\n",
       " 125: '백화점',\n",
       " 126: '일식',\n",
       " 127: '기념탑/기념비/전망대'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_dataset.num2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class infer_CustomDataset(Dataset):\n",
    "    def __init__(self,csv_path,max_vocab_size,voca,transforms,infer=False):\n",
    "        self.all_df = pd.read_csv(csv_path)        \n",
    "        \n",
    "\n",
    "        self.text_list = list(self.all_df['overview'])\n",
    "        self.img_path_list = list(self.all_df['img_path'])\n",
    "       \n",
    "       \n",
    "        self.TEXT = voca\n",
    "    \n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.infer = infer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "       \n",
    "        text = self.text_list[index]\n",
    "        text_vector = self.TEXT.numericalize(text)\n",
    "        text_len = len(text_vector)\n",
    "        \n",
    "        ## Image\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        # Label\n",
    "        if self.infer:\n",
    "            return image, torch.Tensor(text_vector).view(-1).to(torch.long),torch.tensor([text_len],dtype=torch.long)\n",
    "        else:\n",
    "            label = self.lable2num[self.label_list[index]]\n",
    "            return image, torch.Tensor(text_vector).view(-1).to(torch.long), torch.tensor([label],dtype=torch.long),torch.tensor([text_len],dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_df)\n",
    "\n",
    "\n",
    "\n",
    "class MyCollate_infer:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        image = [item[0] for item in batch]  \n",
    "        text = [item[1] for item in batch]  \n",
    "        text = nn.utils.rnn.pad_sequence(text, batch_first=True, padding_value = self.pad_idx) \n",
    "        text_len = [item[2] for item in batch]  \n",
    "        return {\"image\":torch.stack(image),\"text\":text,\"text_len\":torch.stack(text_len)}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "test_dataset = infer_CustomDataset('./test.csv',CFG['MAX_VOCAB_SIZE'],train_all_dataset.TEXT,test_transform,infer=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], num_workers = CFG['NUM_WORKERS'],pin_memory=True, collate_fn = MyCollate_infer(pad_idx=pad_idx))\n",
    "    \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_preds = []\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for data_batch in test_loader:\n",
    "        count  += len(data_batch['image'])\n",
    "\n",
    "        img = data_batch['image']\n",
    "        text = data_batch['text']\n",
    "        text_len = data_batch['text_len']\n",
    "\n",
    "        img = img.float().to(device)\n",
    "        text = text.to(device)\n",
    "        text_len = text_len.to(device)\n",
    "        \n",
    "        model_pred = model(img, text,text_len)\n",
    "        model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.infer_CustomDataset at 0x7fca8ef90610>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7280\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in model_preds:\n",
    "    result.append(train_all_dataset.num2label[i])\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['cat3'] = result\n",
    "\n",
    "submit.to_csv('./submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>수련시설</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>홈스테이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>TEST_07275</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7276</th>\n",
       "      <td>TEST_07276</td>\n",
       "      <td>공연장</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>TEST_07277</td>\n",
       "      <td>야영장,오토캠핑장</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>TEST_07278</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>TEST_07279</td>\n",
       "      <td>박물관</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7280 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       cat3\n",
       "0     TEST_00000         한식\n",
       "1     TEST_00001         한식\n",
       "2     TEST_00002         한식\n",
       "3     TEST_00003       수련시설\n",
       "4     TEST_00004       홈스테이\n",
       "...          ...        ...\n",
       "7275  TEST_07275         한식\n",
       "7276  TEST_07276        공연장\n",
       "7277  TEST_07277  야영장,오토캠핑장\n",
       "7278  TEST_07278         한식\n",
       "7279  TEST_07279        박물관\n",
       "\n",
       "[7280 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02f73df80f6b7cfb1d2d2729c6624b9061c0386599073f9b468acf97e0bc0e85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
